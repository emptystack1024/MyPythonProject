{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://hf-mirror.com/nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境：Mistrall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f79bf179c0477a9ad120d2da8f6780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Answer---------------------------------\n",
      " The percentage change of the net income from Q4 FY23 to Q4 FY24 is calculated using the formula (($12,285 million - $1,414 million) / $1,414 million * 100), resulting in a 769% increase.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"/home/asus/文档/AIModel/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is the percentage change of the net income from Q4 FY23 to Q4 FY24?\"}\n",
    "]\n",
    "\n",
    "document = \"\"\"NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.\\nFor the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.\\nQ4 Fiscal 2024 Summary\\nGAAP\\n| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |\\n| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |\\n| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |\\n| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |\\n| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |\\n| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |\\n| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |\"\"\"\n",
    "\n",
    "def get_formatted_input(messages, context):\n",
    "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "    instruction = \"Please give a full and complete answer for the question.\"\n",
    "\n",
    "    for item in messages:\n",
    "        if item['role'] == \"user\":\n",
    "            ## only apply this instruction for the first user turn\n",
    "            item['content'] = instruction + \" \" + item['content']\n",
    "            break\n",
    "\n",
    "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n",
    "    \n",
    "    return formatted_input\n",
    "\n",
    "formatted_input = get_formatted_input(messages, document)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n",
    "\n",
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(\"------------------------Answer---------------------------------\")\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafa6c646d324f8ea4019355e077b406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "## load ChatQA-1.5 tokenizer and model\n",
    "model_id = \"/home/asus/文档/AIModel/Llama3-ChatQA-1.5-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "## load retriever tokenizer and model\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained('./dragon-multiturn-query-encoder')\n",
    "query_encoder = AutoModel.from_pretrained('./dragon-multiturn-query-encoder')\n",
    "context_encoder = AutoModel.from_pretrained('./dragon-multiturn-context-encoder')\n",
    "\n",
    "## prepare documents, we take landrover car manual document that we provide as an example\n",
    "chunk_list = json.load(open(\"./docs.json\"))['landrover']\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"how to connect the bluetooth in the car?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可能问题内存爆了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "### running retrieval\n",
    "## convert query into a format as follows:\n",
    "## user: {user}\\nagent: {agent}\\nuser: {user}\n",
    "formatted_query_for_retriever = '\\n'.join([turn['role'] + \": \" + turn['content'] for turn in messages]).strip()\n",
    "\n",
    "query_input = retriever_tokenizer(formatted_query_for_retriever, return_tensors='pt')\n",
    "ctx_input = retriever_tokenizer(chunk_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "query_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\n",
    "ctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Compute similarity scores using dot product and rank the similarity\n",
    "similarities = query_emb.matmul(ctx_emb.transpose(0, 1)) # (1, num_ctx)\n",
    "ranked_results = torch.argsort(similarities, dim=-1, descending=True) # (1, num_ctx)\n",
    "\n",
    "## get top-n chunks (n=5)\n",
    "retrieved_chunks = [chunk_list[idx] for idx in ranked_results.tolist()[0][:5]]\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "### running text generation\n",
    "formatted_input = get_formatted_input(messages, context)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n",
    "\n",
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
