{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 使用 Langchain 和 GLM 完成简单的简历信息抽取任务。\n",
    "\n",
    "**This tutorial is Only in Chinese explanation**\n",
    "\n",
    "本代码，我将使用 Langchain 配合 GLM-4 完成 Word 文档的简历信息抽取任务。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c7735c37bd4a71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 配置相关环境\n",
    "由于需要安装部分依赖，这里我们需要安装必要的依赖，如果你已经安装了这些依赖，你可以跳过这个步骤。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f6db85940dd9ad"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langchain unstructured python-docx  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:34:16.558790Z",
     "start_time": "2024-04-28T12:34:12.378643Z"
    }
   },
   "id": "4fa261f2679ba7e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: unstructured in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (0.13.4)\n",
      "Requirement already satisfied: python-docx in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (0.0.34)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (0.1.46)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (0.1.51)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: chardet in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (5.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (2.11.1)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (2024.2.7)\n",
      "Requirement already satisfied: langdetect in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: backoff in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (4.9.0)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (0.22.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from nltk->unstructured) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from nltk->unstructured) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from nltk->unstructured) (4.65.2)\n",
      "Requirement already satisfied: deepdiff>=6.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured-client->unstructured) (7.0.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "接着，我们需要将我们的 API_KEY 配置到环境变量中，用于调用 GLM-4 模型。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "741ecbd0ae7b3148"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"7cc2d454b5b31371f5c6c990fddc7636.EPXYYOmeKYWvDEqW\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:34:16.564936Z",
     "start_time": "2024-04-28T12:34:16.560310Z"
    }
   },
   "id": "5f982f839d2340d3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 读取简历文档\n",
    "\n",
    "我们需要将简历的文档用 Langchain 的 UnstructuredWordDocumentLoader 读入，并填充到我们的模板中。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd69981c5a6fb246"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "loader = UnstructuredWordDocumentLoader(\"data/resume.docx\")\n",
    "data = loader.load()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:34:20.346150Z",
     "start_time": "2024-04-28T12:34:16.566944Z"
    }
   },
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\33398/nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocument_loaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UnstructuredWordDocumentLoader\n\u001B[0;32m      3\u001B[0m loader \u001B[38;5;241m=\u001B[39m UnstructuredWordDocumentLoader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/resume.docx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m data \u001B[38;5;241m=\u001B[39m loader\u001B[38;5;241m.\u001B[39mload()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:29\u001B[0m, in \u001B[0;36mBaseLoader.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[0;32m     28\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlazy_load())\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:88\u001B[0m, in \u001B[0;36mUnstructuredBaseLoader.lazy_load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlazy_load\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Document]:\n\u001B[0;32m     87\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load file.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 88\u001B[0m     elements \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_elements()\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_process_elements(elements)\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124melements\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_community\\document_loaders\\word_document.py:125\u001B[0m, in \u001B[0;36mUnstructuredWordDocumentLoader._get_elements\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01munstructured\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpartition\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m partition_docx\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m partition_docx(filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munstructured_kwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\documents\\elements.py:570\u001B[0m, in \u001B[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Element]:\n\u001B[1;32m--> 570\u001B[0m     elements \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    571\u001B[0m     sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(func)\n\u001B[0;32m    572\u001B[0m     params: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:622\u001B[0m, in \u001B[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    620\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Element]:\n\u001B[1;32m--> 622\u001B[0m     elements \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    623\u001B[0m     sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(func)\n\u001B[0;32m    624\u001B[0m     params: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:582\u001B[0m, in \u001B[0;36madd_metadata.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    580\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Element]:\n\u001B[1;32m--> 582\u001B[0m     elements \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    583\u001B[0m     sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(func)\n\u001B[0;32m    584\u001B[0m     params: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:83\u001B[0m, in \u001B[0;36madd_chunking_strategy.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m call_args\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# -- call the partitioning function to get the elements --\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m elements \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# -- look for a chunking-strategy argument --\u001B[39;00m\n\u001B[0;32m     86\u001B[0m call_args \u001B[38;5;241m=\u001B[39m get_call_args_applying_defaults()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\docx.py:245\u001B[0m, in \u001B[0;36mpartition_docx\u001B[1;34m(filename, file, metadata_filename, include_page_breaks, include_metadata, infer_table_structure, metadata_last_modified, chunking_strategy, languages, detect_language_per_element, date_from_file_object, starting_page_number, **kwargs)\u001B[0m\n\u001B[0;32m    230\u001B[0m elements \u001B[38;5;241m=\u001B[39m _DocxPartitioner\u001B[38;5;241m.\u001B[39miter_document_elements(\n\u001B[0;32m    231\u001B[0m     filename,\n\u001B[0;32m    232\u001B[0m     file,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m     starting_page_number\u001B[38;5;241m=\u001B[39mstarting_page_number,\n\u001B[0;32m    239\u001B[0m )\n\u001B[0;32m    240\u001B[0m elements \u001B[38;5;241m=\u001B[39m apply_lang_metadata(\n\u001B[0;32m    241\u001B[0m     elements\u001B[38;5;241m=\u001B[39melements,\n\u001B[0;32m    242\u001B[0m     languages\u001B[38;5;241m=\u001B[39mlanguages,\n\u001B[0;32m    243\u001B[0m     detect_language_per_element\u001B[38;5;241m=\u001B[39mdetect_language_per_element,\n\u001B[0;32m    244\u001B[0m )\n\u001B[1;32m--> 245\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(elements)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\lang.py:397\u001B[0m, in \u001B[0;36mapply_lang_metadata\u001B[1;34m(elements, languages, detect_language_per_element)\u001B[0m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;66;03m# Convert elements to a list to get the text, detect the language, and add it to the elements\u001B[39;00m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elements, List):\n\u001B[1;32m--> 397\u001B[0m     elements \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(elements)\n\u001B[0;32m    399\u001B[0m full_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(e\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m elements \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    400\u001B[0m detected_languages \u001B[38;5;241m=\u001B[39m detect_languages(text\u001B[38;5;241m=\u001B[39mfull_text, languages\u001B[38;5;241m=\u001B[39mlanguages)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\docx.py:329\u001B[0m, in \u001B[0;36m_DocxPartitioner._iter_document_elements\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block_item \u001B[38;5;129;01min\u001B[39;00m section\u001B[38;5;241m.\u001B[39miter_inner_content():\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;66;03m# -- a block-item can be a Paragraph or a Table, maybe others later so elif here.\u001B[39;00m\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;66;03m# -- Paragraph is more common so check that first.\u001B[39;00m\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(block_item, Paragraph):\n\u001B[1;32m--> 329\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_paragraph_elements(block_item)\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(  \u001B[38;5;66;03m# pyright: ignore[reportUnnecessaryIsInstance]\u001B[39;00m\n\u001B[0;32m    331\u001B[0m         block_item, DocxTable\n\u001B[0;32m    332\u001B[0m     ):\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_table_element(block_item)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\docx.py:570\u001B[0m, in \u001B[0;36m_DocxPartitioner._iter_paragraph_elements\u001B[1;34m(self, paragraph)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m iter_paragraph_items(paragraph):\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Paragraph):\n\u001B[1;32m--> 570\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_classify_paragraph_to_element(item)\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    572\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_increment_page_number()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\docx.py:390\u001B[0m, in \u001B[0;36m_DocxPartitioner._classify_paragraph_to_element\u001B[1;34m(self, paragraph)\u001B[0m\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;66;03m# NOTE(scanny) - try to recognize the element type by parsing its text\u001B[39;00m\n\u001B[1;32m--> 390\u001B[0m TextSubCls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_paragraph_text_for_element_type(paragraph)\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TextSubCls:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m TextSubCls(text\u001B[38;5;241m=\u001B[39mtext, metadata\u001B[38;5;241m=\u001B[39mmetadata, detection_origin\u001B[38;5;241m=\u001B[39mDETECTION_ORIGIN)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\docx.py:899\u001B[0m, in \u001B[0;36m_DocxPartitioner._parse_paragraph_text_for_element_type\u001B[1;34m(self, paragraph)\u001B[0m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_email_address(text):\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m EmailAddress\n\u001B[1;32m--> 899\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_possible_narrative_text(text):\n\u001B[0;32m    900\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m NarrativeText\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_possible_title(text):\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\text_type.py:78\u001B[0m, in \u001B[0;36mis_possible_narrative_text\u001B[1;34m(text, cap_threshold, non_alpha_threshold, languages, language_checks)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# NOTE(robinson): it gets read in from the environment as a string so we need to\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# cast it to a float\u001B[39;00m\n\u001B[0;32m     75\u001B[0m cap_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\n\u001B[0;32m     76\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNSTRUCTURED_NARRATIVE_TEXT_CAP_THRESHOLD\u001B[39m\u001B[38;5;124m\"\u001B[39m, cap_threshold),\n\u001B[0;32m     77\u001B[0m )\n\u001B[1;32m---> 78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exceeds_cap_ratio(text, threshold\u001B[38;5;241m=\u001B[39mcap_threshold):\n\u001B[0;32m     79\u001B[0m     trace_logger\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNot narrative. Text exceeds cap ratio \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcap_threshold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# type: ignore # noqa: E501\u001B[39;00m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\text_type.py:274\u001B[0m, in \u001B[0;36mexceeds_cap_ratio\u001B[1;34m(text, threshold)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Checks the title ratio in a section of text. If a sufficient proportion of the words\u001B[39;00m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;124;03mare capitalized, that can be indicated on non-narrative text (i.e. \"1A. Risk Factors\").\u001B[39;00m\n\u001B[0;32m    263\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;124;03m    the function returns True\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;66;03m# NOTE(robinson) - Currently limiting this to only sections of text with one sentence.\u001B[39;00m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;66;03m# The assumption is that sections with multiple sentences are not titles.\u001B[39;00m\n\u001B[1;32m--> 274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sentence_count(text, \u001B[38;5;241m3\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text\u001B[38;5;241m.\u001B[39misupper():\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\partition\\text_type.py:223\u001B[0m, in \u001B[0;36msentence_count\u001B[1;34m(text, min_length)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentence_count\u001B[39m(text: \u001B[38;5;28mstr\u001B[39m, min_length: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    213\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Checks the sentence count for a section of text. Titles should not be more than one\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;124;03m    sentence.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m        The min number of words a section needs to be for it to be considered a sentence.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 223\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m sent_tokenize(text)\n\u001B[0;32m    224\u001B[0m     count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m sentences:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:30\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001B[39;00m\n\u001B[0;32m     29\u001B[0m _download_nltk_package_if_not_present(package_category\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers\u001B[39m\u001B[38;5;124m\"\u001B[39m, package_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _sent_tokenize(text)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\nltk\\data.py:750\u001B[0m, in \u001B[0;36mload\u001B[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[0;32m    747\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<<Loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresource_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[1;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m _open(resource_url)\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    753\u001B[0m     resource_val \u001B[38;5;241m=\u001B[39m opened_resource\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\nltk\\data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[1;34m(resource_url)\u001B[0m\n\u001B[0;32m    873\u001B[0m protocol, path_ \u001B[38;5;241m=\u001B[39m split_resource_url(resource_url)\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, path \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n\u001B[0;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[0;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/english.pickle\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\33398/nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\.conda\\\\envs\\\\pythonProject\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\33398\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "设定好模板，这个模板将会作为我们的系统提示词。我们将使用 Langchain 的 ChatPromptTemplate 来构建这个模板。其中，{resume} 将会被我们的简历内容填充。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90f6dca3b6fec516"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "你是 ZhipuAI 的 人事资源管理部门的优秀员工，现在我需要你帮我阅读简历并筛选出合适的人才，请你基于我提供的简历，对简历进行细节的分析，抓取相关的资料并回答我提出的问题。\n",
    "现在，我将会将简历以文字的形式给你提供，具体内容如下:\n",
    "\n",
    "<resume>\n",
    "{resume}\n",
    "</resume>\n",
    "\n",
    "请你根据我的简历，开始回答我的问题吧。请注意我的提问的内容和我需要你回答的格式，我们开始吧：\n",
    "\"\"\"\n",
    "\n",
    "question_prompt = [\n",
    "    \"候选人读过哪些大学？\",\n",
    "    # \"请帮我提取候选人简历中的关键信息，用JSON格式返回给我，我需要的字段是：姓名、性别、年龄、学历、工作年限、工作经历、项目经历、技能、个人优势、个人缺点、兴趣爱好；简历中没有提到的字段也要输出，但字段值为空。json的key可以使用中文，value的长度不要超过100个字符，如果字段值太长，请对内容进行总结摘要再输出。例如工作经历可以只保留公司名称和职位，工作经历和项目经历可以只保留项目名称和项目描述\",\n",
    "    # \"你怎么评价这个候选人，从他现有的资历、技术能力、工作态度、发展潜力进行分析。我们公司目前想招聘一个3-5年工作经验有一定的发展潜力的员工，请结合对候选人的分析和我的招聘需求判断我是否应该给他面试机会？\"\n",
    "]\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(resume=data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c500b35994e14ff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "接着，我们就可以调用 GLM-4 模型，通过模型对简历进行抓取和提取关键信息，获得有效的内容和答案。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e01263d1a189281f"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "for question in question_prompt:\n",
    "    messages = chat_template.format_messages(resume=data)\n",
    "    messages.append(\n",
    "        HumanMessage(\n",
    "            content=question\n",
    "        )\n",
    "    )\n",
    "    llm = ChatZhipuAI(\n",
    "        temperature=0.01,\n",
    "        model=\"glm-4\",\n",
    "        max_tokens=8192,\n",
    "        stream=False,\n",
    "    )\n",
    "    messages.append(\n",
    "        AIMessage(\n",
    "            content=llm(messages).content\n",
    "        )\n",
    "    )\n",
    "    print(messages[-1].content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f6fc2837b9d98b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 结果分析\n",
    "\n",
    "通过大模型，我们可以顺利的抽取出简历中的关键信息，包括教育背景、工作经历等。这样，我们就可以通过简单的代码，完成简历信息的抽取任务。\n",
    "这是一个开放性的demo，意味着你可以自己选择其他任务来接着完成这个场景的研究。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a78cbd80584dfa28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
