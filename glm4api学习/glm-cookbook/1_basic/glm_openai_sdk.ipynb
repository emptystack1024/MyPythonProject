{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a626e20197e21c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using ZhipuAI GLM-4 though OpenAI SDK and Ecosystem\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "本节代码将带领大家使用`OpenAI`的 SDK 和支持的框架调用 `GLM-4` 模型\n",
    "你将在本章节学习到如何使用 Langchain 的 `ChatOpenAI` 类来调用 ZhipuAI 的 GLM-4 模型，以及如何使用 `OpenAI SDK` 类来实现调用GLM-4模型。\n",
    "\n",
    "This notebook section will walk you through calling `GLM-4` models using the `OpenAI` SDK and supporting frameworks.\n",
    "In this section you will learn how to call ZhipuAI's GLM-4 models using Langchain's `ChatOpenAI` class, and how to use the `OpenAI SDK` class to implement calling GLM-4 models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e7836f5b885b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Use OpenAI SDK\n",
    "\n",
    "首先，需要导入 `OpenAI` SDK，同时，修改 `OpenAI` 的 `base_url` 为 `https://open.bigmodel.cn/api/paas/v4/` \n",
    "以便调用 `GLM-4` 模型。完成更换后，仅需填写 Key 就能使用 `GLM-4` 模型。\n",
    "\n",
    "First, you need to import the `OpenAI` SDK and change the `OpenAI` `base_url` to `https://open.bigmodel.cn/api/paas/v4/` in order to call the `GLM-4` model. \n",
    "in order to call the `GLM-4` model. Once you have done this, you can use the `GLM-4` model by simply filling in the Key.\n",
    "``python"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T10:50:03.156486Z",
     "start_time": "2024-04-27T10:49:43.318194Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install openai langchain_openai -i https://pypi.org/simple",
   "id": "ae5fb39b45c3f7af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (1.23.6)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (4.65.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain_openai) (0.1.46)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai) (3.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\33398\\.conda\\envs\\pythonproject\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.2.1)\n",
      "Downloading langchain_openai-0.1.4-py3-none-any.whl (33 kB)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/798.7 kB 1.3 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 30.7/798.7 kB 1.3 MB/s eta 0:00:01\n",
      "   ----- -------------------------------- 122.9/798.7 kB 901.1 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 153.6/798.7 kB 833.5 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 153.6/798.7 kB 833.5 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 153.6/798.7 kB 833.5 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 194.6/798.7 kB 562.0 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 256.0/798.7 kB 684.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 276.5/798.7 kB 710.0 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 286.7/798.7 kB 655.2 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 286.7/798.7 kB 655.2 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 307.2/798.7 kB 559.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 368.6/798.7 kB 588.5 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 389.1/798.7 kB 591.1 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 399.4/798.7 kB 553.5 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 399.4/798.7 kB 553.5 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 399.4/798.7 kB 553.5 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 450.6/798.7 kB 531.8 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 450.6/798.7 kB 531.8 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 471.0/798.7 kB 517.2 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 471.0/798.7 kB 517.2 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 481.3/798.7 kB 471.0 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 481.3/798.7 kB 471.0 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 501.8/798.7 kB 443.2 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 501.8/798.7 kB 443.2 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 501.8/798.7 kB 443.2 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 522.2/798.7 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 522.2/798.7 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.7 kB 397.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.7 kB 397.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.7 kB 397.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.7 kB 397.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.7 kB 397.7 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 553.0/798.7 kB 343.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 563.2/798.7 kB 285.3 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.7 kB 243.1 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 604.2/798.7 kB 214.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 604.2/798.7 kB 214.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 604.2/798.7 kB 214.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 604.2/798.7 kB 214.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 604.2/798.7 kB 214.7 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 614.4/798.7 kB 201.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 614.4/798.7 kB 201.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 614.4/798.7 kB 201.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 614.4/798.7 kB 201.4 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 634.9/798.7 kB 195.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 634.9/798.7 kB 195.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 645.1/798.7 kB 189.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 645.1/798.7 kB 189.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 645.1/798.7 kB 189.0 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 665.6/798.7 kB 188.9 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 665.6/798.7 kB 188.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 686.1/798.7 kB 188.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 686.1/798.7 kB 188.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 686.1/798.7 kB 188.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 696.3/798.7 kB 185.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 696.3/798.7 kB 185.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 716.8/798.7 kB 185.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 716.8/798.7 kB 185.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 727.0/798.7 kB 184.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 727.0/798.7 kB 184.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 727.0/798.7 kB 184.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 727.0/798.7 kB 184.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 747.5/798.7 kB 181.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 747.5/798.7 kB 181.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 768.0/798.7 kB 181.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 768.0/798.7 kB 181.6 kB/s eta 0:00:01\n",
      "   -------------------------------------  778.2/798.7 kB 179.4 kB/s eta 0:00:01\n",
      "   -------------------------------------  778.2/798.7 kB 179.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 798.7/798.7 kB 180.2 kB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, langchain_openai\n",
      "Successfully installed langchain_openai-0.1.4 tiktoken-0.6.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:49:04.751427Z",
     "start_time": "2024-04-27T10:49:02.130679Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"7cc2d454b5b31371f5c6c990fddc7636.EPXYYOmeKYWvDEqW\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "11e3e2b3eddb8819",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Simple use of OpenAI SDK to call GLM-4\n",
    "\n",
    "我将带领大家完成最基础的对话调用。\n",
    "请注意：\n",
    "- `temperature` 参数的区间为 (0,1)。\n",
    "- `do_sample = False (temperature = 0)` 在 OpenAI 调用中并不适用。\n",
    "-  异步操作不适用。\n",
    "\n",
    "其他例子(例如工具调用)，也均可以按照OpenAI的方式进行调用。\n",
    "\n",
    "\n",
    "I will lead everyone to complete the most basic dialogue calls.\n",
    "Please note:\n",
    "\n",
    "- The interval of the `temperature` parameter is (0,1).\n",
    "- `do_sample=False (temperature=0)` is not applicable in OpenAI calls.\n",
    "- Asynchronous operations are not applicable.\n",
    "\n",
    "Other examples (such as tool calls) can also be called in the same way as OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "id": "46b558c16e324725",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:49:08.486260Z",
     "start_time": "2024-04-27T10:49:06.862859Z"
    }
   },
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\"\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.7,\n",
    "    temperature=0.9,\n",
    "    stream=False,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='8605592118730949543', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", role='assistant', function_call=None, tool_calls=None))], created=1714214949, model='glm-4', object=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=9, total_tokens=35), request_id='8605592118730949543')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ba894923a1806fc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Using Langchain's ChatOpenAI class\n",
    "\n",
    "Langchain 的 `ChatOpenAI` 类是对 `OpenAI` SDK 的封装，可以更方便调用。这里展示了如何使用 `ChatOpenAI` 类来调用 `GLM-4` 模型。\n",
    "\n",
    "Langchain's `ChatOpenAI` class is a wrapper around the `OpenAI` SDK, making it easier to call. Here we show how to use the `ChatOpenAI` class to call the `GLM-4` model."
   ]
  },
  {
   "cell_type": "code",
   "id": "95ca98869dcb246a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:50:42.309758Z",
     "start_time": "2024-04-27T10:50:39.448977Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=\"your zhipuai api key\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"tell me a joke\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: tell me a joke\u001B[0m\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'code': '401', 'message': '令牌已过期或验证不正确！'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 35\u001B[0m\n\u001B[0;32m     28\u001B[0m memory \u001B[38;5;241m=\u001B[39m ConversationBufferMemory(memory_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_history\u001B[39m\u001B[38;5;124m\"\u001B[39m, return_messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     29\u001B[0m conversation \u001B[38;5;241m=\u001B[39m LLMChain(\n\u001B[0;32m     30\u001B[0m     llm\u001B[38;5;241m=\u001B[39mllm,\n\u001B[0;32m     31\u001B[0m     prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m     32\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     33\u001B[0m     memory\u001B[38;5;241m=\u001B[39mmemory\n\u001B[0;32m     34\u001B[0m )\n\u001B[1;32m---> 35\u001B[0m conversation\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtell me a joke\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 153\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    156\u001B[0m     )\n\u001B[0;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate([inputs], run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    116\u001B[0m         prompts,\n\u001B[0;32m    117\u001B[0m         stop,\n\u001B[0;32m    118\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    120\u001B[0m     )\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    124\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    554\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    558\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    559\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    419\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    420\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 421\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    422\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    423\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    424\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    425\u001B[0m ]\n\u001B[0;32m    426\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 411\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    412\u001B[0m                 m,\n\u001B[0;32m    413\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    414\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    415\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    416\u001B[0m             )\n\u001B[0;32m    417\u001B[0m         )\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    419\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 632\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    633\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    634\u001B[0m         )\n\u001B[0;32m    635\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    636\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:567\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    565\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    566\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m--> 567\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:579\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    577\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    578\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 579\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    581\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    582\u001B[0m             {\n\u001B[0;32m    583\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[0;32m    584\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    585\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[0;32m    586\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[0;32m    587\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[0;32m    588\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[0;32m    589\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[0;32m    590\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[0;32m    591\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[0;32m    592\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[0;32m    593\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[0;32m    594\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[0;32m    595\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[0;32m    596\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[0;32m    597\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    598\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[0;32m    599\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    600\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[0;32m    601\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    602\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[0;32m    603\u001B[0m             },\n\u001B[0;32m    604\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[0;32m    605\u001B[0m         ),\n\u001B[0;32m    606\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    607\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    608\u001B[0m         ),\n\u001B[0;32m    609\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[0;32m    610\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    611\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[0;32m    612\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:1232\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1220\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1227\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1229\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1230\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1231\u001B[0m     )\n\u001B[1;32m-> 1232\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    922\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    923\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    924\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    925\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    926\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[0;32m    927\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:1012\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1009\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1011\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1012\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1015\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1016\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1019\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1020\u001B[0m )\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'code': '401', 'message': '令牌已过期或验证不正确！'}}"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "2c6c560191f25ec5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Calling GLM-4 with LangChain AgentExecutor\n",
    "同时，GLM-4也能很好的接入到 LangChain 的 AgentExecutor 中，这里展示了如何使用 `AgentExecutor` 来调用 `GLM-4` 模型。\n",
    "\n",
    "GLM-4 also plugs nicely into LangChain's AgentExecutor, which shows how to call a `GLM-4` model using `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "id": "4db6cc6154c30560",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:50:56.065968Z",
     "start_time": "2024-04-27T10:50:45.826045Z"
    }
   },
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "tools = [TavilySearchResults(max_results=2)]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "# Choose the LLM to use\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'code': '401', 'message': '令牌已过期或验证不正确！'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m agent \u001B[38;5;241m=\u001B[39m create_react_agent(llm, tools, prompt)\n\u001B[0;32m     10\u001B[0m agent_executor \u001B[38;5;241m=\u001B[39m AgentExecutor(agent\u001B[38;5;241m=\u001B[39magent, tools\u001B[38;5;241m=\u001B[39mtools, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 11\u001B[0m agent_executor\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhat is LangChain?\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\base.py:163\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    162\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    164\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\chains\\base.py:153\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    152\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 153\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    154\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    155\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    156\u001B[0m     )\n\u001B[0;32m    158\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    159\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\agents\\agent.py:1432\u001B[0m, in \u001B[0;36mAgentExecutor._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[0;32m   1431\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_continue(iterations, time_elapsed):\n\u001B[1;32m-> 1432\u001B[0m     next_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_take_next_step(\n\u001B[0;32m   1433\u001B[0m         name_to_tool_map,\n\u001B[0;32m   1434\u001B[0m         color_mapping,\n\u001B[0;32m   1435\u001B[0m         inputs,\n\u001B[0;32m   1436\u001B[0m         intermediate_steps,\n\u001B[0;32m   1437\u001B[0m         run_manager\u001B[38;5;241m=\u001B[39mrun_manager,\n\u001B[0;32m   1438\u001B[0m     )\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[0;32m   1440\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return(\n\u001B[0;32m   1441\u001B[0m             next_step_output, intermediate_steps, run_manager\u001B[38;5;241m=\u001B[39mrun_manager\n\u001B[0;32m   1442\u001B[0m         )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\agents\\agent.py:1138\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_next_step\u001B[39m(\n\u001B[0;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1131\u001B[0m     name_to_tool_map: Dict[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1135\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1136\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[AgentFinish, List[Tuple[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[0;32m   1137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consume_next_step(\n\u001B[1;32m-> 1138\u001B[0m         [\n\u001B[0;32m   1139\u001B[0m             a\n\u001B[0;32m   1140\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_next_step(\n\u001B[0;32m   1141\u001B[0m                 name_to_tool_map,\n\u001B[0;32m   1142\u001B[0m                 color_mapping,\n\u001B[0;32m   1143\u001B[0m                 inputs,\n\u001B[0;32m   1144\u001B[0m                 intermediate_steps,\n\u001B[0;32m   1145\u001B[0m                 run_manager,\n\u001B[0;32m   1146\u001B[0m             )\n\u001B[0;32m   1147\u001B[0m         ]\n\u001B[0;32m   1148\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\agents\\agent.py:1138\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_next_step\u001B[39m(\n\u001B[0;32m   1130\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1131\u001B[0m     name_to_tool_map: Dict[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1135\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1136\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[AgentFinish, List[Tuple[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[0;32m   1137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consume_next_step(\n\u001B[1;32m-> 1138\u001B[0m         [\n\u001B[0;32m   1139\u001B[0m             a\n\u001B[0;32m   1140\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_next_step(\n\u001B[0;32m   1141\u001B[0m                 name_to_tool_map,\n\u001B[0;32m   1142\u001B[0m                 color_mapping,\n\u001B[0;32m   1143\u001B[0m                 inputs,\n\u001B[0;32m   1144\u001B[0m                 intermediate_steps,\n\u001B[0;32m   1145\u001B[0m                 run_manager,\n\u001B[0;32m   1146\u001B[0m             )\n\u001B[0;32m   1147\u001B[0m         ]\n\u001B[0;32m   1148\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\agents\\agent.py:1166\u001B[0m, in \u001B[0;36mAgentExecutor._iter_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m   1163\u001B[0m     intermediate_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_intermediate_steps(intermediate_steps)\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[1;32m-> 1166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mplan(\n\u001B[0;32m   1167\u001B[0m         intermediate_steps,\n\u001B[0;32m   1168\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1169\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,\n\u001B[0;32m   1170\u001B[0m     )\n\u001B[0;32m   1171\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1172\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain\\agents\\agent.py:397\u001B[0m, in \u001B[0;36mRunnableAgent.plan\u001B[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    389\u001B[0m final_output: Any \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_runnable:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001B[39;00m\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;66;03m# streaming\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    395\u001B[0m     \u001B[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001B[39;00m\n\u001B[0;32m    396\u001B[0m     \u001B[38;5;66;03m# accumulate the output into final output and return that.\u001B[39;00m\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunnable\u001B[38;5;241m.\u001B[39mstream(inputs, config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}):\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m final_output \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    399\u001B[0m             final_output \u001B[38;5;241m=\u001B[39m chunk\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2875\u001B[0m, in \u001B[0;36mRunnableSequence.stream\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2869\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstream\u001B[39m(\n\u001B[0;32m   2870\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2871\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   2872\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2873\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   2874\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Output]:\n\u001B[1;32m-> 2875\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(\u001B[38;5;28miter\u001B[39m([\u001B[38;5;28minput\u001B[39m]), config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2862\u001B[0m, in \u001B[0;36mRunnableSequence.transform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2856\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\n\u001B[0;32m   2857\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2858\u001B[0m     \u001B[38;5;28minput\u001B[39m: Iterator[Input],\n\u001B[0;32m   2859\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2860\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   2861\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Output]:\n\u001B[1;32m-> 2862\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transform_stream_with_config(\n\u001B[0;32m   2863\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   2864\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transform,\n\u001B[0;32m   2865\u001B[0m         patch_config(config, run_name\u001B[38;5;241m=\u001B[39m(config \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m   2866\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2867\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1881\u001B[0m, in \u001B[0;36mRunnable._transform_stream_with_config\u001B[1;34m(self, input, transformer, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1879\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1880\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1881\u001B[0m         chunk: Output \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mnext\u001B[39m, iterator)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m   1882\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[0;32m   1883\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m final_output_supported:\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2826\u001B[0m, in \u001B[0;36mRunnableSequence._transform\u001B[1;34m(self, input, run_manager, config)\u001B[0m\n\u001B[0;32m   2817\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m steps:\n\u001B[0;32m   2818\u001B[0m     final_pipeline \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[0;32m   2819\u001B[0m         final_pipeline,\n\u001B[0;32m   2820\u001B[0m         patch_config(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2823\u001B[0m         ),\n\u001B[0;32m   2824\u001B[0m     )\n\u001B[1;32m-> 2826\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m final_pipeline:\n\u001B[0;32m   2827\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m output\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1282\u001B[0m, in \u001B[0;36mRunnable.transform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   1279\u001B[0m final: Input\n\u001B[0;32m   1280\u001B[0m got_first_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ichunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28minput\u001B[39m:\n\u001B[0;32m   1283\u001B[0m     \u001B[38;5;66;03m# The default implementation of transform is to buffer input and\u001B[39;00m\n\u001B[0;32m   1284\u001B[0m     \u001B[38;5;66;03m# then call stream.\u001B[39;00m\n\u001B[0;32m   1285\u001B[0m     \u001B[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001B[39;00m\n\u001B[0;32m   1286\u001B[0m     \u001B[38;5;66;03m# the `+` operator.\u001B[39;00m\n\u001B[0;32m   1287\u001B[0m     \u001B[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001B[39;00m\n\u001B[0;32m   1288\u001B[0m     \u001B[38;5;66;03m# only operate on the last chunk,\u001B[39;00m\n\u001B[0;32m   1289\u001B[0m     \u001B[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001B[39;00m\n\u001B[0;32m   1290\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m got_first_val:\n\u001B[0;32m   1291\u001B[0m         final \u001B[38;5;241m=\u001B[39m ichunk\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4736\u001B[0m, in \u001B[0;36mRunnableBindingBase.transform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   4730\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\n\u001B[0;32m   4731\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4732\u001B[0m     \u001B[38;5;28minput\u001B[39m: Iterator[Input],\n\u001B[0;32m   4733\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   4734\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   4735\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Output]:\n\u001B[1;32m-> 4736\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[0;32m   4737\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   4738\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   4739\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   4740\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1300\u001B[0m, in \u001B[0;36mRunnable.transform\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   1297\u001B[0m             final \u001B[38;5;241m=\u001B[39m ichunk\n\u001B[0;32m   1299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m got_first_val:\n\u001B[1;32m-> 1300\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream(final, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:249\u001B[0m, in \u001B[0;36mBaseChatModel.stream\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    243\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(\n\u001B[0;32m    244\u001B[0m         e,\n\u001B[0;32m    245\u001B[0m         response\u001B[38;5;241m=\u001B[39mLLMResult(\n\u001B[0;32m    246\u001B[0m             generations\u001B[38;5;241m=\u001B[39m[[generation]] \u001B[38;5;28;01mif\u001B[39;00m generation \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[0;32m    247\u001B[0m         ),\n\u001B[0;32m    248\u001B[0m     )\n\u001B[1;32m--> 249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    251\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_end(LLMResult(generations\u001B[38;5;241m=\u001B[39m[[generation]]))\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:229\u001B[0m, in \u001B[0;36mBaseChatModel.stream\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    227\u001B[0m generation: Optional[ChatGenerationChunk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 229\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    230\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m chunk\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mid \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m             chunk\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mid \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_manager\u001B[38;5;241m.\u001B[39mrun_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:525\u001B[0m, in \u001B[0;36mChatOpenAI._stream\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    522\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m}\n\u001B[0;32m    524\u001B[0m default_chunk_class \u001B[38;5;241m=\u001B[39m AIMessageChunk\n\u001B[1;32m--> 525\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m response:\n\u001B[0;32m    527\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunk, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:579\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    577\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    578\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 579\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    581\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    582\u001B[0m             {\n\u001B[0;32m    583\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[0;32m    584\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    585\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[0;32m    586\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[0;32m    587\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[0;32m    588\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[0;32m    589\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[0;32m    590\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[0;32m    591\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[0;32m    592\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[0;32m    593\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[0;32m    594\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[0;32m    595\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[0;32m    596\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[0;32m    597\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    598\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[0;32m    599\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    600\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[0;32m    601\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    602\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[0;32m    603\u001B[0m             },\n\u001B[0;32m    604\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[0;32m    605\u001B[0m         ),\n\u001B[0;32m    606\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    607\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    608\u001B[0m         ),\n\u001B[0;32m    609\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[0;32m    610\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    611\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[0;32m    612\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:1232\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1220\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1227\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1229\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1230\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1231\u001B[0m     )\n\u001B[1;32m-> 1232\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    922\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    923\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    924\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    925\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    926\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[0;32m    927\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pythonProject\\Lib\\site-packages\\openai\\_base_client.py:1012\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1009\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1011\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1012\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1015\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1016\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1019\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1020\u001B[0m )\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'code': '401', 'message': '令牌已过期或验证不正确！'}}"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "494064895b79f43a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
