@relation testset

@attribute abstract string
@attribute keyword {CV,NLP} 
@data
'There have been recent advances in predicting odor characteristics using molecular structure parameters of chemicals Although the molecular structure parameters are available for each chemical they cannot be used for chemical mixtures This study will elucidate a computational method of predicting human odor perception from the mass spectra of chemical mixtures such as essential oils Furthermore a method for obtaining similarity among odor descriptors has been proposed although the dataset contains binary values only When the database indicates a set of odor descriptors for one sample only binary data are available and the correlation between the similar descriptors disappears Thus the prediction performance degrades for not considering the similarity among the odor descriptors Since mass spectra dataset is highly dimensional we use auto-encoder to learn the compressed representation from the mass spectra of essential oils in its bottleneck hidden layer and then accomplishes the hierarchical clustering to create odor descriptor groups with similar odor impressions using a matrix of continuous value-based correlation coefficient as well as natural language processing This work will help to expatiate the process of overcoming binary value problem and find out the similarity among odor descriptors using machine learning with natural language semantic representation of words To overcome the problem of disproportionate ratio of positive and negative class for both the continuous value-based correlation coefficient and word similarity based models we use Synthetic Minority Oversampling Technique (SMOTE) This model allows us to predict human odor perception through computer simulations by forming odor descriptors group Accordingly this study demonstrates the feasibility of ensembling machine learning with natural language processing and SMOTE approach for predicting odor descriptor group from mass spectra of essential oils',NLP
'Fully data-driven deep learning-based models are usually designed as language-independent and have been shown to be successful for many natural language processing tasks However when the studied language is not high-resource and the amount of training data is insufficient these models can benefit from the integration of natural language grammar-based information We propose two approaches to dependency parsing especially for languages with restricted amount of training data Our first approach combines a state-of-the-art deep learning-based parser with a rule-based approach and the second one incorporates morphological information into the parser In the rule-based approach the parsing decisions made by the rules are encoded and concatenated with the vector representations of the input words as additional information to the deep network The morphology-based approach proposes different methods to include the morphological structure of words into the parser network Experiments are conducted on three different Turkish treebanks and the results suggest that integration of explicit knowledge about the target language to a neural parser through a rule-based parsing system and morphological analysis leads to more accurate annotations and hence increases the parsing performance in terms of attachment scores The proposed methods are developed for Turkish but can be adapted to other languages as well',NLP
'Building distributed services and applications is challenging due to the pitfalls of distribution such as process and communication failures A natural solution to these problems is to detect potential failures and retry the failed computation and/or resend messages Ensuring correctness in such an environment requires distributed services and applications to be idempotent In this paper we study the inter-related aspects of process failures duplicate messages and idempotence We first introduce a simple core language (based on lambda-calculus) inspired by modern distributed computing platforms This language formalizes the notions of a service duplicate requests process failures data partitioning and local atomic transactions that are restricted to a single store We then formalize a desired (generic) correctness criterion for applications written in this language consisting of idempotence (which captures the desired safety properties) and failure-freedom (which captures the desired progress properties) We then propose language support in the form of a monad that automatically ensures failfree idempotence A key characteristic of our implementation is that it is decentralized and does not require distributed coordination We show that the language support can be enriched with other useful constructs such as compensations while retaining the coordination-free decentralized nature of the implementation We have implemented the idempotence monad (and its variants) in F# and C# and used our implementation to build realistic applications on Windows Azure We find that the monad has low runtime overheads and leads to more declarative applications',NLP
'Background: Periprosthetic joint infection (PJI) data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection The goal of this study is to develop a natural language processing (NLP) algorithm to replicate manual chart review for PJI data elements Methods: PJI was identified among all total joint arthroplasty (TJA) procedures performed at a single academic institution between 2000 and 2017 Data elements that comprise the Musculoskeletal Infection Society (MSIS) criteria were manually extracted and used as the gold standard for validation A training sample of 1208 TJA surgeries (170 PJI cases) was randomly selected to develop the prototype NLP algorithms and an additional 1179 surgeries (150 PJI cases) were randomly selected as the test sample The algorithms were applied to all consultation notes operative notes pathology reports and microbiology reports to predict the correct status of PJI based on MSIS criteria Results: The algorithm which identified patients with PJI based on MSIS criteria achieved an f1-score (harmonic mean of precision and recall) of 0911 Algorithm performance in extracting the presence of sinus tract purulence pathologic documentation of inflammation and growth of cultured organisms from the involved TJA achieved f1-scores that ranged from 0771 to 0982 sensitivity that ranged from 0730 to 1000 and specificity that ranged from 0947 to 1000 Conclusion: NLP-enabled algorithms have the potential to automate data collection for PJI diagnostic elements which could directly improve patient care and augment cohort surveillance and research efforts Further validation is needed in other hospital settings (C) 2020 Elsevier Inc All rights reserved',NLP
'Background: Natural Language Processing (NLP) has been shown effective to analyze the content of radiology reports and identify diagnosis or patient characteristics We evaluate the combination of NLP and machine learning to detect thromboembolic disease diagnosis and incidental clinically relevant findings from angiography and venography reports written in French We model thromboembolic diagnosis and incidental findings as a set of concepts modalities and relations between concepts that can be used as features by a supervised machine learning algorithm A corpus of 573 radiology reports was de-identified and manually annotated with the support of NLP tools by a physician for relevant concepts modalities and relations A machine learning classifier was trained on the dataset interpreted by a physician for diagnosis of deep-vein thrombosis pulmonary embolism and clinically relevant incidental findings Decision models accounted for the imbalanced nature of the data and exploited the structure of the reports Results: The best model achieved an F measure of 098 for pulmonary embolism identification 100 for deep vein thrombosis and 080 for incidental clinically relevant findings The use of concepts modalities and relations improved performances in all cases Conclusions: This study demonstrates the benefits of developing an automated method to identify medical concepts modality and relations from radiology reports in French An end-to-end automatic system for annotation and classification which could be applied to other radiology reports databases would be valuable for epidemiological surveillance performance monitoring and accreditation in French hospitals',NLP
'This paper describes the design and implementation of multilingualization (m17n) of a dynamic object-oriented environment called Squeak The goal of this project is to provide a collaborative and late-bound environment where the users can use many different natural languages and characters Squeak is a highly portable implementation of a dynamic objects environment and it is a good starting point toward the future collaborative environment However its text related classes lack the ability to handle natural languages that require extended character sets such as Arabic Chinese Greek Korean and Japanese We have been implementing the multilingualization extension to Squeak The extension we wrote can be classified as follows: 1) new character and string representations for extended character sets 2) keyboard input and the file out of multilingual text mechanism 3) flexible text composition mechanism 4) extended font handling mechanisms including dynamic font loading and outline font handling 5) higher level application changes including a Japanese version of SqueakToys The resulting environment has the following characteristics: 1) various natural languages can be used in the same context 2) the pixels on screen including the appearance of characters can be completely controlled by the program 3) decent word processing facility for a mixture of multiple languages 4) existing Squeak capability such as remote collaborative mechanism will be integrated with it 5) small memory footprint requirement',NLP
'Location metonymy resolution is a study that deals with locations being used in a non-literal way that create problems in several natural language processing tasks such as Named entity recognition and Geographical parsing Many studies were conducted attempting to accurately classify whether the location is used literally or metonymically however most of the approaches that performed well had to employ a considerable amount of resources along with complex machine learning models; those that reduced the resources experienced a decline in performance due to data sparseness This study proposes a novel feature selection approach that uses bag-of-words and augments it with GloVe embeddings to obtain features that can be recognized based on the context of the sentence We then implement a minimalist deep learning model making the entire classification task as light as possible The study found that relying solely on the given datasets to identify features without depending on other external resources can achieve remarkable results despite the small size of the datasets The results obtained from evaluating our method compared to the state-of-the-art methods show that eliminating noise based on the context notwithstanding the usage of low-cost resources has outperformed all of the previous methods with an accuracy of 992% on the WIMCOR dataset',NLP
'Natural language processing (NLP) is a promising tool for collecting data that are usually hard to obtain during extreme weather like community response and infrastructure performance Patterns and trends in abundant data sources such as weather reports news articles and social media may provide insights into potential impacts and early warnings of impending disasters This paper reviews the peer-reviewed studies (journals and conference proceedings) that used NLP to assess extreme weather events focusing on heavy rainfall events The methodology searches four databases (ScienceDirect Web of Science Scopus and IEEE Xplore) for articles published in English before June 2022 The preferred reporting items for systematic reviews and meta-analysis reviews and meta-analysis guidelines were followed to select and refine the search The method led to the identification of thirty-five studies In this study hurricanes typhoons and flooding were considered NLP models were implemented in information extraction topic modeling clustering and classification The findings show that NLP remains underutilized in studying extreme weather events The review demonstrated that NLP could potentially improve the usefulness of social media platforms newspapers and other data sources that could improve weather event assessment In addition NLP could generate new information that should complement data from ground-based sensors reducing monitoring costs Key outcomes of NLP use include improved accuracy increased public safety improved data collection and enhanced decision-making are identified in the study On the other hand researchers must overcome data inadequacy inaccessibility nonrepresentative and immature NLP approaches and computing skill requirements to use NLP properly',NLP
'Objective: Accurate ascertainment of comorbidities is paramount in clinical research While manual adjudication is labor-intensive and expensive the adoption of electronic health records enables computational analysis of free-text documentation using natural language processing (NLP) tools Hypothesis: We sought to develop highly accurate NLP modules to assess for the presence of five key cardiovascular comorbidities in a large electronic health record system Methods: One-thousand clinical notes were randomly selected from a cardiovascular registry at Mass General Brigham Trained physicians manually adjudicated these notes for the following five diagnostic comorbidities: hypertension dyslipidemia diabetes coronary artery disease and stroke/transient ischemic attack Using the open-source Canary NLP system five separate NLP modules were designed based on 800 training-set notes and validated on 200 test-set notes Results: Across the five NLP modules the sentence-level and note-level sensitivity specificity and positive predictive value was always greater than 85% and was most often greater than 90% Accuracy tended to be highest for conditions with greater diagnostic clarity (eg diabetes and hypertension) and slightly lower for conditions whose greater diagnostic challenges (eg myocardial infarction and embolic stroke) may lead to less definitive documentation Conclusion: We designed five open-source and highly accurate NLP modules that can be used to assess for the presence of important cardiovascular comorbidities in free-text health records These modules have been placed in the public domain and can be used for clinical research trial recruitment and population management at any institution as well as serve as the basis for further development of cardiovascular NLP tools',NLP
'The corrections (stipulations) to a proposed research study protocol produced by an institutional review board (IRB) can often be repetitive across many studies; however there is no standard set of stipulations that could be used for example by researchers wishing to anticipate and correct problems in their research proposals prior to submitting to an IRB The objective of the research was to computationally identify the most repetitive types of stipulations generated in the course of IRB deliberations The text of each stipulation was normalized using the natural language processing techniques An undirected weighted network was constructed in which each stipulation was represented by a node and each link if present had weight corresponding to the TF-IDF Cosine Similarity of the stipulations Network analysis software was then used to identify clusters in the network representing similar stipulations The final results were correlated with additional data to produce further insights about the IRB workflow From a corpus of 18582 stipulations we identified 31 types of repetitive stipulations Those types accounted for 3870 stipulations (208% of the corpus) produced for 697 (887%) of all protocols in 392 (also 887%) of all the CNS IRB meetings with stipulations entered in our data source A notable peroportion of the corrections produced by the IRB can be considered highly repetitive Our shareable method relied on a minimal manual analysis and provides an intuitive exploration with theoretically unbounded granularity Finer granularity allowed for the insight that is anticipated to prevent the need for identifying the IRB panel expertise or any human supervision',NLP
'One of humanitys most devastating health crises was COVID-19 Billions of people suffered during this pandemic In comparison with previous global pandemics that have been faced by the world before societies were more accurate with the technical support system during this natural disaster The intersection of data from healthcare units and the analysis of this data into various sophisticated systems were critical factors Different healthcare units have taken special consideration to advance technical inputs to fight against such situations The field of natural language processing (NLP) has dramatically supported this Despite the primitive methods for monitoring the bio-metric factors of a person the use of cognitive science has emerged as one of the most critical features during this pandemic era One of the essential features is the potential to understand the data based on various texts and user inputs The deployment of various NLP systems is one of the most challenging factors in handling the bulk amount of data flowing from multiple sources This study focused on developing a powerful application to advise patients suffering from ailments related to COVID-19 The use of NLP refers to facilitating a user to identify the present critical situation and make necessary decisions while getting infected This article also summarises the challenges associated with NLP and its usage for future NLP-based applications focusing on healthcare units There are a couple of applications that reside for android-based systems as well as web-based chat-bot systems In terms of security and safety application development for iOS is more advanced This study also explains the block meant of an application for advising COVID-19 infection A natural language processing powered application for an iOS operating system is indeed one of its kind which will help people who need to advise proper guidance The article also portrays NLP-based application development for healthcare problems associated with personal reporting systems',NLP
'Background High quality serious illness communication requires good understanding of patients values and beliefs for their treatment at end of life Natural Language Processing (NLP) offers a reliable and scalable method for measuring and analyzing value- and belief-related features of conversations in the natural clinical setting We use a validated NLP corpus and a series of statistical analyses to capture and explain conversation features that characterize the complex domain of moral values and beliefs The objective of this study was to examine the frequency distribution and clustering of morality lexicon expressed by patients during palliative care consultation using the Moral Foundations NLP Dictionary Methods We used text data from 231 audio-recorded and transcribed inpatient PC consultations and data from baseline and follow-up patient questionnaires at two large academic medical centers in the United States With these data we identified different moral expressions in patients using text mining techniques We used latent class analysis to explore if there were qualitatively different underlying patterns in the PC patient population We used Poisson regressions to analyze if individual patient characteristics EOL preferences religion and spiritual beliefs were associated with use of moral terminology Results We found two latent classes: a class in which patients did not use many expressions of morality in their PC consultations and one in which patients did Age race (white) education spiritual needs and whether a patient was affiliated with Christianity or another religion were all associated with membership of the first class Gender financial security and preference for longevity-focused over comfort focused treatment near EOL did not affect class membership Conclusions This study is among the first to use text data from a real-world situation to extract information regarding individual foundations of morality It is the first to test empirically if individual moral expressions are associated with individual characteristics attitudes and emotions',NLP
'Purpose: With increasing volumes of electronic health record data algorithm-driven extraction may aid manual extraction Visual acuity often is extracted manually in vision research The total visual acuity extraction algorithm (TOVA) is presented and validated for automated extraction of visual acuity from free text unstructured clinical notes Methods: Consecutive inpatient ophthalmology notes over an 8-year period from the University of Washington healthcare system in Seattle WA were used for validation of TOVA The total visual acuity extraction algorithm applied natural language processing to recognize Snellen visual acuity in free text notes and assign laterality The best corrected measurement was determined for each eye and converted to logMAR The algorithm was validated against manual extraction of a subset of notes Results: A total of 6266 clinical records were obtained giving 12452 data points In a subset of 644 validated notes comparison of manually extracted data versus TOVA output showed 95% concordance Interrater reliability testing gave kappa statistics of 094 (95% confidence interval [CI] 089-099) 096 (95% CI 094-098) 095 (95% CI 092-098) and 094 (95% CI 090-098) for acuity numerators denominators adjustments and signs respectively Pearson correlation coefficient was 0983 Linear regression showed an R-2 of 0966 (P < 00001) Conclusions: The total visual acuity extraction algorithm is a novel tool for extraction of visual acuity from free text unstructured clinical notes and provides an open source method of data extraction Translational Relevance: Automated visual acuity extraction through natural language processing can be a valuable tool for data extraction from free text ophthalmology notes',NLP
'Recent advances such as GPT BERT and RoBERTa have shown success in incorporating a pre-trained transformer language model and fine-tuning operations to improve downstream NLP systems However this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks In this study we investigate a transferable BERT (TransBERT) training framework which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks for a target task Particularly we propose utilizing three kinds of transfer tasks including natural language inference sentiment classification and next action prediction to further train BERT based on a pre-trained model This enables the model to get a better initialization for the target task We take story-ending prediction as the target task to conduct experiments The final results of 960% and 950% accuracy on two versions of Story Cloze Test datasets dramatically outperform previous state-of-the-art baseline methods Several comparative experiments give some helpful suggestions on how to select transfer tasks to improve BERT Furthermore experiments on six English and three Chinese datasets show that TransBERT generalizes well to other tasks languages and pre-trained models',NLP
'The spontaneous assembly of materials from elementary building blocks is one of the most intriguing natural phenomena Conventional modeling relies physical approaches to examine such processes In this paper a framework is proposed to offer an alternative paradigm via the use of deep learning and specifically the use of generative adversarial models as well as a combination of natural language processing and transformer neural nets to create hierarchical assemblies of building blocks We study the assembly of elementary flame particles into hierarchical materials with features across scales illustrating the Universality-Diversity Principle (UDP) and create novel material using additive manufacturing',NLP
'Students reflective writings in gross anatomy provide a rich source of complex emotions experienced by learners However qualitative approaches to evaluating student writings are resource heavy and timely To overcome this natural language processing a nascent field of artificial intelligence that uses computational techniques for the analysis and synthesis of text was used to compare health professional students reflections on the importance of various regions of the body to their own lives and those of the anatomical donor dissected A total of 1365 anonymous writings (677 about a donor 688 about self) were collected from 132 students Binary and trinary sentiment analysis was performed as well as emotion detection using the National Research Council Emotion Lexicon which classified text into eight emotions: anger fear sadness disgust surprise anticipation trust and joy The most commonly written about body regions were the hands heart and brain The reflections had an overwhelming positive sentiment with major contributing words love and loved Predominant words such as pain contributed to the negative sentiments and reflected various ailments experienced by students and revealed through dissections of the donors The top three emotions were trust joy and anticipation Each body region evoked a unique combination of emotions Similarities between student self-reflections and reflections about their donor were evident suggesting a shared view of humanization and person centeredness Given the pervasiveness of reflections in anatomy adopting a natural language processing approach to analysis could provide a rich source of new information related to students previously undiscovered experiences and competencies',NLP
'Sustainability is a major contemporary issue that affects everyone Many companies now produce an annual sustainability report mainly intended for their stakeholders and the public enumerating their goals and degrees of achievement regarding sustainable development Although sustainability reports are an important resource to understand a companys sustainability strategies and practices the difficulty of extracting key information from dozens or hundreds of pages with sustainability and business jargon has highlighted the need for metrics to effectively measure the content of such reports Accordingly many researchers have attempted to analyze the concepts and messages from sustainability reports using various natural language processing (NLP) methods In this study we propose a novel approach that overcomes the shortcomings of previous studies Using the sentence similarity method and sentiment analysis the study clearly shows thematic practices and trends as well as a significant difference in the balance of positive and negative information in the reports across companies The results of sentiment analysis prove that the new approach of this study is very useful It confirms that companies actively use the sustainability report to improve their positive image when they experience a crisis It confirms that companies actively use the sustainability report to improve their positive image when they experience a crisis The inferences gained from this method will not only help companies produce better reports that can be utilized effectively but also provide researchers with ideas for further research In the concluding section we summarize the implications of our approach and discuss limitations and future research areas',NLP
'Rationale and Objectives: Train and apply natural language processing (NLP) algorithms for automated radiology-arthroscopy correlation of meniscal tears Materials and Methods: In this retrospective single-institution study we trained supervised machine learning models (logistic regression support vector machine and random forest) to detect medial or lateral meniscus tears on free-text MRI reports We trained and evaluated model performances with cross-validation using 3593 manually annotated knee MRI reports To assess radiology-arthroscopy correlation we then randomly partitioned this dataset 80:20 for training and testing where 108 test set MRIs were followed by knee arthroscopy within 1 year These free-text arthroscopy reports were also manually annotated The NLP algorithms trained on the knee MRI training dataset were then evaluated on the MRI and arthroscopy report test datasets We assessed radiology-arthroscopy agreement using the ensembled NLP-extracted findings versus manually annotated findings Results: The NLP models showed high cross-validation performance for meniscal tear detection on knee MRI reports (medial meniscus F1 scores 093-094 lateral meniscus F1 scores 086-088) When these algorithms were evaluated on arthroscopy reports despite never training on arthroscopy reports performance was similar though higher with model ensembling (medial meniscus F1 score 097 lateral meniscus F1 score 099) However ensembling did not improve performance on knee MRI reports In the radiology-arthroscopy test set the ensembled NLP models were able to detect mismatches between MRI and arthroscopy reports with sensitivity 79% and specificity 87% Conclusion: Radiology-arthroscopy correlation can be automated for knee meniscal tears using NLP algorithms which shows promise for education and quality improvement',NLP
'Background: A new illness can come to public attention through social media before it is medically defined formally documented or systematically studied One example is a condition known as breast implant illness (BII) which has been extensively discussed on social media although it is vaguely defined in the medical literature Objective: The objective of this study is to construct a data analysis pipeline to understand emerging illnesses using social media data and to apply the pipeline to understand the key attributes of BII Methods: We constructed a pipeline of social media data analysis using natural language processing and topic modeling Mentions related to signs symptoms diseases disorders and medical procedures were extracted from social media data using the clinical Text Analysis and Knowledge Extraction System We mapped the mentions to standard medical concepts and then summarized these mapped concepts as topics using latent Dirichlet allocation Finally we applied this pipeline to understand BII Results: Our pipeline identified topics related to toxicity cancer and mental health issues that were highly associated with BII Our pipeline also showed that cancers autoimmune disorders and mental health problems were emerging concerns associated with breast implants based on social media discussions Furthermore the pipeline identified mentions such as rupture infection pain and fatigue as common self-reported issues among the public as well as concerns about toxicity from silicone implants Conclusions: Our study could inspire future studies on the suggested symptoms and factors of BII Our study provides the first analysis and derived knowledge of BII from social media using natural language processing techniques and demonstrates the potential of using social media information to better understand similar emerging illnesses',NLP
'Symptoms are common in patients on maintenance hemodialysis but identificationis challenging New informatics approaches including natural language processing (NLP) can be utilized to identify symptoms from narrative clinical documentation Here we utilized NLP to identify seven patient symptoms from notes of maintenance hemodialysis patients of the BioMe Biobank and validated our findings using a separate cohort and the MIMIC-III database NLP performance was compared for symptom detection with International Classification of Diseases (ICD)-9/10 codes and the performance of both methods were validated against manual chart review From 1034 and 519 hemodialysis patients within BioMe and MIMIC-III databases respectively the most frequently identified symptoms by NLP were fatigue pain and nausea/vomiting In BioMe sensitivity for NLP (085 - 099) was higher than for ICD codes (009 - 059) for all symptoms with similar results in the BioMe validation cohort and MIMIC-III ICD codes were significantly more specific for nausea/vomiting in BioMe and more specific for fatigue depression and pain in the MIMIC-III database A majority of patients in both cohorts had four or more symptoms Patients with more symptoms identified by NLP ICD and chart review had more clinical encounters NLP had higher specificity in inpatient notes but higher sensitivity in outpatient notes and performed similarly across pain severity subgroups Thus NLP had higher sensitivity compared to ICD codes for identification of seven common hemodialysis-related symptoms with comparable specificity between the two methods Hence NLP may be useful for the high-throughput identification of patient-centered outcomes when using electronic health records',NLP
'The metaphor of technical debt was introduced to express the trade off between productivity and quality ie when developers take shortcuts or perform quick hacks More recently our work has shown that it is possible to detect technical debt using source code comments (ie self-admitted technical debt) and that the most common types of self-admitted technical debt are design and requirement debt However all approaches thus far heavily depend on the manual classification of source code comments In this paper we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP) We study 10 open source projects: Ant ArgoUML Columba EMF Hibernate JEdit JFreeChart JMeter JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance using as little as 23 percent of the comments for both design and requirement self-admitted technical debt and 80 percent of the best performance using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt respectively The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset',NLP
'This paper describes an application-oriented examination of an automated online support process for the Center of Computing and Information Technology at The University of Arizona The online consulting activity at the Center is modeled as a set of unrelated parallel machines the consultants which process a stream of incoming jobs the online help requests Automatic Indexing a type of Natural Language processing is applied to each job to estimate its machine dependent processing time Following the time estimation scheme we develop an online scheduling procedure whose objective is to minimize the mean flow time (reflecting perceived quality of service)',NLP
'Communicating with machines in the same way we do with other people has been a long-time goal in computer science One of its many advantages would be the ability to give instructions to our computers without the need of learning how to use specific software or programming languages Since were dealing with human language it would make sense to use a model of the human brain to build a system with such capabilities In this work the Hierarchical Temporal Memory algorithms are explored and evaluated as a biologically inspired tool capable of working with natural language Its proposed that task execution can be achieved by training the algorithms to map certain sentences with keywords that correspond to the tasks Different encoders are tested that translate words into a proper representation for the algorithms The configuration of algorithms and encoders with the highest success rate is able to correctly map up to 90% of the sentences from a custom training set The behaviour of the success rates does not vary greatly between different subsets of the training set suggesting that the learning system is able to find patterns and make inferences about missing data',NLP
'The database systems course in an undergraduate computer science degree program is gaining increasing importance due to the continuous supply of database-related jobs as well as the rise of Data Science A key learning goal of learners taking such a course is to understand how SQL queries are executed in an RDBMS in practice An RDBMS typically exposes a query execution plan (QEP) in a visual or textual format which describes the execution steps for a given query However it is often daunting for a learner to comprehend these QEPS containing vendor-specific implementation details In this demonstration we present a novel generic and portable system called LANTERN that generates a natural language (NL)-based description of the execution strategy chosen by the underlying RDBMS to process a query It provides a declarative framework called POOL for subject matter experts (smE) to efficiently create and manipulate the NL descriptions of physical operators of any RDBMS It then exploits POOL to generate the NL descriptions of QEPS by integrating a rule-based and a deep learning-based techniques to infuse language variability in the descriptions Such an NL generation strategy mitigates the impact of boredom on learners caused by repeated exposure of similar text generated by a rule-based system',NLP
'Increasingly data-driven methods have been implemented to understand psychopathology Language is the main source of information in psychiatry and represents big data at the level of the individual Language and behavior are amenable to computational natural language processing (NLP) analytics which may help operationalize the mental status examination In this review we highlight the application of NLP to schizophrenia and its risk states as an exemplar of its use operationalizing tangential and concrete speech as reductions in semantic coherence and syntactic complexity respectively Other clinical applications are reviewed including forecasting suicide risk and detecting intoxication Challenges and future directions are discussed including biomarker development harmoni-zation and application of NLP more broadly to behavior including intonation/prosody facial expression and gesture and the integration of these in dyads and during discourse Similar NLP analytics can also be applied beyond humans to behavioral motifs across species important for modeling psychopathology in animal models Finally clinical neuroscience can inform the development of artificial intelligence',NLP
'NooJ is a linguistic development environment that allows formalizing complex linguistic phenomena such as compound words generation processing as well as analysis We will take advantage of NooJs linguistic engine strength in order to create a new large coverage terminological compound words dictionary for Modern Standard Arabic language Classifying and annotating Arabic compound words would have a major impact on the disambiguation of applications working with Arabic texts The diverse analyzers based on morphological aspect are not able to recognize multiword expressions Morphological analyzers usually separate compound expressions into single terms Therefore recognizing the entire compound words is essential to preserve the semantic of texts and to provide a crucial resource for a better analysis and understanding of Arabic language Our work is composed of three sections First we will deal with a literature review on Arabic compound expressions categories which aims to dress a detailed topology The structural variability of multiword expressions in Arabic language will be studied in order to measure the degree of morphological lexical and grammatical flexibility of multiword expressions Then we will discuss the electronic thematic dictionary of compound Arabic expressions and give detailed description of our methodology and guidelines',NLP
'Language understanding is one of the crucial issues both for the theoretical study of language as well as for applications developed in the domain of natural language processing As Katz (1969 p 100) puts it to understand the ability of natural languages to serve as instrument to the communication of thoughts and ideas we must understand what it is that permits those who speak them consistently to connect the right sounds with the right meanings The proper task of linguistics consists then in the description (and) explanation of the relation between the set of the semantic representations and that of the phonetic forms of utterances; at the same time among the principal difficulties there belongs a specification of the set of semantic representations (Sgall and Hajicova 1970 p 5) In our contribution we present arguments for the approach that follows the tradition of European structuralism which attempted at an account of linguistic meaning the elements of which are understood as points of intersection of conceptual contents (as a reflection of reality) and the organizing principle of the grammar of the individual language (Dokulil and Danes 1958) In other words we examine how deep the sematic representations have to be in order (i) to give an appropriate account of synonymy and (ii) to help to distinguish semantic differences in cases of ambiguity (homonymy)',NLP
'Background Acute bacterial conjunctivitis (ABC) is a relatively common medical condition caused by different pathogens Although it rarely threatens vision it is one of the most common conditions that cause red eyes and may be accompanied by discomfort and discharge The study aimed to identify and characterize inpatients with ABC treated with topical antibiotics Methods The EHRead (R) technology based on natural language processing (NLP) and machine learning was used to extract and analyze the clinical information in the electronic health records (EHRs) of antibiotic-treated patients with conjunctivitis and admitted to five hospitals in Spain between January 2014 and December 2018 Categorical variables were described by frequency whereas numerical variables included the mean standard deviation median and quartiles Results From a source population of 2071812 adult patients who attended the participating hospitals in the study period 11110 patients diagnosed with acute conjunctivitis were identified Six thousand five hundred eighty-three patients were treated with antibiotics comprising the final study population Microbiology was tested only on 121% of patients Antibiotics mainly tobramycin and corticosteroids mainly dexamethasone were usually prescribed NSAIDs were also used in about 50% of patients always combined with antibiotics Conclusions The present study provided a realistic representation of the hospital practice concerning managing patients with acute antibiotic-treated conjunctivitis The diagnosis is usually based on the clinical ground microbiology is rarely tested few bacteria species are involved and local antibiotics are frequently associated with corticosteroids and/or NSAIDs Moreover this study provided clinically relevant outcomes based on new technology that could be applied in clinical practice',NLP
'Introduction: The relationship between cannabis tobacco and vaping devices is both rapidly changing and poorly understood with consumers rapidly shifting between use of all three product types Given this dynamic and evolving landscape there is an urgent need to monitor and better understand co-use dual-use and transition patterns between these products This study describes work that utilizes social media - in this case Reddit - in conjunction with automated Natural Language Processing (NLP) methods to better understand cannabis tobacco and vaping device product usage patterns saZMethods: We collected Reddit data from the period 2013-2018 sourced from eight popular high-volume Reddit communities (subreddits) related to the three product categories We then manually annotated (coded) a set of 2640 Reddit posts and trained a machine learning-based NLP algorithm to automatically identify and disambiguate between cannabis or tobacco mentions (both smoking and vaping) in Reddit posts This classifier was then applied to all data derived from the eight subreddits 767788 posts in total Results: The NLP algorithm achieved an overall moderate performance (overall F-score of 077) When applied to our large corpus of Reddit posts we discovered that over 10% of posts in the smoking cessation subreddit r/ stopsmoking were classified as referring to vaping nicotine and that only 2% of posts from the subreddits r/ electronic_cigarette and r/vaping were classified as referring to smoking (tobacco) cessation Conclusions: This study presents the results of applying an NLP algorithm designed to identify and distinguish between cannabis and tobacco mentions (both smoking and vaping) in Reddit posts hence contributing to our currently limited understanding of co-use dual-use and transition patterns between these products',NLP
'Medication errors often occurred due to the breach of medication rights that are the right patient the right drug the right time the right dose and the right route The aim of this study was to develop a medication-rights detection system using natural language processing and deep neural networks to automate medication-incident identification using free-text incident reports We assessed the performance of deep neural network models in classifying the Advanced Incident Reporting System reports and compared the models performance with that of other common classification methods (including logistic regression support vector machines and the decision-tree method) We also evaluated the effects on prediction outcomes of several deep neural network model settings including number of layers number of neurons and activation regularisation functions The accuracy of the models was measured at 09 or above across model settings and algorithms The average values obtained for accuracy and area under the curve were 0940 (standard deviation: 0011) and 0911 (standard deviation: 0019) respectively It is shown that deep neural network models were more accurate than the other classifiers across all of the tested class labels (including wrong patient wrong drug wrong time wrong dose and wrong route) The deep neural network method outperformed other binary classifiers and our default base case model and parameter arguments setting generally performed well for the five medication-rights datasets The medication-rights detection system developed in this study successfully uses a natural language processing and deep-learning approach to classify patient-safety incidents using the Advanced Incident Reporting System reports which may be transferable to other mandatory and voluntary incident reporting systems worldwide',NLP
'Chinese new words and their part-of-speech (PUS) are particularly problematic in Chinese natural language processing With the fast development of internet and information technology it is impossible to get a complete system dictionary for Chinese natural language processing as new words out of the basic system dictionary are always being created A latent semi-CRF model which combines the strengths of LDCRF (Latent-Dynamic Conditional Random Field) and semi-CRF is proposed to detect the new words together with their PUS synchronously regardless of the types of the new words from the Chinese text without being pre-segmented Unlike the original semi-CRF the LDCRF is applied to generate the candidate entities for training and testing the latent semi-CRF which accelerates the training speed and decreases the computation cost The complexity of the latent semi-CRF could be further adjusted by tuning the number of hidden variables in LDCRF and the number of the candidate entities from the Nbest outputs of the LDCRF A new-words-generating framework is proposed for model training and testing under which the definitions and distributions of the new words conform to the ones existing in real text Specific features called Global Fragment Information for new word detection and PUS tagging are adopted in the model training and testing The experimental results show that the proposed method is capable of detecting even low frequency new words together with their PUS tags The proposed model is found to be performing competitively with the state-of-the-art models presented',NLP
'Objective: The purpose of this study was to develop a natural language processing (NLP) pipeline to identify incidental thyroid nodules (ITNs) meeting criteria for sonographic follow-up and to assess both adherence rates to white paper recommendations and downstream outcomes related to these incidental findings Methods: 21583 non-contrast chest CT reports from 2017 and 2018 were retrospectively evaluated to identify reports which included either an explicit recommendation for thyroid ultrasound a description of a nodule > 15 cm or description of a nodule with suspicious features Reports from 2018 were used to train an NLP algorithm called fastText for automated identification of such reports Algorithm performance was then evaluated on the 2017 reports Next any patient from 2017 with a report meeting criteria for ultrasound follow-up was further evaluated with manual chart review to determine follow-up adherence rates and nodule-related outcomes Results: NLP identified reports with ITNs meeting criteria for sonographic follow-up with an accuracy of 965% (95% CI 962-967) and sensitivity of 921% (95% CI 898-943) In 10006 chest CTs from 2017 ITN follow-up ultrasound was indicated according to white paper criteria in 81 patients (08%) explicitly recommended in 469% (38/81) of patients and obtained in less than half of patients in which it was appropriately recommended (17/35 486%) Discussion: NLP accurately identified chest CT reports meeting criteria for ITN ultrasound follow-up Radiologist adherence to white paper guidelines and subsequent referrer adherence to radiologist recommendations showed room for improvement',NLP
'After years of advocacy and international negotiation the General Assembly of the United Nations voted to officially recognize a stand-alone human right to water and sanitation on 28 July 2010 Since academic scholarship has continued to grow in an effort to understand the implications of the codification of this human right Yet with this growth it has become impractical if not impossible for scholars to keep up with the advancement of academic knowledge or to make sense of it in a systematic way In short to date we know very little about the trends in the literature as they have unfolded over the past thirty years and the topics to which scholars have devoted significant attention within the broader field particularly over time This is an important area of inquiry as developing a comprehensive understanding of where prior literature has focused and where it appears to be going offers scholars an opportunity to identify areas in need of refinement and/or increased attention Given the practicalities of reading thousands of research papers each year this project utilizes natural language processing (NLP) to identify topics and trends in academic literature on the human right to water and sanitation (HRtWS) NLP provides the opportunity to digest large quantities of text data through machine learning culminating with descriptive information on trends and topics in the field since 1990 The results of this exercise show that the research related to the human right to water and sanitation has grown exponentially particularly over the last decade illustrates the multidisciplinary nature of the literature and demonstrates the diversity of topics in the field',NLP
'Background: Experiences with psychedelic drugs such as psilocybin or lysergic acid diethylamide (LSD) are sometimes followed by changes in patterns of tobacco opioid and alcohol consumption But the specific characteristics of psychedelic experiences that lead to changes in drug consumption are unknown Objective: Determine whether quantitative descriptions of psychedelic experiences derived using Natural Language Processing (NLP) would allow us to predict who would quit or reduce using drugs following a psychedelic experience Methods: We recruited 1141 individuals (247 female 894 male) from online social media platforms who reported quitting or reducing using alcohol cannabis opioids or stimulants following a psychedelic experience to provide a verbal narrative of the psychedelic experience they attributed as leading to their reduction in drug use We used NLP to derive topic models that quantitatively described each participants psychedelic experience narrative We then used the vector descriptions of each participants psychedelic experience narrative as input into three different supervised machine learning algorithms to predict long-term drug reduction outcomes Results: We found that the topic models derived through NLP led to quantitative descriptions of participant narratives that differed across participants when grouped by the drug class quit as well as the long-term quit/reduction outcomes Additionally all three machine learning algorithms led to similar prediction accuracy (similar to 65% CI = +/- 021%) for long-term quit/reduction outcomes Conclusions: Using machine learning to analyze written reports of psychedelic experiences may allow for accurate prediction of quit outcomes and what drug is quit or reduced within psychedelic therapy',NLP
'Structured reporting is a favorable and sustainable form of reporting in radiology Among its advantages are better presentation clearer nomenclature and higher quality By using MRRT-compliant templates the content of the categorized items (eg select fields) can be automatically stored in a database which allows further research and quality analytics based on established ontologies like RadLex (R) linked to the items Additionally it is relevant to provide free-text input for descriptions of findings and impressions in complex imaging studies or for the information included with the clinical referral So far however this unstructured content cannot be categorized We developed a solution to analyze and code these free-text parts of the templates in our MRRT-compliant reporting platform using natural language processing (NLP) with RadLex (R) terms in addition to the already categorized items The established hybrid reporting concept is working successfully The NLP tool provides RadLex (R) codes with modifiers (affirmed speculated negated) Radiologists can confirm or reject codes provided by NLP before finalizing the structured report Furthermore users can suggest RadLex (R) codes from free text that is not correctly coded with NLP or can suggest to change the modifier Analyzing free-text fields took 123 s on average Hybrid reporting enables coding of free-text information in our MRRT-compliant templates and thus increases the amount of categorized data that can be stored in the database This enhances the possibilities for further analyses such as correlating clinical information with radiological findings or storing high-quality structured information for machine-learning approaches',NLP
'The long noncoding RNAs (lncRNAs) are ubiquitous in organisms and play crucial role in a variety of biological processes and complex diseases Emerging evidences suggest that lncRNAs interact with corresponding proteins to perform their regulatory functions Therefore identifying interacting lncRNA-protein pairs is the first step in understanding the function and mechanism of lncRNA Since it is time-consuming and expensive to determine lncRNA-protein interactions by high-throughput experiments more robust and accurate computational methods need to be developed In this study we developed a new sequence distributed representation learning based method for potential lncRNA-Protein Interactions Prediction named LPI-Pred which is inspired by the similarity between natural language and biological sequences More specifically lncRNA and protein sequences were divided into k-mer segmentation which can be regard as word in natural language processing Then we trained out the RNA2vec and Pro2vec model using word2vec and human genome-wide lncRNA and protein sequences to mine distribution representation of RNA and protein Then the dimension of complex features is reduced by using feature selection based on Gini information impurity measure Finally these discriminative features are used to train a Random Forest classifier to predict lncRNA-protein interactions Five-fold cross-validation was adopted to evaluate the performance of LPI-Pred on three benchmark datasets including RPI369 RPI488 and RPI2241 The results demonstrate that LPI-Pred can be a useful tool to provide reliable guidance for biological research (C) 2019 The Authors Published by Elsevier BV on behalf of Research Network of Computational and Structural Biotechnology',NLP
'The availability of an abundance of knowledge sources has spurred a large amount of effort in the development and enhancement of Information Retrieval techniques Users information needs are expressed in natural language and successful retrieval is very much dependent on the effective communication of the intended purpose Natural language queries consist of multiple linguistic features which serve to represent the intended search goal Linguistic characteristics that cause semantic ambiguity and misinterpretation of queries as well as additional factors such as the lack of familiarity with the search environment affect the users ability to accurately represent their information needs coined by the concept intention gap The latter directly affects the relevance of the returned search results which may not be to the users satisfaction and therefore is a major issue impacting the effectiveness of information retrieval systems Central to our discussion is the identification of the significant constituents that characterize the query intent and their enrichment through the addition of meaningful terms phrases or even latent representations either manually or automatically to capture their intended meaning Specifically we discuss techniques to achieve the enrichment and in particular those utilizing the information gathered from statistical processing of term dependencies within a document corpus or from external knowledge sources such as ontologies We lay down the anatomy of a generic linguistic based query expansion framework and propose its module-based decomposition covering topical issues from query processing information retrieval computational linguistics and ontology engineering For each of the modules we review state-of-the-art solutions in the literature categorized and analyzed under the light of the techniques used',NLP
'Some techniques such as Inception Deck are used in software elicitation phase of the agile methodologies to unify the vision of all the stakeholders This vision is stored in artifacts such as user stories However these artifacts are written in natural language and may therefore be ambiguous Regarding SCRUM the primary artifact is the product backlog which in turn contains user stories This paper describes how software development process with SCRUM can be improved by replacing the user stories with business process models to solve the ambiguity issue',NLP
'Machine learning (ML) models - like deep neural networks require substantial amounts of training data Also the training dataset should be properly annotated to obtain satisfactory results This paper describes a platform designed to create high-quality datasets By using data workflows adapted for speech technologies and natural language processing systems the user can collect and enrich speech and text data Depending on the end goal the data is passed through multiple processing steps based on human input and ML services To guarantee data quality the platform combines several mechanisms like language tests real-time audits and user behavior into several ML models that act as quality gateways',NLP
'Purpose Natural language processing (NLP) can be used for automatic flagging of radiology reports We assessed deep learning models for classifying non-English head CT reports Methods We retrospectively collected head CT reports (2011-2018) Reports were signed in Hebrew Emergency department (ED) reports of adult patients from January to February for each year (2013-2018) were manually labeled All other reports were used to pre-train an embedding layer We explored two use cases: (1) general labeling use case in which reports were labeled as normal vs pathological; (2) specific labeling use case in which reports were labeled as with and without intra-cranial hemorrhage We tested long short-term memory (LSTM) and LSTM-attention (LSTM-ATN) networks for classifying reports We also evaluated the improvement of adding Word2Vec word embedding Deep learning models were compared with a bag-of-words (BOW) model Results We retrieved 176988 head CT reports for pre-training We manually labeled 7784 reports as normal (463%) or pathological (537%) and 71% with intra-cranial hemorrhage For the general labeling LSTM-ATN-Word2Vec showed the best results (AUC = 0967 +/- 0006 accuracy 908% +/- 001) For the specific labeling all methods showed similar accuracies between 950 and 959% Both LSTM-ATN-Word2Vec and BOW had the highest AUC (0970) Conclusion For a general use case word embedding using a large cohort of non-English head CT reports and ATN improves NLP performance For a more specific task BOW and deep learning showed similar results Models should be explored and tailored to the NLP task',NLP
'Intangible cultural heritage (ICH) is a precious historical and cultural resource of a country Protection and inheritance of ICH is important to the sustainable development of national culture There are many different intangible cultural heritage items in China With the development of information technology ICH database resources were built by government departments or public cultural services institutions but most databases were widely dispersed Certain traditional database systems are disadvantageous to storage management and analysis of massive data At the same time a large quantity of data has been produced accompanied by digital intangible cultural heritage development The public is unable to grasp key knowledge quickly because of the massive and fragmented nature of the data To solve these problems we proposed the intangible cultural heritage knowledge graph to assist knowledge management and provide a service to the public ICH domain ontology was defined with the help of intangible cultural heritage experts and knowledge engineers to regulate the concept attribute and relationship of ICH knowledge In this study massive ICH data were obtained and domain knowledge was extracted from ICH text data using the Natural Language Processing (NLP) technology A knowledge base based on domain ontology and instances for Chinese intangible cultural heritage was constructed and the knowledge graph was developed The pattern and characteristics behind the intangible cultural heritage were presented based on the ICH knowledge graph The knowledge graph for ICH could foster support for organization management and protection of the intangible cultural heritage knowledge The public can also obtain the ICH knowledge quickly and discover the linked knowledge The knowledge graph is helpful for the protection and inheritance of intangible cultural heritage',NLP
'Uninhabited mistakes while writing happens are unstoppable There are certain common errors that occur during writing such as missing letters extra letters disordered letters and misspelled letters These kind of common spelling errors are called phonetics spelling errors These are of a major concern while dealing with phonetics Out of various problems that the phoneticians are trying to solve major portion of it concentrates on varieties of spelling errors Phonetic structures are greatly emphasized based on the effectiveness appropriateness and accuracy In order to keep abreast with the changing and challenging trends of Natural Language Processing (NLP) it is of great importance that one should resolve the problems of spelling errors To achieve the goal numerous realistic and practical approaches have to be adopted that make use of spelling correction algorithms such as Edit distance Habit distance Soundex and Asoundex Through the analysis of these algorithms a new interface is put forward that calculates the Edit distance thereby showing the overall comparative study of phonetic algorithms with the proposed modified Edit Distance algorithm The interface computes the Edit distance between two strings in appropriate and intuitive way contemplating with the comparisons shown in the distance table The Results show that an average of 0937 recall and 0947 precision have been achieved with the F-measure 09417 Through these results it is evident that the recall and F-measures are improved in the proposed Edit-Distance algorithm The revised version of the edit distance algorithm consistently attains finer quality results in comparison with the traditional edit distance algorithm',NLP
'Current research in natural language processing is highly dependent on carefully produced corpora Most existing resources focus on English; some resources focus on languages such as Chinese and French; few resources deal with more than one language This paper presents the Pira dataset a large set of questions and answers about the ocean and the Brazilian coast both in Portuguese and English Pira is to the best of our knowledge the first QA dataset with supporting texts in Portuguese and perhaps more importantly the first bilingual QA dataset that includes this language The Pira dataset consists of 2261 properly curated question/answer (QA) sets in both languages The QA sets were manually created based on two corpora: abstracts related to the Brazilian coast and excerpts of United Nation reports about the ocean The QA sets were validated in a peer-review process with the dataset contributors We discuss some of the advantages as well as limitations of Pira as this new resource can support a set of tasks in NLP such as question-answering information retrieval and machine translation',NLP
'This paper proposes a neural generative model namely Table2Seq to generate a natural language sentence based on a table Specifically the model maps a table to continuous vectors and then generates a natural language sentence by leveraging the semantics of a table Since rare words eg entities and values usually appear in a table we develop a flexible copying mechanism that selectively replicates contents from the table to the output sequence We conduct extensive experiments to demonstrate the effectiveness of our Table2Seq model and the utility of the designed copying mechanism On the WIKIBIO and SIMPLE QUESTIONS datasets the Table2Seq model improves the stateof-the-art results from 3470 to 4026 and from 3332 to 3912 in terms of BLEU-4 scores respectively Moreover we construct an open-domain dataset WIKITABLETEXT that includes 13 318 descriptive sentences for 4962 tables Our Table2Seq model achieves a BLEU-4 score of 3823 on WIKITABLETEXT outperforming template-based and language model based approaches Furthermore through experiments on 1 M table-query pairs from a search engine our Table2Seq model considering the structured part of a table ie table attributes and table cells as additional informationoutperforms a sequence-to-sequence model considering only the sequential part of a table ie table caption',NLP
'Background Falls in older people are common and morbid Prediction models can help identifying individuals at higher fall risk Electronic health records (EHR) offer an opportunity to develop automated prediction tools that may help to identify fall-prone individuals and lower clinical workload However existing models primarily utilise structured EHR data and neglect information in unstructured data Using machine learning and natural language processing (NLP) we aimed to examine the predictive performance provided by unstructured clinical notes and their incremental performance over structured data to predict falls Methods We used primary care EHR data of people aged 65 or over We developed three logistic regression models using the least absolute shrinkage and selection operator: one using structured clinical variables (Baseline) one with topics extracted from unstructured clinical notes (Topic-based) and one by adding clinical variables to the extracted topics (Combi) Model performance was assessed in terms of discrimination using the area under the receiver operating characteristic curve (AUC) and calibration by calibration plots We used 10-fold cross-validation to validate the approach Results Data of 35357 individuals were analysed of which 4734 experienced falls Our NLP topic modelling technique discovered 151 topics from the unstructured clinical notes AUCs and 95% confidence intervals of the Baseline Topic-based and Combi models were 0709 (0700-0719) 0685 (0676-0694) and 0718 (0708-0727) respectively All the models showed good calibration Conclusions Unstructured clinical notes are an additional viable data source to develop and improve prediction models for falls compared to traditional prediction models but the clinical relevance remains limited',NLP
'Purpose Data extraction from radiology free-text reports is time consuming when performed manually Recently more automated extraction methods using natural language processing (NLP) are proposed A previously developed rule-based NLP algorithm showed promise in its ability to extract stroke-related data from radiology reports We aimed to externally validate the accuracy of CHARTextract a rule-based NLP algorithm to extract stroke-related data from free-text radiology reports Methods Free-text reports of CT angiography (CTA) and perfusion (CTP) studies of consecutive patients with acute ischemic stroke admitted to a regional stroke center for endovascular thrombectomy were analyzed from January 2015 to 2021 Stroke-related variables were manually extracted as reference standard from clinical reports including proximal and distal anterior circulation occlusion posterior circulation occlusion presence of ischemia or hemorrhage Alberta stroke program early CT score (ASPECTS) and collateral status These variables were simultaneously extracted using a rule-based NLP algorithm The NLP algorithms accuracy specificity sensitivity positive predictive value (PPV) and negative predictive value (NPV) were assessed Results The NLP algorithms accuracy was > 90% for identifying distal anterior occlusion posterior circulation occlusion hemorrhage and ASPECTS Accuracy was 85% 74% and 79% for proximal anterior circulation occlusion presence of ischemia and collateral status respectively The algorithm confirmed the absence of variables from radiology reports with an 87-100% accuracy Conclusions Rule-based NLP has a moderate to good performance for stroke-related data extraction from free-text imaging reports The algorithms accuracy was affected by inconsistent report styles and lexicon among reporting radiologists',NLP
'Clinical coding is an important task which is required for accurate activity-based funding Natural language processing may be able to assist with improving the efficiency and accuracy of clinical coding The aims of this study were to explore the feasibility of using natural language processing for stroke hospital admissions employed with open-source software libraries to aid in the identification of potentially misclassified (1) category of Adjacent Diagnosis Related Groups (ADRG) (2) the International Statistical Classification of Diseases and Related Health Problems Tenth Revision Australian Modification (ICD-10AM) diagnoses and (3) Diagnosis Related Groups (DRG) Data was collected for consecutive individuals admitted to the Royal Adelaide Hospital Stroke Unit over a five-month period for misclassification identification analysis 152 admissions were included in the study Using free-text discharge summaries a random forest classifier correctly identified two cases classified as B70 (Stroke and Other Cerebrovascular Disorders) that should be classified as B02 (having received endovascular thrombectomy) A regular expression-based analysis correctly identified 33 cases in which ataxia was present but was not coded Two cases were identified that should have been classified as B70D rather than B70A/B/C based on transfer to another centre within five days of admission A variety of techniques may be useful to help identify misclassifications in ADRG ICD-10-AM and DRG codes Such techniques can be implemented with open-source software libraries and may have significant financial implications Future studies may seek to apply open-source software libraries to the identification of misclassifications of all ICD-10-AM diagnoses in stroke patients (c) 2021 Elsevier Ltd All rights reserved',NLP
'Study Design: Retrospective study Objectives: Huge amounts of images and medical reports are being generated in radiology departments While these datasets can potentially be employed to train artificial intelligence tools to detect findings on radiological images the unstructured nature of the reports limits the accessibility of information In this study we tested if natural language processing (NLP) can be useful to generate training data for deep learning models analyzing planar radiographs of the lumbar spine Methods: NLP classifiers based on the Bidirectional Encoder Representations from Transformers (BERT) model able to extract structured information from radiological reports were developed and used to generate annotations for a large set of radiographic images of the lumbar spine (N = 10 287) Deep learning (ResNet-18) models aimed at detecting radiological findings directly from the images were then trained and tested on a set of 204 human-annotated images Results: The NLP models had accuracies between 088 and 098 and specificities between 084 and 099; 7 out of 12 radiological findings had sensitivity >090 The ResNet-18 models showed performances dependent on the specific radiological findings with sensitivities and specificities between 053 and 093 Conclusions: NLP generates valuable data to train deep learning models able to detect radiological findings in spine images Despite the noisy nature of reports and NLP predictions this approach effectively mitigates the difficulties associated with the manual annotation of large quantities of data and opens the way to the era of big data for artificial intelligence in musculoskeletal radiology',NLP
'Introduction Real-world disease management of atopic dermatitis (AD) is hampered by a lack of consistency between providers that treat AD regarding assessment of severity disease activity and quality of life Variability and inconsistency in documentation makes it difficult to understand the impact of AD This study summarizes AD-related symptoms and concerns captured in unstructured qualitative provider notes by healthcare providers during visits with patients with AD Methods Provider notes were obtained for patients with AD (n = 133025) from a USA-based ambulatory electronic health records system The sample included both children (n = 69551) and adults at least 18 years of age (n = 63474) receiving treatment from a variety of specialties including primary care dermatology and allergy/immunology Key skin-related words were identified from a review of a sample of notes and natural language processing (NLP) was applied to determine the frequency of the keywords and bigram patterns Results Provider notes largely focused on symptoms (primarily itch) and symptom relief rather than the impact of AD on work or lifestyle Despite the known relationship between itch and skin pain neuralgia was not widely documented Compared to primary care providers dermatologists and allergist/immunologists notes had more documentation of symptom-related issues Personal and work/life burden issues were not widely documented regardless of specialty Conclusion The topics documented in case notes by healthcare providers about their patients with AD focus largely on symptoms and to a lesser extent treatment but do not reflect the burden of AD on patients lives This finding highlights a potential care gap that warrants further investigation',NLP
'The coronavirus disease 2019 (COVID-19) breaking out in late December 2019 is gradually being controlled in China but it is still spreading rapidly in many other countries and regions worldwide It is urgent to conduct prediction research on the development and spread of the epidemic In this article a hybrid artificial-intelligence (AI) model is proposed for COVID-19 prediction First as traditional epidemic models treat all individuals with coronavirus as having the same infection rate an improved susceptible-infected (ISI) model is proposed to estimate the variety of the infection rates for analyzing the transmission laws and development trend Second considering the effects of prevention and control measures and the increase of the publics prevention awareness the natural language processing (NLP) module and the long short-term memory (LSTM) network are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction The experimental results on the epidemic data of several typical provinces and cities in China show that individuals with coronavirus have a higher infection rate within the third to eighth days after they were infected which is more in line with the actual transmission laws of the epidemic Moreover compared with the traditional epidemic models the proposed hybrid AI model can significantly reduce the errors of the prediction results and obtain the mean absolute percentage errors (MAPEs) with 052% 038% 005% and 086% for the next six days in Wuhan Beijing Shanghai and countrywide respectively',NLP
'Automatic image captioning is a challenging issue in artificial intelligence which covers both the fields of computer vision and natural language processing Inspired by the later advances in machine translation a successful encoder-decoder technique is currently the state-of-the-art in English language captioning In this study we proposed an image captioning model for Turkish Language This paper evaluates the encoder-decoder model on MS COCO database by coupling an encoder Convolutional Neural Network (CNN) -the component that is responsible for extracting the features of the given images- with a decoder Recurrent Neural Network (RNN) -the component that is responsible for generating captions using the given inputs- to generate Turkish captions We conducted the experiments using the most common evaluation metrics such as BLEU METEOR ROUGE and CIDEr Results show that the performance of the proposed model is satisfactory in both qualitative and quantitative evaluations Finally this study introduces a Web platform (http://mscoco-contributorherokuappcom/website/) which is proposed to improve the dataset via crowd-sourcing and free to use The Turkish MS COCO dataset is available for research purpose When all the images are completed a Turkish dataset will be available for comparative studies',NLP
'Background and objective In order for computers to extract useful information from unstructured text a concept normalization system is needed to link relevant concepts in a text to sources that contain further information about the concept Popular concept normalization tools in the biomedical field are dictionary-based In this study we investigate the usefulness of natural language processing (NLP) as an adjunct to dictionary-based concept normalization Methods We compared the performance of two biomedical concept normalization systems MetaMap and Peregrine on the Arizona Disease Corpus with and without the use of a rule-based NLP module Performance was assessed for exact and inexact boundary matching of the system annotations with those of the gold standard and for concept identifier matching Results Without the NLP module MetaMap and Peregrine attained F-scores of 610% and 639% respectively for exact boundary matching and 551% and 569% for concept identifier matching With the aid of the NLP module the F-scores of MetaMap and Peregrine improved to 733% and 780% for boundary matching and to 662% and 698% for concept identifier matching For inexact boundary matching performances further increased to 855% and 854% and to 736% and 733% for concept identifier matching Conclusions We have shown the added value of NLP for the recognition and normalization of diseases with MetaMap and Peregrine The NLP module is general and can be applied in combination with any concept normalization system Whether its use for concept types other than disease is equally advantageous remains to be investigated',NLP
'Objectives To create an advanced image retrieval and data-mining system based on in-house radiology reports Methods Radiology reports are semantically analysed using natural language processing (NLP) techniques and stored in a state-of-the-art search engine Images referenced by sequence and image number in the reports are retrieved from the picture archiving and communication system (PACS) and stored for later viewing A web-based front end is used as an interface to query for images and show the results with the retrieved images and report text Using a comprehensive radiological lexicon for the underlying terminology the search algorithm also finds results for synonyms abbreviations and related topics Results The test set was 108 manually annotated reports analysed by different system configurations Best results were achieved using full syntactic and semantic analysis with a precision of 0929 and recall of 0952 Operating successfully since October 2010 258824 reports have been indexed and a total of 405146 preview images are stored in the database Conclusions Data-mining and NLP techniques provide quick access to a vast repository of images and radiology reports with both high precision and recall values Consequently the system has become a valuable tool in daily clinical routine education and research Key Points  Radiology reports can now be analysed using sophisticated natural language-processing techniques  Semantic text analysis is backed by terminology of a radiological lexicon  The search engine includes results for synonyms abbreviations and compositions  Key images are automatically extracted from radiology reports and fetched from PACS  Such systems help to find diagnoses improve report quality and save time',NLP
'We discuss the use of probability-based natural language processing for Chinese text retrieval We focus on comparing different text extraction methods and probabilistic weighting methods Several document processing methods and probabilistic weighting functions are presented A number of experiments have been conducted on large standard text collections We present the experimental results that compare a word-based text processing method with a character-based method The experimental results also compare a number of term-weighting functions including both single-unit weighting and compound-unit weighting functions',NLP
'This paper describes a method to automatically discover features which distinguish the language use of cultural subgroups operating within the same broader language/culture Sociolinguists have long known that special features such as vocabulary use phonetic features (like accents) and syntactic characteristics develop within the in-group language of frequently interacting subgroups These features set apart the groups language from the discourse of others speaking the same broader language Our interest is to learn these features automatically and use them to distinguish the writing of one subgroup from another The special vocabulary and jargon of various subgroups has often been catalogued This research focuses instead on syntactic differences which can be learned from digital text and the specialized use of vocabulary which is not topic or domain specific (e g we deliberately omit domain related jargon) Our main data source is blogs and related discussions from a number of North American subculture groups such as radical feminists and militia groups In this paper we present our findings on looking for blogs whose participants have a particular subcultural affiliation designated as blogs of interest Our hypothesis is that we can ignore the particular topic of a blog discussion through means described in the paper and isolate other linguistic indicators that help us determine whether or not a blog is of interest We start with an overview of the process of training our system and describe its use in identifying blogs of the desired cultural subgroup We then describe in detail the training process in which a series of grams are scored and aggregated to find key highly indicative blog passages The last section reports on an experiment we conducted that proved the concept against several North American English language blogging communities',NLP
'In this study we attempted to develop a method that applies the notion and technology of natural language processing for operating a function dividing process in conceptual design We formulated a function dividing process from a linguistic viewpoint and constructed linguistic hierarchal structures in this process This method is significant in identifying hierarchal relationships between the upper- and lower-level functions from the viewpoint of linguistic hierarchal relations An experiment was carried out to confirm whether the proposed methods were feasible and whether the extracted relations were meaningful for supporting the function dividing process [DOI: 101115/13467008]',NLP
'The development of large language models (LLMs) is a recent success in the field of generative artificial intelligence (AI) They are computer models able to perform a wide range of natural language processing tasks including content generation question answering or language translation In recent months a growing number of studies aimed to assess their potential applications in the field of medicine including cancer care In this mini review we described the present published evidence for using LLMs in oncology All the available studies assessed ChatGPT an advanced language model developed by OpenAI alone or compared to other LLMs such as Google Bard Chatsonic and Perplexity Although ChatGPT could provide adequate information on the screening or the management of specific solid tumors it also demonstrated a significant error rate and a tendency toward providing obsolete data Therefore an accurate expert-driven verification process remains mandatory to avoid the potential for misinformation and incorrect evidence Overall although this new generative AI-based technology has the potential to revolutionize the field of medicine including that of cancer care it will be necessary to develop rules to guide the application of these tools to maximize benefits and minimize risks',NLP
'Detecting serial crimes is one of the most challenging tasks in crime analysis Linking crimes committed by the same criminal can improve the work efficiency of police offices and maintain public safety Previous crime linkage studies have focused on the crime features of modus operandi (MO) but did not address the crime process In this paper we proposed an approach for detecting serial robbery crimes based on understanding offender MO by integrating crime process information According to the crime narrative text a natural language processing method is used to extract the action and object characteristics of the crime process a dynamic time warping method was introduced in the similarity measurement of these characteristics and an information entropy method was used to weight the similarity of the action and object characteristics to obtain the comprehensive similarity of criminals crime process A real-world robbery dataset is employed to measure the performance of finding serial crimes after adding the crime process information According to the results information about the crime process obtained from the case narrative text has significant separability and can better characterize better the offenders MO Five machine learning algorithms are used to classify the case pairs and identify serial cases and nonserial cases Based on the crime features the results show that the addition of crime process information can substantially improve the effect of detecting serial crimes (C) 2019 Elsevier BV All rights reserved',NLP
'Background: People with Alzheimers disease (AD) often demonstrate difficulties in discourse production Referential communication tasks (RCTs) are used to examine a speakers capability to select and verbally code the characteristics of an object in interactive conversation Objective: In this study we used contextualized word representations from Natural language processing (NLP) to evaluate how well RCTs are able to distinguish between people with AD and cognitively healthy older adults Methods: We adapted machine learning techniques to analyze manually transcribed speech transcripts in an RCT from 28 older adults including 12 with AD and 16 cognitively healthy older adults Two approaches were applied to classify these speech transcript samples: 1) using clinically relevant linguistic features 2) using machine learned representations derived by a state-of-art pretrained NLP transfer learning model Bidirectional Encoder Representation from Transformer (BERT) based classification model Results: The results demonstrated the superior performance of AD detection using a designed transfer learning NLP algorithm Moreover the analysis showed that transcripts of a single image yielded high accuracies in AD detection Conclusion: The results indicated that RCT may be useful as a diagnostic tool for AD and that the task can be simplified to a subset of images without significant sacrifice to diagnostic accuracy which can make RCT an easier and more practical tool for AD diagnosis The results also demonstrate the potential of RCT as a tool to better understand cognitive deficits from the perspective of discourse production in people with AD',NLP
'Background: Social media data may be especially effective for studying diseases associated with high stigma such as Alzheimers disease (AD) Objective: We primarily aimed to identify issues/challenges experienced by patients with AD using natural language processing (NLP) of social media posts Methods: We searched 130 public social media sources between January 1998 and December 2021 for AD stakeholder social media posts using NLP to identify issues/challenges experienced by patients with AD Issues/challenges identified by >= 10% of any AD stakeholder type were described Illustrative posts were selected for qualitative review Secondarily issues/challenges were organized into a conceptual AD identification framework (ADIF) and representation of ADIF categories within clinical instruments was assessed Results: We analyzed 1859077 social media posts from 30341 AD stakeholders (21011 caregivers; 7440 clinicians; 1890 patients) The most common issues/challenges were Worry/anxiety (342%) Pain (33%) Malaise (287%) Confusional state (271%) and Falls (239%) Patients reported a markedly higher volume of issues/challenges than other stakeholders Patient posts reflected the broader scope of patient burden caregiver posts captured both patient and caregiver burden and clinician posts tended to be targeted Less than 5% of the high frequency issues/challenges were in the function and independence and social and relational well-being categories of the ADIF suggesting these issues/challenges may be difficult to capture No single clinical instrument covered all ADIF categories; social and relational well-being was least represented Conclusion: NLP of AD stakeholder social media data revealed a broad spectrum of real-world insights regarding patient burden',NLP
'Background: We developed an accurate stakeholder-informed automated natural language processing (NLP) system to measure the quality of heart failure (HF) inpatient care and explored the potential for adoption of this system within an integrated health care system Objective: To accurately automate a United States Department of Veterans Affairs (VA) quality measure for inpatients with HF Methods: We automated the HF quality measure Congestive Heart Failure Inpatient Measure 19 (CHI19) that identifies whether a given patient has left ventricular ejection fraction (LVEF) <40% and if so whether an angiotensin-converting enzyme inhibitor or angiotensin-receptor blocker was prescribed at discharge if there were no contraindications We used documents from 1083 unique inpatients from eight VA medical centers to develop a reference standard (RS) to train (n=314) and test (n=769) the Congestive Heart Failure Information Extraction Framework (CHIEF) We also conducted semi-structured interviews (n=15) for stakeholder feedback on implementation of the CHIEF Results: The CHIEF classified each hospitalization in the test set with a sensitivity (SN) of 989% and positive predictive value of 987% compared with an RS and SN of 985% for available External Peer Review Program assessments Of the 1083 patients available for the NLP system the CHIEF evaluated and classified 100% of cases Stakeholders identified potential implementation facilitators and clinical uses of the CHIEF Conclusions: The CHIEF provided complete data for all patients in the cohort and could potentially improve the efficiency timeliness and utility of HF quality measurements',NLP
'Objective:Social media is an important pharmacovigilance data source for adverse drug reaction (ADR) identification Human review of social media data is infeasible due to data quantity thus natural language processing techniques are necessary Social media includes informal vocabulary and irregular grammar which challenge natural language processing methods Our objective is to develop a scalable deep-learning approach that exceeds state-of-the-art ADR detection performance in social media Materials and Methods:We developed a recurrent neural network (RNN) model that labels words in an input sequence with ADR membership tags The only input features are word-embedding vectors which can be formed through task-independent pretraining or during ADR detection training Results:Our best-performing RNN model used pretrained word embeddings created from a large non-domain-specific Twitter dataset It achieved an approximate match F-measure of 0755 for ADR identification on the dataset compared to 0631 for a baseline lexicon system and 065 for the state-of-the-art conditional random field model Feature analysis indicated that semantic information in pretrained word embeddings boosted sensitivity and combined with contextual awareness captured in the RNN precision Discussion:Our model required no task-specific feature engineering suggesting generalizability to additional sequence-labeling tasks Learning curve analysis showed that our model reached optimal performance with fewer training examples than the other models Conclusion:ADR detection performance in social media is significantly improved by using a contextually aware model and word embeddings formed from large unlabeled datasets The approach reduces manual data-labeling requirements and is scalable to large social media datasets',NLP
'Recently sentiment classification has received considerable attention within the natural language processing research community However since most recent works regarding sentiment classification have been done in the English language there are accordingly not enough sentiment resources in other languages Manual construction of reliable sentiment resources is a very difficult and time-consuming task Cross-lingual sentiment classification aims to utilize annotated sentiment resources in one language (typically English) for sentiment classification of text documents in another language Most existing research works rely on automatic machine translation services to directly project information from one language to another However different term distribution between original and translated text documents and translation errors are two main problems faced in the case of using only machine translation To overcome these problems we propose a novel learning model based on active learning and semi-supervised co-training to incorporate unlabelled data from the target language into the learning process in a bi-view framework This model attempts to enrich training data by adding the most confident automatically-labelled examples as well as a few of the most informative manually-labelled examples from unlabelled data in an iterative process Further in this model we consider the density of unlabelled data so as to select more representative unlabelled examples in order to avoid outlier selection in active learning The proposed model was applied to book review datasets in three different languages Experiments showed that our model can effectively improve the cross-lingual sentiment classification performance and reduce labelling efforts in comparison with some baseline methods (C) 2014 Elsevier Ltd All rights reserved',NLP
'The exponential growth of social networks has given rise to a wide variety of content Some social content violates the integrity and dignity of users therefore this task has become challenging The need to deal with short texts poorly written language unbalanced classes and non-thematic aspects These can lead to overfitting in deep neural network (DNN) models used for classification tasks Empirical evidence in previous studies indicates that some of these problems can be overcome by improving the optimization process of the DNN weights to avoid overfitting Moreover a well-defined learning process in the input examples could improve the order of the patterns learned throughout the optimization process In this paper we propose four Curriculum Learning strategies and a new Hybrid Genetic-Gradient Algorithm that proved to improve the performance of DNN models detecting the class of interest even in highly imbalanced datasets',NLP
'Language Models have long been a prolific area of study in the field of Natural Language Processing (NLP) One of the newer kinds of language models and some of the most used are Word Embeddings (WE) WE are vector space representations of a vocabulary learned by a non-supervised neural network based on the context in which words appear WE have been widely used in downstream tasks in many areas of study in NLP These areas usually use these vector models as a feature in the processing of textual data This paper presents the evaluation of newly released WE models for the Portuguese language trained with a corpus composed of 49 billion tokens The first evaluation presented an intrinsic task in which WEs had to correctly build semantic and syntactic relations The second evaluation presented an extrinsic task in which the WE models were used in two downstream tasks: Named Entity Recognition and Semantic Similarity between Sentences Our results show that a diverse and comprehensive corpus can often outperform a larger less textually diverse corpus and that passing the text in parts to the WE generating algorithm may cause loss of quality',NLP
'Distributional models of lexical semantics have proven to be powerful accounts of how word meanings are acquired from the natural language environment (Gunther Rinaldi & Marelli 2019; Kumar 2020) Standard models of this type acquire the meaning of words through the learning of word co-occurrence statistics across large corpora However these models ignore social and communicative aspects of language processing which is considered central to usagebased and adaptive theories of language (Tomasello 2003; Beckner et al 2009) Johns (2021) recently demonstrated that integrating social and communicative information into a lexical strength measure allowed for benchmark fits to be attained for lexical organization data indicating that the social world contains important statistical information for language learning and processing Through the analysis of the communication patterns of over 330000 individuals on the online forum Reddit totaling approximately 55 billion words of text the findings of the current article demonstrates that social information about word usage allows for unique aspects of a words meaning to be acquired providing a new pathway for distributional model development',NLP
'The existence of sound-symbolism (or a non-arbitrary link between form and meaning) is well-attested However sound-symbolism has mostly been investigated with nonwords in forced choice tasks neither of which are representative of natural language This study uses ideophones which are naturally occurring sound-symbolic words that depict sensory information to investigate how sensitive Dutch speakers are to sound-symbolism in Japanese in a learning task Participants were taught 2 sets of Japanese ideophones; 1 set with the ideophones real meanings in Dutch the other set with their opposite meanings In Experiment 1 participants learned the ideophones and their real meanings much better than the ideophones with their opposite meanings Moreover despite the learning rounds participants were still able to guess the real meanings of the ideophones in a 2-alternative forced-choice test after they were informed of the manipulation This shows that natural language sound-symbolism is robust beyond 2-alternative forced-choice paradigms and affects broader language processes such as word learning In Experiment 2 participants learned regular Japanese adjectives with the same manipulation and there was no difference between real and opposite conditions This shows that natural language sound-symbolism is especially strong in ideophones and that people learn words better when form and meaning match The highlights of this study are as follows: (a) Dutch speakers learn real meanings of Japanese ideophones better than opposite meanings (b) Dutch speakers accurately guess meanings of Japanese ideophones (c) this sensitivity happens despite learning some opposite pairings (d) no such learning effect exists for regular Japanese adjectives and (e) this shows the importance of sound-symbolism in scaffolding language learning',NLP
'The P300 speller is a common brain-computer interface (BCI) application designed to communicate language by detecting event related potentials in a subjects electroencephalogram (EEG) signal Information about the structure of natural language can be valuable for BCI communication systems but few attempts have been made to incorporate this domain knowledge into the classifier In this study we treat BCI communication as a hidden Markov model (HMM) where hidden states are target characters and the EEG signal is the visible output Using the Viterbi algorithm language information can be incorporated in classification and errors can be corrected automatically This method was first evaluated offline on a dataset of 15 healthy subjects who had a significant increase in bit rate from a previously published naive Bayes approach and an average 32% increase from standard classification with dynamic stopping An online pilot study of five healthy subjects verified these results as the average bit rate achieved using the HMM method was significantly higher than that using the naive Bayes and standard methods These findings strongly support the integration of domain-specific knowledge into BCI classification to improve system performance and accuracy',NLP
'Health-focused apps with chatbots (healthbots) have a critical role in addressing gaps in quality healthcare There is limited evidence on how such healthbots are developed and applied in practice Our review of healthbots aims to classify types of healthbots contexts of use and their natural language processing capabilities Eligible apps were those that were health-related had an embedded text-based conversational agent available in English and were available for free download through the Google Play or Apple iOS store Apps were identified using 42Matters software a mobile app search engine Apps were assessed using an evaluation framework addressing chatbot characteristics and natural language processing features The review suggests uptake across 33 low- and high-income countries Most healthbots are patient-facing available on a mobile interface and provide a range of functions including health education and counselling support assessment of symptoms and assistance with tasks such as scheduling Most of the 78 apps reviewed focus on primary care and mental health only 6 (759%) had a theoretical underpinning and 10 (1235%) complied with health information privacy regulations Our assessment indicated that only a few apps use machine learning and natural language processing approaches despite such marketing claims Most apps allowed for a finite-state input where the dialogue is led by the system and follows a predetermined algorithm Healthbots are potentially transformative in centering care around the user; however they are in a nascent state of development and require further research on development automation and adoption for a population-level health impact',NLP
'Many recent models in software engineering introduced deep neural models based on the Transformer architecture or use transformer-based Pre-trained Language Models (PLM) trained on code Although these models achieve the state of the arts results in many downstream tasks such as code summarization and bug detection they are based on Transformer and PLM which are mainly studied in the Natural Language Processing (NLP) field The current studies rely on the reasoning and practices from NLP for these models in code despite the differences between natural languages and programming languages There is also limited literature on explaining how code is modeled Here we investigate the attention behavior of PLM on code and compare it with natural language We pre-trained BERT a Transformer based PLM on code and explored what kind of information it learns both semantic and syntactic We run several experiments to analyze the attention values of code constructs on each other and what BERT learns in each layer Our analyses show that BERT pays more attention to syntactic entities specifically identifiers and separators in contrast to the most attended token [CLS] in NLP This observation motivated us to leverage identifiers to represent the code sequence instead of the [CLS] token when used for code clone detection Our results show that employing embeddings from identifiers increases the performance of BERT by 605% and 4% F1-score in its lower layers and the upper layers respectively When identifiers embeddings are used in CodeBERT a code-based PLM the performance is improved by 21-24% in the F1-score of clone detection The findings can benefit the research community by using code-specific representations instead of applying the common embeddings used in NLP and open new directions for developing smaller models with similar performance',NLP
'Background Cardiac magnetic resonance (CMR) imaging is important for diagnosis and risk stratification of hypertrophic cardiomyopathy (HCM) patients However collection of information from large numbers of CMR reports by manual review is time-consuming error-prone and costly Natural language processing (NLP) is an artificial intelligence method for automated extraction of information from narrative text including text in CMR reports in electronic health records (EHR) Our objective was to assess whether NLP can accurately extract diagnosis of HCM from CMR reports Methods An NLP system with two tiers was developed for information extraction from narrative text in CMR reports; the first tier extracted information regarding HCM diagnosis while the second extracted categorical and numeric concepts for HCM classification We randomly allocated 200 HCM patients with CMR reports from 2004 to 2018 into training (100 patients with 185 CMR reports) and testing sets (100 patients with 206 reports) Results NLP algorithms demonstrated very high performance compared to manual annotation The algorithm to extract HCM diagnosis had accuracy of 099 The accuracy for categorical concepts included HCM morphologic subtype 099 systolic anterior motion of the mitral valve 096 mitral regurgitation 093 left ventricular (LV) obstruction 094 location of obstruction 092 apical pouch 098 LV delayed enhancement 093 left atrial enlargement 099 and right atrial enlargement 098 Accuracy for numeric concepts included maximal LV wall thickness 096 LV mass 099 LV mass index 098 LV ejection fraction 098 and right ventricular ejection fraction 099 Conclusions NLP identified and classified HCM from CMR narrative text reports with very high performance',NLP
'Introduction: Routinely collected healthcare data are a powerful research resource but often lack detailed disease-specific information that is collected in clinical free text such as histopathology reports We aim to use natural Language Processing (NLP) techniques to extract detailed clinical and pathological information from histopathology reports to enrich routinely collected data Methods: We used the general architecture for text engineering (GATE) framework to build an NLP information extraction system using rule-based techniques During validation we deployed our rule-based NLP pipeline on 200 previously unseen de-identified and pseudonymised basal cell carcinoma (BCC) histopathological reports from Swansea Bay University Health Board Wales UK The results of our algorithm were compared with gold standard human annotation by two independent and blinded expert clinicians involved in skin cancer care Results: We identified 11224 items of information with a mean precision recall and F1 score of 860% (95% CI: 751-969) 842% (95% CI: 728-961) and 845% (95% CI: 730-951) respectively The difference between clinician annotator F1 scores was 79% in comparison with 155% between the NLP pipeline and the gold standard corpus Cohens Kappa score on annotated tokens was 085 Conclusion: Using an NLP rule-based approach for named entity recognition in BCC we have been able to develop and validate a pipeline with a potential application in improving the quality of cancer registry data supporting service planning and enhancing the quality of routinely collected data for research',NLP
'Background With a significant increase in utilisation of computed tomography (CT) inappropriate imaging is a significant concern Manual justification audits of radiology referrals are time-consuming and require financial resources We aimed to retrospectively audit justification of brain CT referrals by applying natural language processing and traditional machine learning (ML) techniques to predict their justification based on the audit outcomes Methods Two human experts retrospectively analysed justification of 375 adult brain CT referrals performed in a tertiary referral hospital during the 2019 calendar year using a cloud-based platform for structured referring Cohens kappa was computed to measure inter-rater reliability Referrals were represented as bag-of-words (BOW) and term frequency-inverse document frequency models Text preprocessing techniques including custom stop words (CSW) and spell correction (SC) were applied to the referral text Logistic regression random forest and support vector machines (SVM) were used to predict the justification of referrals A test set (300/75) was used to compute weighted accuracy sensitivity specificity and the area under the curve (AUC) Results In total 253 (675%) examinations were deemed justified 75 (200%) as unjustified and 47 (125%) as maybe justified The agreement between the annotators was strong (kappa = 0835) The BOW + CSW + SC + SVM outperformed other binary models with a weighted accuracy of 92% a sensitivity of 91% a specificity of 93% and an AUC of 0948 Conclusions Traditional ML models can accurately predict justification of unstructured brain CT referrals This offers potential for automated justification analysis of CT referrals in clinical departments',NLP
'We created a deep learning model trained on text classified by natural language processing (NLP) to assess right ventricular (RV) size and function from echocardiographic images We included 12684 examinations with corresponding written reports for text classification After manual annotation of 1489 reports we trained an NLP model to classify the remaining 10651 reports A view classifier was developed to select the 4-chamber or RVfocused view from an echocardiographic examination (n = 539) The final models were two image classification models trained on the predicted labels from the combined manual annotation and NLP models and the corresponding echocardiographic view to assess RV function (training set n = 11008) and size (training set n = 9951 The text classifier identified impaired RV function with 99% sensitivity and 98% specificity and RV enlargement with 98% sensitivity and 98% specificity The view classification model identified the 4-chamber view with 92% accuracy and the RV-focused view with 73% accuracy The image classification models identified impaired RV function with 93% sensitivity and 72% specificity and an enlarged RV with 80% sensitivity and 85% specificity; agreement with the written reports was substantial (both kappa = 065) Our findings show that models for automatic image assessment can be trained to classify RV size and function by using model-annotated data from written echocardiography reports This pipeline for auto-annotation of the echocardiographic images using a NLP model with medical reports as input can be used to train an image-assessment model without manual annotation of images and enables fast and inexpensive expansion of the training dataset when needed',NLP
'Investors utilise social media such as Twitter as a means of sharing news surrounding financials stocks listed on international stock exchanges Company ticker symbols are used to uniquely identify companies listed on stock exchanges and can be embedded within tweets to create clickable hyperlinks referred to as cashtags allowing investors to associate their tweets with specific companies The main limitation is that identical ticker symbols are present on exchanges all over the world and when searching for such cashtags on Twitter a stream of tweets is returned which match any company in which the cashtag refers to - we refer to this as a cashtag collision The presence of colliding cashtags could sow confusion for investors seeking news regarding a specific company A resolution to this issue would benefit investors who rely on the speediness of tweets for financial information saving them precious time We propose a methodology to resolve this problem which combines Natural Language Processing and Data Fusion to construct company-specific corpora to aid in the detection and resolution of colliding cashtags so that tweets can be classified as being related to a specific stock exchange or not Supervised machine learning classifiers are trained twice on each tweet - once on a count vectorisation of the tweet text and again with the assistance of features contained in the company-specific corpora We validate the cashtag collision methodology by carrying out an experiment involving companies listed on the London Stock Exchange Results show that several machine learning classifiers benefit from the use of the custom corpora yielding higher classification accuracy in the prediction and resolution of colliding cashtags (C) 2019 The Authors Published by Elsevier Ltd',NLP
'People of the Internet era usually rely on online reviews to make decisions about an online purchase a hotel booking or a car rental and much more since people believe that making decisions based on others opinions lead to making the right choice As writing fake reviews come with monetary gain the opinion spam activities have increased dramatically on online review websites Thus opinion spam in reviews has become a big challenge to people to make purchase decisions as well as damages the reputation of review websites Hence the deceptive opinion spam detection is an essential task in the field of natural language processing Most of the existing research on opinion spam detection uses the traditional bag-of-words model to represent the review text features and apply standard machine learning models such as Support-Vector Machines or Wye Bayes as classifiers There is only a few state of the art methods which have utilized neural network based methods for spam detection With the recent advancements in deep learning there are successful applications of Convolutional Neural Networks (CNN) for natural language processing problems which have achieved improved performance In this study a CNN model is developed to detect opinion spam using the features extracted from the pre-trained GloVe: Global Vectors for Word Representation model Moreover some word and character level features used in existing research work are extracted from the text and concatenated with a feature set extracted by the convolutional layers of the model to improve the performance The proposed model found to outperform the state-of-the-art method and the inclusion of additional features improved the performance further',NLP
'This paper defends the choice of a linguistically-based content ontology for natural language processing and demonstrates that a single common-sense ontology produces plausible interpretations at all levels from parsing through reasoning The paper explores some of the problems and tradeoffs for a method which has just one content ontology A linguistically-based content ontology represents the world view encoded in natural language The content ontology (as opposed to the formal semantic ontology which distinguishes events from propositions and so on) is best grounded in the culture rather than in the world itself or in the mind By world view we mean naive assumptions about what there is in the world and how it should be classified These assumptions are time-worn and reflected in language at several levels: morphology syntax and lexical semantics The content ontology presented in the paper is part of a Naive Semantic lexicon Naive Semantics is a lexical theory in which associated with each word sense is a naive theory (or set of beliefs) about the objects or events of reference While naive semantic representations are not combinations of a closed set of primitives they are also limited by a shallowness assumption Included is just the information required to form a semantic interpretation incrementally not all of the information known about objects The Naive Semantic ontology is based upon a particular language its syntax and its word senses To the extent that other languages codify similar world views we predict that their ontologies are similar Applied in a computational natural language understanding system this linguistically-motivated ontology (along with other native semantic information) is sufficient to disambiguate words disambiguate syntactic structure disambiguate formal semantic representations resolve anaphoric expressions and perform reasoning tasks with text (C) 1995 Academic Press Limited',NLP
'Objectives: Experience-related neuroplasticity suggests that bilinguals who actively manage their two languages would develop more efficient neural organization at brain regions related to language control which also overlap with areas involved in executive control Our aim was to examine how active bilingualism-manifested as the regular balanced use of two languages and language switching-may be related to the different domains of executive control in highly proficient healthy older adult bilinguals controlling for age processing speed and fluid intelligence Methods: Participants were 76 community-dwelling older adults who reported being physically and mentally healthy and showed no signs of cognitive impairment They completed a self-report questionnaire on their language background two computer measures for previously identified covariates (processing speed as measured by two-choice reaction time (RT) task and fluid intelligence as measured by the Ravens Progressive Matrices) as well as a battery of computerized executive control tasks (Color-shape Task Switching Stroop Flanker and Spatial 2-back task) Results: Regression analyses showed that even after controlling for age processing speed and fluid intelligence more balanced bilingualism usage and less frequent language switching predicted higher goal maintenance (nonswitch trials RT in Color-shape Task Switching) and conflict monitoring abilities (global RT in Color-shape Task Switching and Flanker task) Discussion: Results suggest that active bilingualism may provide benefits to maintaining specific executive control abilities in older adult bilinguals against the natural age-related declines',NLP
'A phonemicization or grapheme-to-phoneme conversion (G2P) is a process of converting a word into its pronunciation It is one of the essential components in speech synthesis speech recognition and natural language processing The deep learning (DL)-based state-of-the-art G2P model generally gives low phoneme error rate (PER) as well as word error rate (WER) for high-resource languages such as English and European but not for low-resource languages Therefore some conventional machine learning (ML) based G2P models incorporated with specific linguistic knowledge are preferable for low-resource languages However these models are poor for several low-resource languages because of various issues For instance an Indonesian G2P model works well for roots but gives a high PER for derivatives Most errors come from the ambiguities of some roots and derivative words containing four prefixes: < ber > < meng > < peng > and < ter > In this research an Indonesian G2P model based on n-gram combined with stemmer and phonotactic rules (NGTSP) is proposed to solve those problems An investigation based on 5-fold cross-validation using 50 k Indonesian words informs that the proposed NGTSP gives a much lower PER of 078% than the state-of-the-art Transformer-based G2P model (114%) Besides it also provides a much faster processing time (C) 2021 The Authors Published by Elsevier BV on behalf of King Saud University',NLP
'Short texts like advertisements are characterised by a number of slogans phrases words symbols etc To improve the quality of textual data it is necessary to filter out noise textual data from important data The aim of this work is to determine to what extent it is necessary to carry out the time consuming data pre-processing in the process of discovering sequential patterns in English and Slovak advertisement corpora For this purpose an experiment was conducted focusing on data pre-processing in these two comparable corpora We try to find out to what extent removing the stop words has an influence on a quantity and quality of extracted rules Stop words removal has no impact on the quantity and quality of extracted rules in English as well as in Slovak advertisement corpora Only language has a significant impact on the quantity and quality of extracted rules',NLP
'Mathematics is an effective testbed for measuring the problem-solving ability of machine learning models The current benchmark for deep learning-based solutions is grade school math problems: given a natural language description of a problem the task is to analyse the problem exploit heuristics generated from a very large set of solved examples and then generate an answer In this paper a descendant of the third generation of Generative Pre-trained Transformer Networks (GPT-3) is used to develop a zero-shot learning approach to solve this problem The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one Specifically the architectural solution is built upon OpenAI Codex a descendant of GPT-3 for programming tasks trained on public GitHub repositories the worlds largest source code hosting service Experimental results clearly show the potential of the approach: by exploiting the Python as programming language proposed pipeline achieves the 1863% solve rate against the 682% of GPT-3 Finally by using a fine-tuned verifier the correctness of the answer can be ranked at runtime and then improved by generating a predefined number of trials With this approach for 10 trials and an ideal verifier the proposed pipeline achieves 5420% solve rate',NLP
'This paper presents MAKEDONKA the first open-source Macedonian language synthesizer that is based on the Deep Learning approach The paper provides an overview of the numerous attempts to achieve a human-like reproducible speech which has unfortunately shown to be unsuccessful due to the work invisibility and lack of integration examples with real software tools The recent advances in Machine Learning the Deep Learning-based methodologies provide novel methods for feature engineering that allow for smooth transitions in the synthesized speech making it sound natural and human-like This paper presents a methodology for end-to-end speech synthesis that is based on a fully-convolutional sequence-to-sequence acoustic model with a position-augmented attention mechanism-Deep Voice 3 Our model directly synthesizes Macedonian speech from characters We created a dataset that contains approximately 20 h of speech from a native Macedonian female speaker and we use it to train the text-to-speech (TTS) model The achieved MOS score of 393 makes our model appropriate for application in any kind of software that needs text-to-speech service in the Macedonian language Our TTS platform is publicly available for use and ready for integration',NLP
'Documents written in natural language constitute a major part of the artefacts produced during the software engineering life cycle Studies indicate that more than 80% of enterprise data is stored in some sort of unstructured form mainly as text Therefore the growth of user-generated content especially from social media provides a huge amount of data which allows discovering the experiences opinions and feelings of users Text mining refers to the set of tools techniques and algorithms adopted to extract useful information from unstructured data Considering that Portuguese ranks among the ten most spoken languages and it is the second most common in Twitter this study aims to map current primary studies that relate to the application of text mining for Portuguese A systematic mapping method was applied and 6075 primary studies were retrieved up to the year 2014 A total of 203 studies were included from which more than 60% analyse texts written in Brazilian variant The majority of studies focus on the text classification task Support vector machine and Naive Bayes appear as main the algorithms Folha de SAo Paulo and Publico newspapers appear as main corpora followed by the Portuguese Attorney Generals Office corpus and Twitter',NLP
'Automatic text summarization is an important and useful research area in natural language processing and information retrieval Most of current approaches for text summarization do not make full use of human reading process This paper proposes a multi-document scanning mechanism by simulating human reading process The mechanism simulates human memory of words association between words and three cognitive processes invoked when reading Changes of human memory of topic words in reading process are used to denote sentences significance based on which sentences are then ordered and extracted to form a summary Experiments on DUC2007 test data show that our proposing method is efficient and outperforms two baseline methods',NLP
'We have developed a bilingual interface to the Novell network operating system called the Dialoguer This system carries on a conversation with the user in Arabic or English or a combination of the two and attempts to help the user use the Novell network operating system Learning to use an operating system is a major barrier in starting to use computers There is no single standard for operating systems which makes it difficult for novice users to learn a new operating system With the proliferation of client-server environments users will eventually end up using one network operating system or another These problems motivated our choice of an area to work in and they have made it easy to find real users to test our system This system is both an expert system and a natural language interface The system embodies expert knowledge of the operating system commands and of a large variety of plans that the user may want to carry out The system also contains a natural language understanding component and a response generation component The Dialoguer makes extensive use of case frame tables in both components Algorithms for handling a bilingual dialogue are one of the important contributions of this paper along with the Arabic case frames',NLP
'While simulated game environments have greatly accelerated research in reinforcement learning existing environments lack the open-domain realism of tasks in computer vision or natural language processing which operate on artifacts created by humans in natural organic settings To foster reinforcement learning research in such settings we introduce the World of Bits (WoB) a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions The two main challenges are: (i) to curate a diverse set of natural web-based tasks and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web To tackle this we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the website Finally we show that agents trained via behavioral cloning and reinforcement learning can complete a range of web-based tasks',NLP
'Readability is an active field of research in the late nineteenth century and vigorously persuaded to date The recent boom in data-driven machine learning has created a viable path forward for readability classification and ranking The evaluation of text readability is a time-honoured issue with even more relevance in todays information-rich world This paper addresses the task of readability assessment for the English language Given the input sentences the objective is to predict its level of readability which corresponds to the level of literacy anticipated from the target readers This readability aspect plays a crucial role in drafting and comprehending processes of English language learning Selecting and presenting a suitable collection of sentences for English Language Learners may play a vital role in enhancing their learning curve In this research we have used 30000 English sentences for experimentation Additionally they have been annotated into seven different readability levels using Flesch Kincaid Later various experiments were conducted using five Machine Learning algorithms ie KNN SVM LR NB and ANN The classification models render excellent and stable results The ANN model obtained an F-score of 095% on the test set The developed model may be used in education setup for tasks such as language learning assessing the reading and writing abilities of a learner',NLP
'This study investigated the underlying link between speech and music by examining whether and to what extent congenital amusia a musical disorder characterized by degraded pitch processing would impact spoken sentence comprehension for speakers of Mandarin a tone language Sixteen Mandarin-speaking amusics and 16 matched controls were tested on the intelligibility of news-like Mandarin sentences with natural and flat fundamental frequency (F-0) contours (created via speech resynthesis) under four signal-to-noise (SNR) conditions (no noise +5 0 and -5 dB SNR) While speech intelligibility in quiet and extremely noisy conditions (SNR= -5 dB) was not significantly compromised by flattened F-0 both amusic and control groups achieved better performance with natural-F-0 sentences than flat-F-0 sentences under moderately noisy conditions (SNR= +5 and 0 dB) Relative to normal listeners amusics demonstrated reduced speech intelligibility in both quiet and noise regardless of whether the F-0 contours of the sentences were natural or flattened This deficit in speech intelligibility was not associated with impaired pitch perception in amusia These findings provide evidence for impaired speech comprehension in congenital amusia suggesting that the deficit of amusics extends beyond pitch processing and includes segmental processing (C) 2014 Elsevier Ltd All rights reserved',NLP
'After kidney transplantation graft rejection must be prevented Therefore a multitude of parameters of the patient is observed pre- and postoperatively To support this process the Screen Reject research project is developing a data warehouse optimized for kidney rejection diagnostics In the course of this project it was discovered that important information are only available in form of free texts instead of structured data and can therefore not be processed by standard ETL tools which is necessary to establish a digital expert system for rejection diagnostics Due to this reason data integration has been improved by a combination of methods from natural language processing and methods from image processing Based on state-of-the-art data warehousing technologies (Microsoft SSIS) a generic data integration tool has been developed The tool was evaluated by extracting Banff-classification from 218 pathology reports and extracting HLA mismatches from about 1700 PDF files both written in german language',NLP
'The article is devoted to the problem of linguistic specific features of Chinese language and methodological development of curriculum for Chinese language learning through the use of film material in educational process with a particular emphasis on the students studying Chinese outside China This paper systematically focuses on those aspects of contemporary Chinese language which are the most difficult to assimilate critically revises different approaches to the organisation of the educational process for students with different levels of knowledge The paper shows the authors methodic use of film footage as a way to create in class the conditions presenting Chinese language environment that is intended to help students to master their listening skills The article presents the detailed linguistic and methodological explanations of the developed teaching system Special attention is paid to the attempt to reveal the universal principles of the programme implementation using film materials at different stages of learning The authors give practical recommendations aimed at formation among the students as soon as possible general and specialized knowledge of spoken Chinese language and ability to use it as well as development of the ability of listening to spoken speech at a natural pace This work is based on the analysis of a number of Russian and Chinese researchers and practical experiments by the authors',NLP
'The ability to objectively quantify the complexity of a text can be a useful indicator of how likely learners of a given level will comprehend it Before creating more complex models of assessing text difficulty the basic building block of a text consists of words and inherently its overall difficulty is greatly influenced by the complexity of underlying words One approach is to measure a words Age of Acquisition (AoA) an estimate of the average age at which a speaker of a language understands the semantics of a specific word Age of Exposure (AoE) statistically models the process of word learning and in turn an estimate of a given words AoA In this paper we expand on the model proposed by AoE by training regression models that learn and generalize AoA word lists across multiple languages including English German French and Spanish Our approach allows for the estimation of AoA scores for words that are not found in the original lists up to the majority of the target languages vocabulary Our method can be uniformly applied across multiple languages though the usage of parallel corpora and helps bridge the gap in the size of AoA word lists available for non-English languages This effort is particularly important for efforts toward extending AI to languages with fewer resources and bench marked corpora',NLP
'We investigate a model of language evolution based on population game dynamics with learning First we examine the case of two genetic variants of universal grammar (UG) the heart of the human language faculty assuming each admits two possible grammars The dynamics are driven by a communication game We prove using dynamical systems techniques that if the payoff matrix obeys certain constraints then the two UGs are stable against invasion by each other that is they are evolutionarily stable Then we prove a similar theorem for an arbitrary number of disjoint UGs In both theorems the constraints are independent of the learning process Intuitively if a mutation in UG results in grammars that are incompatible with the established languages then the mutation will die out because mutants will be unable to communicate and therefore unable to realize any potential benefit of the mutation An example for which these theorems do not apply shows that compatible mutations may or may not be able to invade depending on the populations history and the learning process These results suggest that the genetic history of language is constrained by the need for compatibility and that mutations in the language faculty may have died out or taken over due more to historical accident than to any straightforward notion of relative fitness',NLP
'Current driving functions are continuously improved As target vision of this development the final state of unattended reliable navigation under all possible circumstances is defined Besides the requirements and the test cases the main accompanying specification artifact for the iterative development of such functions is the scenario catalog which is usually created by hand The included scenarios often differ in terms of the used abstraction level the chosen natural language and the degree of completeness depending on the current development phase There is currently no methodology available which allows the iterative evolution of textual scenario descriptions based on natural language and that can be used for the development of automated driving functions (ADF) As a solution approach the methodology scenario-accompanied text-based iterative Evaluation of automated driving Functions (stiEF) as embedding workflow and an embedded textual domain-specific language (DSL) for scenario creation arc presented The DSL facilitates the iterative multilingual evolution of scenario descriptions and the generation of the corresponding simulation parameters and scenario sketches For this scenarios with different levels of detail are used as central specification artifacts This approach avoids ambiguities in the descriptions guides the user through their creation process and ensures completeness This is demonstrated by a prototypical realization',NLP
'Software architecture compliance is concerned with the alignment of implementation with its desired architecture and detecting potential inconsistencies The work presented in this paper is specifically concerned with behavioral architecture compliance That is the focus is on semantic alignment of implementation and architecture In particular this paper evaluates three representative approaches for describing semantic constraints in terms of their understandability namely natural language descriptions as used in many architecture documentations today a structured language based on specification patterns that abstract underlying temporal logic formulas and a structured cause-effect language that is based on Complex Event Processing We conducted a controlled experiment with 190 participants using a simple randomized design with one alternative per experimental unit Overall all approaches support a high level of correct understanding and the statistical inference suggests that all tested approaches are equally well suited for describing semantic constraints for behavioral architecture compliance in terms of understandability In consequence this indicates that it is possible to benefit from the tested structured languages with underlying formal representations for automated verification without having to suffer from decreased understandability Vice versa the results suggest that the use of natural language can be a suitable way to document architecture semantics when reliable automated support for formal verification is of minor importance',NLP
'Linking concepts and named entities to knowledge bases has become a crucial Natural Language Understanding task In this respect recent works have shown the key advantage of exploiting textual definitions in various Natural Language Processing applications However to date there are no reliable large-scale corpora of sense-annotated textual definitions available to the research community In this paper we present a large-scale high-quality corpus of disambiguated glosses in multiple languages comprising sense annotations of both concepts and named entities from a unified sense inventory Our approach for the construction and disambiguation of the corpus builds upon the structure of a large multilingual semantic network and a state-of-the-art disambiguation system; first we gather complementary information of equivalent definitions across different languages to provide context for disambiguation and then we combine it with a semantic similarity-based refinement As a result we obtain a multilingual corpus of textual definitions featuring over 38 million definitions in 263 languages and we make it freely available at http://lcluniroma1it/disambiguated- glosses Experiments on Open Information Extraction and Sense Clustering show how two state-of-the-art approaches improve their performance by integrating our disambiguated corpus into their pipeline',NLP
'One of the most precious gift of nature to human beings is the ability to express himself by responding to the events occurring in his surroundings Every normal human being sees listens and then reacts to the situations by speaking himself out But there are some unfortunate ones who are deprived of this valuable gift This creates a gap between the normal human beings and the deprived ones This application will help for both of them to communicate with each other The system is mainly consists of two modules first module is drawing out Indian Sign Language(ISL) gestures from real-time video and mapping it with human-understandable speech Accordingly second module will take natural language as input and map it with equivalent Indian Sign Language animated gestures Processing from video to speech will include frame formation from videos finding region of interest (ROI) and mapping of images with language knowledge base using Correlational based approach then relevant audio generation using Google Text-to-Speech (TTS) API The other way round natural language is mapped with equivalent Indian Sign Language gestures by conversion of speech to text using Google Speech-to-Text (STT) API further mapping the text to relevant animated gestures from the database',NLP
'Stock market prediction is one of the complex analysis of all time Different expert analysts as well as computer scientists are working for the development of a stable and robust platform for the prediction of future stock value The primary challenge is the nature of the movement of the daily price which depends on various factors To build a predictive model for the analysis of stock data and prediction is an active area of research However we found only a few numbers of studies performed on Korean stock market analysis including both KOSDAQ and KOSPI companies This study proposed a three-stage approach based on Natural Language Processing and Deep Learning techniques to analyze comprehends the past and present market scenario and also predict the future value of a stock This study involves the application of natural processing techniques and deep learning techniques on around 2500 Korean companies covering KOSDAQ and KOSPI Firstly this paper successfully presents the current condition of the stock and overall Korean stock exchange; secondly it recommends the potential months and weeks for investment and finally it predicts the future value of a stock with high accuracy This paper may pose as a structural framework for developing a complete stock market prediction application',NLP
'Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate summarize translate and classify texts Yet these language models still fail to match the language abilities of humans Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words the human brain would continuously predict a hierarchy of representations that spans multiple timescales To test this hypothesis we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories First we confirmed that the activations of modern language models linearly map onto the brain responses to speech Second we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping Finally we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level longer-range and more contextual representations than temporal cortices Overall these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition Current machine learning language algorithms make adjacent word-level predictions In this work Caucheteux et al show that the human brain probably uses long-range and hierarchical predictions taking into account up to eight possible words into the future',NLP
'Covering: up to the end of 2020 The machine learning field can be defined as the study and application of algorithms that perform classification and prediction tasks through pattern recognition instead of explicitly defined rules Among other areas machine learning has excelled in natural language processing As such methods have excelled at understanding written languages (eg English) they are also being applied to biological problems to better understand the genomic language In this review we focus on recent advances in applying machine learning to natural products and genomics and how those advances are improving our understanding of natural product biology chemistry and drug discovery We discuss machine learning applications in genome mining (identifying biosynthetic signatures in genomic data) predictions of what structures will be created from those genomic signatures and the types of activity we might expect from those molecules We further explore the application of these approaches to data derived from complex microbiomes with a focus on the human microbiome We also review challenges in leveraging machine learning approaches in the field and how the availability of other omics data layers provides value Finally we provide insights into the challenges associated with interpreting machine learning models and the underlying biology and promises of applying machine learning to natural product drug discovery We believe that the application of machine learning methods to natural product research is poised to accelerate the identification of new molecular entities that may be used to treat a variety of disease indications',NLP
'A robot participating in natural dialogue with a human interlocutor may need to discuss reason about or initiate actions concerning dialogue-referenced entities To do so the robot must first identify or create new representations for those entities a capability known as reference resolution We previously presented algorithms for resolving references occurring in definite noun phrases In this paper we present GH-POWER: an algorithm for resolving references occurring in a wider array of linguistic forms by making novel extensions to the Givenness Hierarchy and evaluate GH-POWER on natural task-based human-human and human-robot dialogues',NLP
'Sentiment analysis is a widely researched area due to its various applications in customer services brand monitoring and market research Automatic sentiment classification is an important but challenging task Contrary to the English language sentiment analysis for low-resource languages like Urdu is an under-explored research area Most of the work on sentiment analysis in the Urdu language is domain-dependent where models are mostly trained and tested on the same dataset on limited domains However sentiments in different domains are expressed differently and manually annotating the datasets for all possible domains is unfeasible Training a sentiment classifier using annotated data on one domain and testing it on another domain results in poor performance as the terms appearing in the source domain (training data) might not appear in the target (testing data) domain In this paper we present a baseline method for cross-domain sentiment analysis in the Urdu language using two different domains Feature extraction is performed using n-grams and word embedding techniques Sentiment classification is performed using machine learning and deep learning classifiers The proposed method achieves an accuracy precision recall and F1 scores of 077 083 068 and 075 respectively',NLP
'While increasing evidence points to a critical role for the motor system in language processing the focus of previous work has been on the linguistic category of verbs Here we tested whether nouns are effective in modulating the motor system and further whether different kinds of nouns - those referring to artifacts or natural items and items that are graspable or ungraspable - would differentially modulate the system A Transcranial Magnetic Stimulation (TMS) study was carried out to compare modulation of the motor system when subjects read nouns referring to objects which are Artificial or Natural and which are Graspable or Ungraspable TMS was applied to the primary motor cortex representation of the first dorsal interosseous (FDI) muscle of the right hand at 150 ms after noun presentation Analyses of Motor Evoked Potentials (MEPs) revealed that across the duration of the task nouns referring to graspable artifacts (tools) were associated with significantly greater MEP areas Analyses of the initial presentation of items revealed a main effect of graspability The findings are in line with an embodied view of nouns with MEP measures modulated according to whether nouns referred to natural objects or artifacts (tools) confirming tools as a special class of items in motor terms Additionally our data support a difference for graspable versus non graspable objects an effect which for natural objects is restricted to initial presentation of items (C) 2011 Elsevier Ltd All rights reserved',NLP
'Transcreators extract crucial information from text written in one language for a specific media type and translate this text into a different language and a different media type Multiple factors drive changes in narrative structures in different languages for different media platforms AI-based approaches can be used to extract critical information elements from text and augment human analysis and insight to facilitate transcreation In this study we apply self-supervised learning and active few-shot learning based on generative pretrained transformer models (eg GPT-N) to perform information extraction We also used Wikifier (https://wikifierorg/) to annotate the related text with links to relevant Wikipedia concepts to augment human users with additional explanations The performance statistics were collected using four news stories and the results show that self-supervised approach is error-prone because the GPT-3 pretrained language model can generate synthetic information based on patterns learned from its huge training corpus instead of reflecting only relevant facts in the prompted text On the other hand active few-shot learning worked very well with 875% accuracy on the experimental examples Wikifier also provides a large number of correct and useful links to named entities such as human names locations organizations and concepts Transcreators can leverage these AI tools to augment their ability to effectively perform their tasks',NLP
'Presently there has been a surge for representing information in a common language for use cases like question-answering system machine translation text summarization etc UNL has been a centre of attraction for researchers in the past two decades and many have tried to harness its power This paper throws a glimpse into the strides made by such researchers As UNL provides a language independent platform it eases out the decision making by accessing the valuable and meaningful information which is otherwise a challenging errand Thus it captures information and finds its major application in Natural Language Processing (NLP) domain The paper is written with the intent to introduce its readers to the UNL framework and explain how it is being used to solve some real world problems',NLP
'HCI research has for long been dedicated to better and more naturally facilitating information transfer between humans and machines Unfortunately humans most natural form of communication speech is also one of the most difficult modalities to be understood by machines -despite and perhaps because it is the highest-bandwidth communication channel we possess While significant research efforts from engineering to linguistic and to cognitive sciences have been spent on improving machines ability to understand speech the MobileHCI community (and the HCI field at large) has been relatively timid in embracing this modality as a central focus of research This can be attributed in part to the unexpected variations in error rates when processing speech in contrast with often-unfounded claims of success from industry but also to the intrinsic difficulty of designing and especially evaluating speech and natural language interfaces As such the development of interactive speech-based systems is mostly driven by engineering efforts to improve such systems with respect to largely arbitrary performance metrics Such developments have often been void of any user-centered design principles or consideration for usability or usefulness The goal of this course is to inform the MobileHCI community of the current state of speech and natural language research to dispel some of the myths surrounding speech-based interaction as well as to provide an opportunity for researchers and practitioners to learn more about how speech recognition and speech synthesis work what are their limitations and how they could be used to enhance current interaction paradigms Through this we hope that HCI researchers and practitioners will learn how to combine recent advances in speech processing with user-centred principles in designing more usable and useful speechbased interactive systems',NLP
'What is the relation between the material conventional symbol structures that we encounter in the spoken and written word and human thought? A common assumption that structures a wide variety of otherwise competing views is that the way in which these material conventional symbol-structures do their work is by being translated into some kind of content-matching inner code One alternative to this view is the tempting but thoroughly elusive idea that we somehow think in some natural language (such as English) In the present treatment I explore a third option which I shall call the complementarity view of language According to this third view the actual symbol structures of a given language add cognitive value by complementing (without being replicated by) the more basic modes of operation and representation endemic to the biological brain The cognitive bonus that language brings is on this model not to be cashed out either via the ultimately mysterious notion of thinking in a given natural language or via some process of exhaustive translation into another inner code Instead we should try to think in terms of a kind of coordination dynamics in which the forms and structures of a language qua material symbol system play a key and irreducible role Understanding language as a complementary cognitive resource is I argue an important part of the much larger project (sometimes glossed in terms of the extended mind) of understanding human cognition as essentially and multiply hybrid: as involving a complex interplay between internal biological resources and external non-biological resources',NLP
'We address the problem of synthesizing code completions for programs using APIs Given a program with holes we synthesize completions for holes with the most likely sequences of method calls Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase and index these into a statistical language model We then employ the language model to find the highest ranked sentences and use them to synthesize a code completion Our approach is able to synthesize sequences of calls across multiple objects together with their arguments Experiments show that our approach is fast and effective Virtually all computed completions typecheck and the desired completion appears in the top 3 results in 90% of the cases',NLP
'Previous imaging studies have used mostly perceptually abstracted idealized or static stimuli to show segregation of function in the cerebral cortex We wanted to learn whether functional segregation is maintained during more natural complex and dynamic conditions when many features have to be processed simultaneously and identify regions whose activity correlates with the perception of specific features To achieve this we used functional magnetic resonance imaging (fMRI) to measure brain activity when human observers viewed freely dynamic natural scenes (a James Bond movie) The intensity with which they perceived different features (color faces language and human bodies) was assessed psychometrically in separate sessions In all subjects different features were perceived with a high degree of independence over time We found that the perception of each feature correlated with activity in separate specialized areas whose activity also varied independently We conclude that even in natural conditions when many features have to be processed simultaneously functional specialization is preserved Our method thus opens a new way of brain mapping which allows the localization of a multitude of brain areas based on a single experiment using uncontrolled natural stimuli Furthermore our results show that the intensity of activity in a specialized area is linearly correlated with the intensity of its perceptual experience This leads us to suggest that each specialized area is directly responsible for the creation of a feature-specific conscious percept (a microconsciousness) (C) 2003 Wiley-Liss Inc',NLP
'Additive composition (Foltz et al in Discourse Process 15:285-307 1998; Landauer and Dumais in Psychol Rev 104(2):211 1997; Mitchell and Lapata in Cognit Sci 34(8):1388-1429 2010) is a widely used method for computing meanings of phrases which takes the average of vector representations of the constituent words In this article we prove an upper bound for the bias of additive composition which is the first theoretical analysis on compositional frameworks from a machine learning point of view The bound is written in terms of collocation strength; we prove that the more exclusively two successive words tend to occur together the more accurate one can guarantee their additive composition as an approximation to the natural phrase vector Our proof relies on properties of natural language data that are empirically verified and can be theoretically derived from an assumption that the data is generated from a Hierarchical Pitman-Yor Process The theory endorses additive composition as a reasonable operation for calculating meanings of phrases and suggests ways to improve additive compositionality including: transforming entries of distributional word vectors by a function that meets a specific condition constructing a novel type of vector representations to make additive composition sensitive to word order and utilizing singular value decomposition to train word vectors',NLP
'In social networks the users tend to express more themselves by sharing publicly their opinions emotions and sentiments the benefits of analyzing such data are eminent however the process of extracting and transforming these raw data can be a very challenging task particularly when the sentiments are expressed in Arabic language Two main categories of Arabic are massively used in social networks namely the modern standard Arabic which is the official language and the dialectal Arabic which is itself subdivided to several categories depending on countries and regions In this paper we focus on analyzing Facebook comments that are expressed in modern standard or in Moroccan dialectal Arabic; therefore we put these two language categories under the scope by testing and comparing two approaches The first one is the classical approach that considers all Arabic text as homogeneous The second one that we propose require a text classification beforehand sentiment classification based on language categories: the standard and the dialectal Arabic The idea behind this approach is to adapt the text preprocessing on each language category with more precision In supervised classification we have applied two of the most reputed classifiers in sentiment analysis applications Naive Bayes and SVM The results of this study are promising since good performance were obtained',NLP
'Emotion detection is a critical component in allowing machines to understand and respond to human emotions In this paper we explore the potential of pre-trained transformer-based language models namely GPT35 and RoBERTa for emotion detection in natural language processing Specifically we focus on examining the quality of emotion detection in LLMs and their potential as automatic labeling generators to improve accuracy The emotional response to two significant events the murder of Zhina (Mahsa) Amini in Iran and the earthquake in Turkey and Syria is analyzed We observe that GPTs generative nature hinders its performance in fine-grained emotion classification whereas RoBERTas fine-tuning abilities and extensive pre-training specifically for emotions enable more accurate predictions within a limited set of emotional labels',NLP
'In this paper we study the discriminative modeling of Spoken Language Understanding (SLU) using Conditional Random Fields (CRF) and Statistical Machine Translation (SMT) alignment models Previous discriminative approaches to SLU have been dependent on n-gram features Other previous works have used SMT alignment models to predict the output labels We have used SMT alignment models to align the abstract labels and trained CRF to predict the labels We show that the state transition features improve the performance Furthermore we have compared the proposed method with two baseline approaches; Hidden Vector States (HVS) and baseline-CRF The results show that for the F-measure the proposed method outperforms HVS by 174% and baseline-CRF by 17% on ATIS corpus',NLP
'This paper focuses on carrying out the Named Entity Recognition and Classification (NERC) task on Kannada a major Dravidian language spoken in India Low resource conditions such as absence of external linguistic resources and gazetteers in Kannada and other Dravidian languages pose obstacles to the NERC task LSTM networks with their capability of learning long-term dependencies present an effective solution to this task without the need of a deeper understanding of the semantics of the language This paper describes a novel supervised machine learning model for Kannada NERC using Bidirectional LSTM networks The network model is trained and validated on a manually annotated corpus and gives encouraging results in terms of various evaluation metrics',NLP
'This paper examins approaches for translation between English and morphology-rich languages Experiment with English-Russian and English-Lithuanian revels that pure statistical approaches on 10 million word corpus gives unsatisfactory translation Then several Web-available linguistic resources are suggested for translation Syntax parsers bilingual and semantic dictionaries bilingual parallel corpus and monolingual Web-based corpus are integrated in one comprehensive statistical model Multi-abstraction language representation is used for statistical induction of syntactic and semantic transformation rules called multi-alignment templates The decoding model is described using the feature functions a log-linear modeling approach and A* search algorithm An evaluation of this approach is performed on the English-Lithuanian language pair Presented experimental results demonstrates that the multi-abstraction approach and hybridization of learning methods can improve quality of translation',NLP
'Optical character recognition is a key field that attempts to convert printed text into a digital representation In this digital era more printed materials are available and they aremostly unclassified When dealingwith large data any unclassified data are useless The study of categorizing digital messages into some intelligible structure is known as natural language processing (NLP) Also the input should be in digital format OCR mitigates the burden of translating printed text into digital text Even though OCR has become popular for English scripts and many other foreign languages there has been not much development in OCR for Indian regional languages like Tamil Telugu Malayalam etc The major reason is that the existing literature on regional language contains a large number of characters and several methods are proposed to combine them in order to make words Tamil has 247 characters which include alphabets (12) consonants (18) and alphabets + consonants (216) The combination in character makes the language difficult in converting to digital format This research work has introduced a method to label each class with a particular number by reducing the class length to 108 This makes it much easier to process due to less number of classes The usage of Convolutional Neural Network (CNN) made the processing smoother The proposed CNN architecture was self-made with various hit and trial methods which will also be discussed in this paper',NLP
'Inductive transfer learning is widespread in computer vision applications However in natural language processing (NLP) applications is still an under-explored area The most common transfer learning method in NLP is the use of pre-trained word embeddings The Universal Language Model Fine-Tuning (ULMFiT) is a recent approach which proposes to train a language model and transfer its knowledge to a final classifier During the classification step ULMFiT uses a max and average pooling layer to select the useful information of an embedding sequence We propose to replace max and average pooling layers with a soft attention mechanism The goal is to learn the most important information of the embedding sequence rather than assuming that they are max and average values We evaluate the proposed approach in six datasets and achieve the best performance in all of them against literature approaches',NLP
'Assistive robot systems are designed to help individuals with physical disabilities perform routine daily living activities Current assistive robot systems are increasingly capable of performing a variety of tasks that benefit the human user This has created the need to specify the particular task that the disabled human user desires the system to perform Enhanced user interfaces such as joysticks can provide fine-grained control commands but this is time-consuming and mentally and physically taxing This article describes an ongoing project that aims to provide people with physical disabilities a method to control a robot arm using voice commands The goal is to enable the user to control the system using natural language ie without learning a special robot control vocabulary The work describes the design and evaluation of a real-time framework that combines speech recognition camera-based object detection and an inference module that matches the potentially ambiguous results of speech recognition with object detection outputs to generate a unique control input for a computer vision-based robot arm Thus the integration of natural language and object detection systems reduces the ambiguity in specifying tasks one of the major bottlenecks in voice-based user interfaces A modified version of the deep learning-based object detection network YOLO (You only look once) is used to identify all potential objects of interest in the environment Evaluating this integrated voice and object recognition-based user interface indicates that tasks can be specified accurately in different settings',NLP
'Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language (2) causal sentences convey essential context about the subject of requirements and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts such as test cases Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts determining whether the automatic extraction reduces the manual effort of requirements formalization Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents 558 of which were causal (1252%) The best evaluation of a requirements document provided an automatic extraction of 4857% cause-effect graphs on average which demonstrates the feasibility of the approach Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts',NLP
'In this paper we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology While prior work has investigated this setting in the monolingual case using English speech data this work represents the first effort to apply these techniques to languages beyond English Using spoken captions collected in English and Hindi we show that the same model architecture can be successfully applied to both languages Further we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models Finally we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval',NLP
'The task of recognizing and generating paraphrases is an essential component in many Arabic natural language processing (NLP) applications A well-established machine translation approach for automatically extracting paraphrases leverages bilingual corpora to find the equivalent meaning of phrases in a single language is performed by pivoting over a shared translation in another language Neural machine translation has recently become a viable alternative approach to the more widely-used statistical machine translation In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based mainly on neural networks Our model describes paraphrases in a continuous space and generates candidate paraphrases for an Arabic source input Experimental results across datasets confirm that neural paraphrases significantly outperform those obtained with (C) 2018 The Authors Published by Elsevier BV',NLP
'A large amount of materials science knowledge is generated and stored as text published in peer-reviewed scientific literature While recent developments in natural language processing such as Bidirectional Encoder Representations from Transformers (BERT) models provide promising information extraction tools these models may yield suboptimal results when applied on materials domain since they are not trained in materials science specific notations and jargons Here we present a materials-aware language model namely MatSciBERT trained on a large corpus of peer-reviewed materials science publications We show that MatSciBERT outperforms SciBERT a language model trained on science corpus and establish state-of-the-art results on three downstream tasks named entity recognition relation classification and abstract classification We make the pre-trained weights of MatSciBERT publicly accessible for accelerated materials discovery and information extraction from materials science texts',NLP
'This paper introduces a knowledge base and a method for the automatic classification of given simple sentences of the Kazakh language based on the construction of their parse trees and ontological models For this the structure of each type of sentence is represented by the construction of corresponding parse tree To create a knowledge base taking into account the semantics of the components of simple sentences ontological models which will be combined into a single ontological model of simple sentences are developed Then the queries for the knowledge base are formulated and a reasoner which automatically classifies simple sentences is launched The results of the work will be used to create intelligent question-answer training knowledge evaluating and other systems',NLP
'Natural Language Processing (NLP) directed to ambiguous representation for software requirements Ambiguity at different levels creates different representation and meaning This paper reduces the issues of ambiguity levels for the Software Requirements Specification (SRS) using formal methods The end result shows the effectiveness in specifications through Z language The Z specification is created for the commercial application of online food ordering system to improve the order details accuracy and efficiency The stakeholder needs for food ordering system are gathered from the project goal The system is designed using Unified Modeling Language (UML) illustration of use case diagram The specification is created for the system behavior to remove the ambiguity Along with this Z/EVES tool is used for the evaluation of Z specifications for the demonstration',NLP
'To model the data and functions in various computer science applications the researcher uses a Data Flow Diagram (DFD) DFD has been constructed using [open-source software tools that provide users with different shapes and environments However the existing approaches require substantial human effort the validity of the generated output is still a loophole and they have never gained traction in practice Our research objective is to develop a semi-automated tool for drawing complex Data Flow Diagrams in the shortest time according to the specified features of the intended system We developed a Natural Language Interface (NLI) that allows the user to compose a query and identify the system functionality and constraints for the composition of DFD Natural Language Processing (NLP) techniques are applied to scrapped data to extract the keywords and develop a data repository Also we developed rule-based algorithms to map user queries onto respective token shapes to draw the required functionality into appropriate levels of DFD For verification output DFDs were converted into conceptual digraphs using adjacency and permutation matrices to evaluate isomorphism The empirical results reflect that the DFDs generated by the system are correct complete and significant& COPY; 2023 The Author(s) Published by Elsevier BV on behalf of King Saud University This is an open access article under the CC BY-NC-ND license (http://creativecommonsorg/licenses/by-nc-nd/40/)',NLP
'Recently transfer learning from pre-trained language models has proven to be effective in a variety of natural language processing tasks including sentiment analysis This paper aims at identifying deep transfer learning baselines for sentiment analysis in Russian Firstly we identified the most used publicly available sentiment analysis datasets in Russian and recent language models which officially support the Russian language Secondly we fine-tuned Mul-tilingual Bidirectional Encoder Representations from Transformers (BERT) RuBERT and two versions of the Multilingual Universal Sentence Encoder and obtained strong or even new state -of-the-art results on seven sentiment datasets in Russian: SentRuEval-2016 SentiRuEval-2015 RuTweetCorp RuSentiment LINIS Crowd and Kaggle Russian News Dataset and RuReviews Lastly we made fine-tuned models publicly available for the research community',NLP
'The Working Procedure Model (WPM) which is composed by a set of models is used to describe a process of a part made from roughcast to product And the WPM plays a key role while part being produced The process information comprises process drawing and process steps and shows a sequencing and asymptotic course that a part is made The 3D model cant be constructed automatically by the existing method of parameterized design Focusing on process sheets this paper studies how to apply and implement the Natural Language Understanding into the 3D reconstruction is researched The method of asymptotic approximation product was proposed which constructs 3D process model automatically and intelligence Compared with the traditional 3D model reconstruction based on orthographic projections or engineering drawing the process information has some advantages followed On the one hand the reconstruction object is translated from the complicated engineering drawing into a series of more simple process drawing With added plentiful process information for reconstruction the disturbances are avoided such as irrelevant graph symbol and label etc And more the form change of both neighbor process drawings is so little that the engineering drawings interpretation is no difficulty in addition the abnormal solution and multi-solution can he avoided during reconstruction and the problems how to be applicable to more objects is solved ultimately Therefore the utility method for 3D reconstruction model will be possible On the other hand the WPM not only includes the information about parts characters hut also can deliver the information of design process and engineering to the downstream',NLP
'Grammar induction is one of attractive research areas of natural language processing Since both supervised and to some extent semi-supervised grammar induction methods require large treebanks and for many languages such treebanks do not currently exist we focused our attention on unsupervised approaches Constituent Context Model (CCM) seems to be the state of the art in unsupervised grammar induction In this paper we show that the performance of CCM in free word order languages (FWOLs) such as Persian is inferior to that of fixed order languages such as English We also introduce a novel approach called parent-based Constituent context model (PCCM) and show that by using some history notion of context and constituent information of each spans parent the performance of CCM especially in dealing with FWOLs can be significantly improved',NLP
'Language Modeling is at the core of many natural language processing tasks We analyze two such recent models: a Gated Convolutional Network (GCN) with five layers on the Wikitext-2 dataset and a Transformer network with 24 layers on the Google Billion Word dataset We find that when executed on modern graphics processors 30% of the execution time is due to the final adaptive softmax layer Analytical modeling of the computation and memory demands of the GCN shows that this behavior will persist even if the hidden state is increased - which could be needed to improve accuracy or to support a wider vocabulary We present variations of the adaptive softmax layer that reduce execution time for the layer by 40% and that scale better with the hidden state',NLP
'WordNets are useful resources for natural language processing Various WordNets for different languages have been developed by different goups Recently World WordNet Database Structure (WWDS) was proposed by Redkar et al (2015) as a common platform to store these different WordNets However it is undenitilized due to lack of programming interface In this paper we present WWDS APIs which are designed to address this shortcoming These WWDS APIs in conjunction with WWDS act as a wrapper that enables developers to utilize WordNets without worrying about the underlying storage structure The APIs are developed in PHP Java and Python as they are the preferred programming languages of most developers and researchers working in language technologies These APIs can help in various applications like machine translation word sense disambiguation multilingual information retrieval etc',NLP
'In this work we present a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction showcasing it on the Greek language The goals of this paper are twofold: First we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture Second we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline to which we apply a series of pre-processing and triple extraction tasks Finally we back-translate the extracted triples to Greek We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language',NLP
'Social media users have the proclivity to write majority of the data for under resourced languages in code-mixed format Code-mixing is defined as mixing of two or more languages in a single sentence Research in code-mixed text helps apprehend security threats prevalent on social media platforms In such instances language identification is an imperative task of code-mixed text The focus of this paper is to carry out a word-level language identification (WLLI) of Malayalam-English code-mixed data from social media platforms like YouTube This study was centered around BERT a transformer model along with its variants - CamemBERT DistilBERT - for intuitive perception of the language at the word-level The propounded approach entails tagging Malayalam-English code-mixed data set with six labels: Malayalam (mal) English (eng) acronyms (acr) universal (univ) mixed (mix) and undefined (undef) Newly developed corpus of Malayalam-English was deployed for appraisal of the effectiveness of state-of-the-art models like BERT Evaluation of the proffered approach accomplished with other code-mixed language such as Hindi-English notched a 9% increase in the F1-score',NLP
'In the face of rapid globalization the concept of translation performs the most important role in continuing the existence of native languages Most of the research on Natural Language Processing in Neural Machine Translation has achieved an impressive result through parallel corpus dataset Low resourced languages confront low performance due to the lack of parallel corpus data Creating parallel corpus for language pair is more expensive and needs the persons who are expert knowledge for both languages In this research we present the availability of developing the translator for Sinhala-Tamil languages pair using monolingual corpus dataset In this paper the Byte Pair Encoding (BPE) is applied for overcoming the Out-Of-Vocabulary (OOV) problem in both Sinhala and Tamil languages Our first part of the research is using monolingual word embedding approach for developing the translation in between Sinhala-Tamil language pair only using monolingual corpora The second part of the research we use both parallel and monolingual corpus data with transformer architecture The BLEU score and the synonyms analysis are used to evaluate the approach we suggested',NLP
'The unprecedented use of Earths resources by humans in combination with increasing natural variability in natural processes over the past century is affecting the evolution of the Earth system To better understand natural processes and their potential future trajectories requires improved integration with and quantification of human processes Similarly to mitigate risk and facilitate socio-economic development requires a better understanding of how the natural system (eg climate variability and change extreme weather events and processes affecting soil fertility) affects human processes Our understanding of these interactions and feedback between human and natural systems has been formalized through a variety of modelling approaches However a common conceptual framework or set of guidelines to model human-natural-system feedbacks is lacking The presented research lays out a conceptual framework that includes representing model coupling configuration in combination with the frequency of interaction and coordination of communication between coupled models Four different approaches used to couple representations of the human and natural system are presented in relation to this framework which vary in the processes represented and in the scale of their application From the development and experience associated with the four models of coupled human-natural systems the following eight lessons were identified that if taken into account by future coupled human-natural-systems model developments may increase their success: (1) leverage the power of sensitivity analysis with models (2) remember modelling is an iterative process (3) create a common language (4) make code open-access (5) ensure consistency (6) reconcile spatio-temporal mismatch (7) construct homogeneous units and (8) incorporating feedback increases non-linearity and variability Following a discussion of feedbacks a way forward to expedite model coupling and increase the longevity and interoperability of models is given which suggests the use of a wrapper container software a standardized applications programming interface (API) the incorporation of standard names the mitigation of sunk costs by creating interfaces to multiple coupling frameworks and the adoption of reproducible workflow environments to wire the pieces together',NLP
'[Context and motivation] Natural language is the main presentation means in industrial requirements documents In such documents system behavior is specified either in the form of scenarios or in the form of automata described in natural language The behavior descriptions are often incomplete: For the authors of requirements documents some facts are so obvious that they forget to mention them; this surely causes problems for the requirements analyst [Question/problem] Formalization of textual behavior description can reveal deficiencies in requirements documents Formalization can take two major forms: it can be based either on interaction sequences or on automata Cf Survey [1] Translation of textual scenarios to interaction sequences (Message Sequence Charts or MSCs) was presented in our previous work [234] To close the gap and to provide translation techniques for both formalism types an algorithm translating textual descriptions of automata to automata themselves is necessary [Principal ideas/results] It was shown in our previous work that discourse context modeling allows to complete information missing from scenarios written in natural language and to translate scenarios to MSCs The goal of the approach presented in this paper is to translate textual descriptions of automata to automata themselves by adapting discourse context modeling to texts describing automata [Contribution] The presented paper shows how the previously developed context modeling approach can be adapted in order to become applicable to texts describing automata The proposed approach to translation of text to automata was evaluated on a case study which proved applicability of the approach',NLP
'End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently diagnosing failures in these tests and maintaining them can be tedious Techniques for computing natural-language descriptions of test cases can help increase test readability However so far such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases In this paper we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications We present two techniques that apply natural-language processing to information available in the browser document object model (DOM) The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation However supervised approach requires a training dataset Sowe propose the second technique which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically We implemented these techniques along with two simpler baseline techniques in a tool called CrawLabel (available as a plugin to Crawljax a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications Our results indicate that the supervised approach can achieve precision recall and F1-score of 8338 6064 and 6640 respectively The unsupervised approach although less effective is competitive achieving scores of 7237 5812 and 5977 We highlight key results and discuss the implications of our findings',NLP
'In developing a system to help CTICU physicians write patient notes we hypothesized that a spoken language interface for entering observations after physical examination would be more convenient for a physician than a more traditional menu-based system We developed a prototype spoken language interface allowing input of one type of information with which we could experiment with factors impacting use of speech In this paper we report on a sequence of experiments where we asked physicians to use different interfaces testing how such a system could be used as part of their workflow as well as its accuracy in different locations with different levels of domain information Our study shows that we can significantly improve accuracy with integration of patient specific and high coverage domain grammars',NLP
'Nonstandard words such as proper nouns abbreviations and acronyms are a major obstacle in natural language text processing and information retrieval Acronyms in particular are difficult to read and process because they are often domain-specific with high degree of polysemy In this paper we propose a language modeling approach for the automatic disambiguation of acronym senses using context information First a dictionary of all possible expansions of acronyms is generated automatically The dictionary is used to search for all possible expansions or senses to expand a given acronym The extracted dictionary consists of about 17 thousands acronym-expansion pairs defining 1829 expansions from different fields where the average number of expansions per acronym was 947 Training data is automatically collected from downloaded documents identified from the results of search engine queries The collected data is used to build a unigram language model that models the context of each candidate expansion At the in-context expansion prediction phase the relevance of acronym expansion candidates is calculated based on the similarity between the context of each specific acronym occurrence and the language model of each candidate expansion Unlike other work in the literature our approach has the option to reject to expand an acronym if it is not confident on disambiguation We have evaluated the performance of our language modeling approach and compared it with tf-idf discriminative approach',NLP
'The traditional intelligent tutoring system as a web-based education tools used for adaptive learning cant solve the encountered question in time while the student is learning with the absence of nature language understanding system In this paper a web-based intelligent tutoring system is firstly introduced to solve the encountered question in real time Secondly a concept model representing the concept connotation the extension and the relation between them is presented to support the nature language understanding system in order to extract the questions meaning Furthermore a knowledge representation of verb and noun in impersonal mature domain is involved Thirdly the semantic processing arithmetic of the verb phase is given Finally the result of experiment and application of arithmetic are shown',NLP
'Information retrieval is an important direction in the area of natural language processing This paper introduced semidiscrete matrix decomposition in latent semantic indexing We aimed at its disadvantage in storage space and presented SSDD then we compare the difference of SVD and SDD and SSDD in performance',NLP
'We describe the derivations in a pregroup grammar as the 2-cells of a free compact 2-category defined by the grammar The 2-cells of this category are the intermediary parsing structures necessary for a semantic interpretation when pregroups are used in natural language processing The construction of the free compact 2-category also provides another cut-free axiomatisation of compact bilinear logic',NLP
'Respecify is a new web-based requirements authoring tool using constrained natural language (CNL) to guide the authoring process and drive the creation of alternative views that elucidate the complexity of specifications thus reducing the occurrence of certain types of requirements quality defects This tool demo motivates Respecifys existence its key features and evaluates its current use in industry',NLP
'Artificial Intelligence has guided technological progress in recent years; it has shown significant development with increased academic studies on Machine Learning and the high demand for this field in the sector In addition to the advancement of technology day by day the pandemic which has become a part of our lives since early 2020 has led to social media occupying a larger place in the lives of individuals Therefore social media posts have become an excellent data source for the field of sentiment analysis The main contribution of this study is based on the Natural Language Processing method which is one of the machine learning topics in the literature Sentiment analysis classification is a solid example for machine learning tasks that belongs to human-machine interaction It is essential to make the computer understand people emotional situation with classifiers There are a limited number of Turkish language studies in the literature Turkish language has different types of linguistic features from English Since Turkish is an agglutinative language it is challenging to make sentiment analysis with that language This paper aims to perform sentiment analysis of several machine learning algorithms on Turkish language datasets that are collected from Twitter In this research besides using public dataset that belongs to Beyaz (2021) to get more general results another dataset is created to understand the impact of the pandemic on people and to learn about public opinions Therefore a custom dataset namely SentimentSet (Balli 2021) was created consisting of Turkish tweets that were filtered with words such as pandemic and corona by manually marking as positive negative or neutral Besides SentimentSet could be used in future researches as benchmark dataset Results show classification accuracy of not only up to similar to 87% with test data from datasets of both datasets and trained models but also up to similar to 84% with small Sample Test Data generated by the same methods as SentimentSet dataset These research results contributed to indicating Turkish language specific sentiment analysis that is dependent on language specifications',NLP
'In task-oriented dialogue systems spoken language understanding (SLU) aims to convert users queries expressed by natural language to structured representations SLU usually consists of two parts namely intent identification and slot filling Although many methods have been proposed for SLU these methods generally process each utterance individually which loses context information in dialogues In this paper we propose a hierarchical LSTM based model for SLU The dialogue history is memorized by a turn-level LSTM and it is used to assist the prediction of intent and slot tags Consequently the understanding of the current turn is dependent on the preceding turns We conduct experiments on the NLPCC 2018 Shared Task 4 dataset The results demonstrate that the dialogue history is effective for SLU and our model outperforms all baselines',NLP
'There are two basic approaches for semantic processing in spoken language understanding: a rule based approach and a statistic approach In this paper we combine both of them in a novel way by using statistical and syntactical dynamic bayesian networks (DBNs) together with Graphical Models (GMs) for spoken language understanding (SLU) GMs merge in a complex mathematical way probability with graph theory This results in four different setups which raise in their complexity Comparing our results to a baseline system we achieve a F1-measure of 937% in word classes and 957% in concepts for our best setup in the ATIS-Task This outperforms the baseline system relatively by 37% in word classes and by 82% in concepts The expermiments were performend with the graphical model toolkit (GMTK)',NLP
'Right-one way jumping finite automata are deterministic devices that process their input in a discontinuous fashion We generalise these devices to nondeterministic machines More precisely we study the impact on the computational power of these machines when allowing multiple initial states and/or a nondeterministic transition function including spontaneous or lambda-transitions We show inclusion relations and incomparability results of the induced language families Since for right-one way jumping devices the use of spontaneous transitions is subject to different natural interpretations we also study this subject in detail showing that most interpretations are equivalent to each other and lead to the same language families Finally we also study inclusion and incomparability results to classical language families and to the families of languages accepted by finite automata with translucent letters (c) 2021 Published by Elsevier Inc',NLP
'Dialectal Arabic (DA) poses serious challenges for Natural Language Processing (NLP) The number and sophistication of tools and datasets in DA are very limited in comparison to Modern Standard Arabic (MSA) and other languages MSA tools do not effectively model DA which makes the direct use of MSA NLP tools for handling dialects impractical This is particularly a challenge for the creation of tools to support learning Arabic as a living language on the web where authentic material can be found in both MSA and DA In this paper we present the Dialectal Arabic Linguistic Learning Assistant (DALILA) a Chrome extension that utilizes cutting-edge Arabic dialect NLP research to assist learners and non-native speakers in understanding text written in either MSA or DA DALILA provides dialectal word analysis and English gloss corresponding to each word',NLP
'Linguistic complexity research being a very actively developing field an increasing number of text analysis tools are created that use natural language processing techniques for the automatic extraction of quantifiable measures of linguistic complexity While most tools are designed to analyse only one language the CTAP open source linguistic complexity measurement tool is capable of processing multiple languages making cross-lingual comparisons possible Although it was originally developed for English the architecture has been extended to support multi-lingual analyses Here we present the Italian component of CTAP describe its implementation and compare it to the existing linguistic complexity tools for Italian Offering general text length statistics and features for lexical syntactic and morpho-syntactic complexity (including measures of lexical frequency lexical diversity lexical and syntactical variation part-ofspeech density) CTAP is currently the most comprehensive linguistic complexity measurement tool for Italian and the only one allowing the comparison of Italian texts to multiple other languages within one tool',NLP
'Code-Mixing (CM) is a very commonly observed mode of communication in a multilingual configuration The trends of using this newly emerging language has its effect as a culling option especially in platforms like social media This becomes particularly important in the context of technology and health where expressing the upcoming advancements is difficult in native language Despite the change of such language dynamics current dialog systems cannot handle a switch between languages across sentences and mixing within a sentence Everyday conversations are fabricated in this mixed language and analyzing dialog acts in this language is very essential in further advancements of making interaction with personal assistants more natural The problem is further compounded with crossing the script barriers in code-mixing In this paper we take the first step towards understanding code-mixing in dialog processing by recognizing dialog act (intention) of the code-mixed utterance Considering the dearth of resources in code-mixed languages we design our current system using only word level resources such as language identification transliteration and lexical translation Our best performing system is HMM based with an F-score of 7667',NLP
'Zigzag conversational patterns of contents in social media are often perceived as noisy or informal text Unrestricted usage of vocabulary in social media communications complicates the processing of code-mixed text This paper accentuates two major aspects of code mixed text: Offensive Language Identification and Sentiment Analysis for Malayalam-English code-mixed data set The proffered framework addresses 3 key points apropos these tasks-dependencies among features created by embedding methods (Word2Vec and FastText) comparative analysis of deep learning algorithms (uni-/bi-directional models hybrid models and transformer approaches) relevance of selective translation and transliteration and hyper-parameter optimization-which ensued in F1-Scores (models accuracy) of 076 for Forum for Information Retrieval Evaluation (FIRE) 2020 and 099 for European Chapter of the Association for Computational Linguistics (EACL) 2021 data sets A detailed error analysis was also done to give meaningful insights The submitted strategy turned in the best results among the benchmarked models dealing with Malayalam-English code-mixed messages and it serves as an important step towards societal good',NLP
'This paper introduces a system for generating questions automatically for Punjabi The System transforms a declarative sentence into its interrogative counterpart It accepts sentences as an input and produces possible set of questions for the given input Not much work has been done in the field of Question Generation for Indian Languages The current paper represents the Question Generation System for Punjabi language to generate questions for the given input in Gurmukhi script For Punjabi adequate annotated corpora POS taggers and other NLP tools are not yet available in the required measure Thus this system relies on the Named Entity Recognition tool Also various Punjabi Language dependent rules have been developed to generate output based on the named entity found in the given input sentence',NLP
'Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various downstream tasks These pretraining methods are frequently extended with recurrence adversarial or linguistic propertymasking Recently contrastive self-supervised training objectives have enabled successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar In NLP however a single token augmentation can invert the meaning of a sentence during input-input contrastive learning which led to input-output contrastive approaches that avoid the issue by instead contrasting over input-label pairs In this primer we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling zero to few-shot learning pretraining data-efficiency and specific NLP tasks We overview key contrastive learning concepts with lessons learned from prior research and structure works by applications Finally we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining',NLP
'This paper presents an intelligent question answering system based on ontology which adopts Chinese natural language processing technology to generate concise and appropriate answers to the students questions The accuracy and intelligence of the system are guaranteed through ontology based knowledge description word segmentation and question pattern recognition',NLP
'We discuss the nature and the scope of linguistic (morphological syntactic and semantic) variation of terms and its impact on two information retrieval tasks: term acquisition and automatic indexing A review of natural language processing techniques existing in these two areas is done along with an in-depth presentation of FASTR a corpus processor for the recognition normalization and acquisition of multi-word terms',NLP
'The rise of online communication platforms has been accompanied by some undesirable effects such as the proliferation of aggressive and abusive behaviour online Aiming to tackle this problem the natural language processing (NLP) community has experimented with a range of techniques for abuse detection While achieving substantial success these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users disregarding the emotional state of the users and how this might affect their language The latter is however inextricably linked to abusive behaviour In this paper we present the first joint model of emotion and abusive language detection experimenting in a multi-task learning framework that allows one task to inform the other Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets',NLP
'In this paper we introduce Woolery a preliminary design and implementation of an intelligent annotator interface for structured documents (ie JSON) This annotator infers a mapping between the semantics of the keys in the document and an appropriate FrameNet structure which can then be exported for downstream natural language processing tasks',NLP
'Syntactic analysis is the core technology of natural language processing and it is the cornerstone for further linguistic analysis This paper first introduces the basic grammatical system and summary the technology of current parsing Then analysis the characteristics of probabilistic context-free grammars deep and introduce the method of improving for probabilistic context-free The last we point the difficulty of Chinese parsing',NLP
'A model for knowledge and reasoning based on self-organizing semantic networks is presented The self-organization is driven by locally active elements in the network which allows to model the cognitive process of understanding as an iteration of classification and segmentation both of which are particular locally active elements Applications to Natural Language Understanding are indicated',NLP
'This work demonstrates a NLP pipeline on classifying different canine parvovirus diagnosis from visit summaries The preliminary results show promising efficacy in employing BERT based models for this task This work also reveals a way to make use of the largely untapped data from veterinary free-text fields to help improve pet health',NLP
'While much promise has been demonstrated in the learning analytics field with sentiment analysis the analyses are typically post hoc The Unizin Sentiment Visualizer demonstrates that the application of sentiment analysis in real-time provides a powerful new tool to support students in complex learning environments',NLP
'Language is constantly evolving through speakers natural innovations However our understanding of how subtle linguistic innovations are processed is still surprisingly limited To date most studies investigating processing of non-conventional language have focused on metaphors Using both brain event-related potentials (ERPs) and offline judgements the present study investigated the acceptability as well as the temporal dynamics associated with processing novel and conventional multiword units By manipulating both the degree of similarity and degree of conventionality the results revealed that the acceptability of novel items hinges on their similarity to known multiword units ERPs revealed that similarity modulated a late positive component (LPC) 550-750 ms after stimulus presentation; this LPC was significantly correlated with acceptability Additionally processing of novel multiword units was not modulated by exposure to related multiword units indicating that similarity-based processing was not dependent on recent prior exposure',NLP
'Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing Recently a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI These models can be categorized into either single-stream or dual-stream encoders We study the differences between these two categories and show how they can be unified under a single theoretical framework We then conduct controlled experiments to discern the empirical differences between five vision and language BERTs Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results but they also reveal that the embedding layer plays a crucial role in these massive models',NLP
'Author summary In this manuscript authors explain the development and validation of a novel artificial intelligence approach to support and simplify the early steps of the process from SELEX to help discriminate binding between deoxynucleotide aptamers from those unspecified targets of DNA-binding sequences The approach was implemented based on Natural Language Processing and Machine Learning CountVectorizer a Natural Language Processing method was used to extract information from nucleotide sequences Four Machine Learning algorithms (Logistic Regression Decision Tree Gaussian Naive Bayes and Support Vector Machines) were trained using data from the Natural Language Processing method along with sequence information From these four trained machine learning algorithms the best performance and selected model was Support Vectors Machines because it had the best discriminatory metrics (ie Accuracy (A) = 0995; AUROC (AU) = 0998) In general all models showed good metric results for predicting DNA aptamer sequences The Machine Learning model complexity and difficult interpretation may hinder its application into the standard practice For this reason the development of a web-app is already taking place to facilitate the interpretation and application of the obtained results The selection of a DNA aptamer through the Systematic Evolution of Ligands by EXponential enrichment (SELEX) method involves multiple binding steps in which a target and a library of randomized DNA sequences are mixed for selection of a single nucleotide-specific molecule Usually 10 to 20 steps are required for SELEX to be completed Throughout this process it is necessary to discriminate between true DNA aptamers and unspecified DNA-binding sequences Thus a novel machine learning-based approach was developed to support and simplify the early steps of the SELEX process to help discriminate binding between DNA aptamers from those unspecified targets of DNA-binding sequences An Artificial Intelligence (AI) approach to identify aptamers were implemented based on Natural Language Processing (NLP) and Machine Learning (ML) NLP method (CountVectorizer) was used to extract information from the nucleotide sequences Four ML algorithms (Logistic Regression Decision Tree Gaussian Naive Bayes Support Vector Machines) were trained using data from the NLP method along with sequence information The best performing model was Support Vector Machines because it had the best ability to discriminate between positive and negative classes In our model an Accuracy (A) of 0995 the fraction of samples that the model correctly classified and an Area Under the Receiving Operating Curve (AUROC) of 0998 the degree by which a model is capable of distinguishing between classes were observed The developed AI approach is useful to identify potential DNA aptamers to reduce the amount of rounds in a SELEX selection This new approach could be applied in the design of DNA libraries and result in a more efficient and faster process for DNA aptamers to be chosen during SELEX',NLP
'Sentence classification is a fundamental task in natural language processing In this paper clickbait detection is taken as an example to study the sentence classification with a transferring network Clickbait are news headlines that exaggerate the facts or hide partial facts headlines Statistics show that clickbaits are prevalent among all languages However previous research on clickbait detection mainly focus on English exploiting lexical or syntactical features that are not shared by other languages On the other hand it would be both time-consuming and labor-intensive to annotate a clickbait dataset by humans Recently neural language model that represent each word by a real-valued dense vector show that words with similar meanings across languages are close to each other in the vector space Inspired by this transfer learning is proposed to be applied to transfer the model on clickbait detection from a source language to other languages with very few annotations This paper trains the source model on English corpus and transfers it to corpus in Chinese Experimental results show that transfer learning model in this paper can achieve similar performance on the target language using less annotation showing the effectiveness and robustness of this model',NLP
'Assamese is one of the regional languages of India spoken by the people of Assam and other north eastern states of India Parts Of Speech (POS) tagging is one of the most important research issue as it is the basic need for any Natural Language Processing (NLP) An automated way to provide a Parts Of Speech label to a word on a context is known as Parts Of Speech Tagging Assamese is one among the less computationally aware languages of India This paper presents our works on POS tagging for Assamese sentences using Conditional Random Field (CRF) and Transformation Based Learning (TBL) We obtain 8717 and 6773 percent tagging accuracy for TBL and CRF respectively that are train through a manually tagged corpus',NLP
'Lexicography is a domain of activity that must keep pace with the needs of the users Nowadays when there is a diversified range of users from the traditional kind (looking up a printed dictionary) to the engineer specialized in the natural language processing a new way of representing lexical knowledge has emerged: the semantic network the most widely known type of it being the wordnet Its advantage is that it makes such knowledge accessible both to humans and to machines In a wordnet content words are organized in synsets according to their meanings: each word occurs as many times as many meanings it has Synsets are interlinked by semantic relations of various types (hypo/hyperonymy meronymy/holonymy troponymy cause etc) Word forms can be further linked by lexical relations (synonymy antonymy derivational relations) Semantic relations are conceptual thus have cross-lingual validity while lexical relations are language specific A language resource such as a wordnet can be seen as a repository of more resources: as synsets are associated a gloss a wordnet can be looked up as an explanatory dictionary; from it one can extract various semantic relations dictionaries or even a derivational dictionary Once such resources are created for various languages and they are aligned (ie the corresponding synsets in the different languages are clearly marked and the semantic relations between them are considered to be the same) one can extract from them multilingual dictionaries in which the interlingual correspondences are marked at the word sense level not at the word level A wordnet is extremely valuable for natural language processing: it is like a thesaurus from which the semantic and lexical relations between semantically disambiguated words can be easily exploited in various tasks (such as question answering information retrieval and others) involving the expansion of the key words inserted by a user or calculating the semantic similarity between words',NLP
'We propose a new word embedding model called SPhrase that incorporates supervised phrase information Our method modifies traditional word embeddings by ensuring that all target words in a phrase have exactly the same context We demonstrate that including this information within a context window produces superior embeddings for both intrinsic evaluation tasks and downstream extrinsic tasks',NLP
'Annotation tools play an important role in building high-quality annotation corpus Although existing text annotation tools may provide many features to meet different needs for text annotation an easy-to-install tool for annotating clinical narrative documents is still demanded In response we developed MedTator a serverless web-based tool for corpus annotation',NLP
'An alternative to compressed suffix arrays is introduced based on representing a sequence of integers using Fibonacci encodings thereby reducing the space requirements of state-of-the-art implementations of the suffix array while retaining the searching functionalities Empirical tests support the theoretical space complexity improvements and show that there is no deterioration in the processing times',NLP
'We are concerned with the automatic semantic interpretation of legal modificatory provisions We propose a novel approach which pairs deep syntactic parsing and a fine-grained taxonomy of legal modifications Although still in a developmental stage the implemented system can be used to annotate with meta-information modificatory provisions of NormaInRete documents',NLP
'Large-scale graph computation is central to applications ranging from language processing to social networks However natural graphs tend to have skewed power-law distributions where a small subset of the vertices have a large number of neighbors Existing graph-parallel systems suffer from load imbalance high communication cost or suboptimal and complex processing In this paper we present GraphA an Adaptive approach to efficient partitioning and computation of large-scale natural graphs GraphA provides an adaptive and uniform graph partitioning algorithm which partitions the datasets in a load-balanced manner by using an incremental number of hash functions We have implemented GraphA both on Spark and on GraphLab Extensive evaluation shows that GraphA remarkably outperforms state-of-the-art graph-parallel systems (GraphX and PowerLyra) in ingress time execution time and storage overhead for both real-world and synthetic graphs',NLP
'The method of teaching with help of four elementary skills - reading speaking writing and listening is definitely inconceivable to teaching any foreign language But automatic answers following the exercise and students passive memorising does not make speaking language and its learning very natural This has forced me to find and create the way how to make learners think and realise the point and meaning of learning itself My paper is about special skill I have tried to apply - thinking skill I have chosen it to compliment other four which teacher normally uses when teaching foreign language (reading speaking listening skills) At the same time I put thinking skill into a role to support and enhance learning process To find and use some methods how to make students think make their own opinion and also teach them to apply their own experience to learning process was the main goal of this survey The methods I have applied were provoking students to think before they learn',NLP
'Past maintenance logs may encapsulate meaningful data for predicting the duration of machine breakdowns the potential causes of a problem or the necessity to stop production to perform repair activities These insights may be accessed using machine learning (ML) However maintenance logs tend to have imbalanced distributions and rely on noisy unstructured text data provided by operators Additionally the limited interpretability of ML models results in human reluctance when accepting model predictions Hence this study explored the use of two recent deep learning models (CamemBERT and FlauBERT) for natural language processing (NLP) to harness unstructured data from maintenance logs The class imbalance effect was mitigated using data-level and algorithm-level approaches To improve interpretability a technique called LIME was employed to interpret single predictions and to propose a method for insight extraction from several maintenance reports Results suggest three key points: CamemBERT and FlauBERT can achieve excellent results with minimum text pre-processing and hyperparameter tuning Second random oversampling (ROS) generally mitigates the effect of class imbalance However ROS was observed to be unnecessary when performing pertinent data pre-processing Finally at the maintenance level the proposed insight extraction method can provide valuable information from a set of poorly structured maintenance reports',NLP
'While automatic keyphrase extraction has been examined extensively state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks We present a survey of the state of the art in automatic keyphrase extraction examining the major sources of errors made by existing systems and discussing the challenges ahead',NLP
'Efficient monitoring of textual information seems to be important for crime detection and crime investigation This paper describes the functionality and the architecture of the MPI system which was developed for Web information monitoring It is worthwhile to say that some system modules may function in a business environment',NLP
'The modern electronic dictionaries of natural languages should be universal In the linguistic aspects they should be a multilinked database similar in their contents to the combinatorial dictionary by I Melcuk but with more stress on thesaurical links and word combinations In interface aspects they should have their data accessible to a text processing software a human user and a lexicographer',NLP
'We present an approach to text summarization that is entirely rooted in a formal model of terminological knowledge representation and reasoning Text summarization is considered an operator-based transformation process Knowledge representation structures as generated by a natural language text understanding system are mapped by salience merge and generalization operators into conceptually more abstract knowledge structures forming a text summary at the representational level',NLP
'In recent years multi-objective optimization (MOO) techniques have become popular due to their potentiality in solving a wide variety of real-world problems including bioinformatics wireless networks natural language processing image processing astronomy and astrophysics and many more In the current paper we have presented a survey of recently developed MOO-based algorithms Some of the applications along with possible future research directions are also discussed',NLP
'In the context of disaster management geospatial information plays a crucial role in the decision-making process to protect and save the population Gathering a maximum of information from different sources to oversee the current situation is a complex task due to the diversity of data formats and structures Although several approaches have been designed to integrate data from different sources into an ontology they mainly require background knowledge of the data However non-standard data set schema (NSDS) of relational geospatial data retrieved from eg web feature services are not always documented This lack of background knowledge is a major challenge for automatic semantic data integration Focusing on this problem this article presents an automatic approach for geospatial data integration in NSDS This approach does a schema mapping according to the result of an ontology matching corresponding to a semantic interpretation process This process is based on geocoding and natural language processing This article extends work done in a previous publication by an improved unit detection algorithm data quality and provenance enrichments the detection of feature clusters It also presents an improved evaluation process to better assess the performance of this approach compared to a manually created ontology These experiments have shown the automatic approach obtains an error of semantic interpretation around 10% according to a manual approach',NLP
'We present a method for constructing a statistical machine translation system automatically from unannotated examples in a manner consistent with the principles of dependency grammar The method involves learning a generative statistical model of paired dependency derivations of source and target sentences Such a dependency transduction model consists of collections of weighted head transducers Head transducers are finite-state machines with different formal properties from standard finite-state transducers When applied to machine translation the acquired head transducers are applied middle out efficiently converting source head words and dependents directly into their counterparts in the target language We present experimental results on the accuracy of our models for English-Spanish and English-Japanese translation the training examples being pairs of transcribed spontaneous utterances and their translations A hierarchical decomposition of bi-language strings emerges from our training process; this decomposition may or may not correspond to familiar linguistic phrase structure However no explicit semantic representations are involved suggesting an approach to language processing in which natural language itself is the semantic representation',NLP
'Cross-Language Plagiarism refers to the unacknowledged reuse of a text involving its translation from one natural language to another without proper referencing to the original source One of the common problems in data processing is efficient large-scale text comparison especially semantic based similarity due to the increase in the number of publications and the rate of suspicious documents sources of plagiarism CLPD nature could be more complicated than simple copy+translate and paste thus the detecting process exposes the need for a vague concept and fuzzy sets techniques in a big data environment to reveal dishonest practices in Arabic documents In this paper we propose a new Cross-Language Plagiarism Detection based on fuzzy-semantic similarity using WordNet and two semantic approaches Wu&Palmer and Lin; the work is done in a parallel way using Apache Hadoop with its distributed file system HDFS and the MapReduce programming model The experimental results show that the Fuzzy Wu & Palmer have high performance than Fuzzy Lin',NLP
'This paper presents a brief review of work done on the visualisation of texts Work done in this area is divided into two broad categories: those that focus on visualising a large set of documents and those that focus on visualising individual text files A total of 6 programs is reviewed each representing a unique method of visualising texts',NLP
'Knowledge of which lexical items convey the same meaning in a given context is important for many Natural Language Processing tasks This paper concerns the substitutability of discourse connectives in particular This paper proposes a data-driven method based on a Minimum Description Length (MDL) criterion for automatically learning substitutability of connectives The method is shown to outperform two baseline classifiers',NLP
'The division of internal structures and external space of geographical entities is foundation of spatial analysis query and reasoning Most current division methods are crisp and inconsistent with human cognitive habits Usually existing geographic information systems(GISs) analyze spatial data directly by certain spatial analysis methods and then use natural language or words to explain analysis results so the encoding process is absence but it is necessary in intelligent GIS (IGIS) Fuzzy geographical entities and phenomena occur throughout the real world and these semantics of words related spatial locations and relations usually involve uncertainties First the geographical perceptual computing (GPC) model based on CWW is proposed and it includes four modules: input geo-encoder geographical CWW engine and geo-decoder Then the trapezoidal fuzzy set is adopted to represent spatial words A fine fuzzy spatial partitioning model of line objects based on CWW is proposed The interior of a line object is divided into several parts according to fuzzy logic and human cognitive habits The exterior of a line entity is then divided into several parts by combining direction relation and distance relation models with fuzzy logic methods This model provides a full natural language description of the interior and exterior of crisp or fuzzy line entities and consistent with human cognitive habits An application case of this model is provided in the last and proves the superiority of this model',NLP
'Media is created by humans for humans to tell stories There exists a natural and imminent need for creating human-centered media analytics to illuminate the stories being told and to understand their impact on individuals and society at large An objective understanding of media content has numerous applications for different stakeholders from creators to decision-/policy-makers to consumers Advances in multimodal signal processing and machine learning (ML) can enable detailed and nuanced characterization of media content (of who what how where and why) at scale They can also aid our understanding of the impact of media on a range of issues including individual experiences behavioral cultural and societal trends and commercial outcomes Modern deep learning models combined with audiovisual signal processing can analyze entertainment media such as Film & TV content to quantify gender age and race representations This creates awareness in an objective way that was hitherto impossible On the other hand text mining and natural language processing allow nuanced understanding of language use and spoken interactions in media such as News to track patterns and trends across different contexts Moreover advances in human sensing have enabled us to directly measure the influence of media on an individuals physiology (and brain) while social media analysis enables tracking the societal impact of media content on different cross sections of the society This article reviews representative methodologies and algorithms tools and systems advancing human-centered media understanding through ML in the pursuit of developing computational media intelligence',NLP
'We introduce DisCoPy an open source toolbox for computing with monoidal categories The library provides an intuitive syntax for defining string diagrams and monoidal functors Its modularity allows the efficient implementation of computational experiments in the various applications of category theory where diagrams have become a lingua franca As an example we used DisCoPy to perform natural language processing on quantum hardware for the first time',NLP
'The commercialisation of natural language processing began over 35 years ago but its only in the last year or two that its become substantially more visible largely because of the intense popular interest in artificial intelligence So whats the state of commercial NLP today? We survey the main industry categories of relevance and offer comment on where the action is today',NLP
'The paper introduces the method of automation indexing for Chinese archives The authors have made some research on the algorithm of subject indexing and classification indexing The weight function of indexing descriptors and the thesaurus of keywords-descriptors are organized reasonably The fuzzy relation is used in the classification indexing of archives',NLP
'Natural language processing and machine learning can be applied to student feedback to help university administrators and teachers address problematic areas in teaching and learning The proposed system analyzes student comments from both course surveys and online sources to identify sentiment polarity the emotions expressed and satisfaction versus dissatisfaction A comparison with direct-assessment results demonstrates the systems reliability',NLP
'There is a renewed interest in word sense disambiguation (WSD) as it contributes to various applications in natural language processing In this paper we survey vector-based methods for WSD in machine learning All the methods are corpus-based and use definition of context in the sense introduced by S Marcus [11]',NLP
'This paper deals with important but underestimated aspect of research - presenting to the general public Amongst many outreach activities Natural Language Processing Centre takes part in Masaryk University project of courses for children 9 to 14 years old We describe specifics to think of when presenting NLP topics to children and use cases of previous and planned courses',NLP
'Paper presents an exploratory case study comparing stemming and lemmatization results for the automatic application of large-scale controlled vocabularies processed against archival encyclopedia entries The results report relative recall and precision evaluations across both results Research shows that while stemming has a higher relative recall lemmatization results in a higher relevance score and eliminates the over-stemming challenges Results provide insight into improving automatic curation workflows for archival resources',NLP
'This presentation is a case study examining how LexisNexis uses scaled active learning on the HPCC Systems environment to focus manual topical annotations on critical documents pulled from a large corpus The active learning system uses natural language processing and machine learning techniques to identify and present next best training set candidates to legal editors combining massive parallel processing with expert human analysis to improve classifier accuracy while minimizing human effort',NLP
'The task of analyzing sentiment has been extensively researched for a variety of languages However due to a dearth of readily available Natural Language Processing methods Urdu sentiment analysis still necessitates additional study by academics When it comes to text processing Urdu has a lot to offer because of its rich morphological structure The most difficult aspect is determining the optimal classifier Several studies have incorporated ensemble learning into their methodology to boost performance by decreasing error rates and preventing overfitting However the baseline classifiers and the fusion procedure limit the performance of the ensemble approaches This research made several contributions to incorporate the symmetries concept into the deep learning model and architecture: firstly it presents a new meta-learning ensemble method for fusing basic machine learning and deep learning models utilizing two tiers of meta-classifiers for Urdu The proposed ensemble technique combines the predictions of both the inter- and intra-committee classifiers on two separate levels Secondly a comparison is made between the performance of various committees of deep baseline classifiers and the performance of the suggested ensemble Model Finally the studys findings are expanded upon by contrasting the proposed ensemble approach efficiency with that of other more advanced ensemble techniques Additionally the proposed model reduces complexity and overfitting in the training process The results show that the classification accuracy of the baseline deep models is greatly enhanced by the proposed MLE approach',NLP
'The process of Darwinian evolution by natural selection was inoculated into four artificial worlds (virtual computers) These systems were used for a Comparative study of the rates degrees and patterns of evolutionary optimizations showing that many features of the evolutionary process are sensitive to the structure of the underlying genetic language Some specific examples of the evolution of increasingly complex structures are described In addition a measure of entropy (diversity) of the evolving ecological community over time was used to study the relationship between evolution and entropy',NLP
'VeriDevOps aims at bringing together fast and cost-effective security verification through formal modelling and verification as well as test generation selection execution and analysis capabilities to enable companies to deliver quality systems with confidence in a fast-paced DevOps environment Security requirements are intended to be processed using NLP advanced algorithms in order to deliver formal specifications of security properties to be checked during development and operation of a system under test',NLP
'This paper presents a general outline related to the numerical classification generalization applied to sound documents The approach put forward combines concepts pertaining to natural language processing and frequency analysis The raw data of the audio documents are converted into strings of alphanumerical characters that are well suited to numerical analysis Thus processing similar to that employed to classify text documents can be applied Combining text and audio analyses makes it possible to generate a system that can categorize large corpuses of heterogeneous documents',NLP
'This paper addresses the problem of extracting analyzing and synthesizing valuable information from continuous text streams covering financial information A text mining framework combining elements from information retrieval information extraction and natural language processing has been implemented The framework is utilized to extract information regarding key actors in the domain how they relate to each other and how these characteristics evolve over time',NLP
'Image Captioning is an emergent topic of research in the domain of artificial intelligence (AI) It utilizes an integration of Computer Vision (CV) and Natural Language Processing (NLP) for generating the image descriptions It finds use in several application areas namely recommendation in editing applications utilization in virtual assistance etc The development of NLP and deep learning (DL) models find useful to derive a bridge among the visual details and textual semantics In this view this paper introduces an Oppositional Harris Hawks Optimization with Deep Learning based Image Captioning (OHHODLIC) technique The OHHO-DLIC technique involves the design of distinct levels of pre-processing Moreover the feature extraction of the images is carried out by the use of EfficientNet model Furthermore the image captioning is performed by bidirectional long short term memory (BiLSTM) model comprising encoder as well as decoder At last the oppositional Harris Hawks optimization (OHHO) based hyperparameter tuning process is performed for effectively adjusting the hyperparameter of the EfficientNet and BiLSTM models The experimental analysis of the OHHO-DLIC technique is carried out on the Flickr 8k Dataset and a comprehensive comparative analysis highlighted the better performance over the recent approaches',NLP
'Machine reading comprehension (MRC) on real web data which means finding answers from a set of candidate passages for a question is a quite arduous task in natural language processing Most state-of-the-art approaches select answers from all passages or from only one single golden paragraph which may cause the overlapping information and the lack of key information To address these problems this paper proposes a hierarchical answer selection framework that can select main content from a set of passages based on the question and predict final answer within this main content Specifically three main parts are employed in this pipeline: First the passage selection model uses a classification mechanism to select passages by passages content and title information which is not fully used in other models; Second a key sentences sequence selection mechanism is modeled by Markov-Decision-Process (MDP) in order to gain as much as effectual answer information as possible; Finally a match-LSTM model is employed to extract the final answer from the selected main content These three modules that shared the same attention-based semantic network and we conduct experimental on DuReader search dataset The results show that our framework outperforms the baseline by a large margin',NLP
'Public hospitals receive and triage a large volume of medical referrals for otorhinolaryngology annually and it can be a challenge to derive knowledge from them as they are written in unstructured text and may be unavailable in electronic formats Acquiring knowledge and insights from these referrals are important to public health management and policymakers Triaging of general practitioner (GP) referrals for ear nose and throat (ENT) specialists is a manual process performed by experienced clinicians but it is time-consuming This paper proposes utilising machine learning and data mining to automate the process of referrals In this study an ensemble of machine learning algorithms to perform clinical text mining against the unstructured referral text in order to derive the relationship among the discovered medical terms was proposed and implemented A set of comprehensive term sets association rules which describe the entire referral datasets characteristics was obtained from the association rule mining experiments The neural network-based text classification model that can classify referrals with high accuracy was developed tested and reported in this paper',NLP
'We present an approach for ontology population from natural language English texts that extracts RDF triples according to FrameBase a Semantic Web ontology derived from FrameNet Processing is decoupled in two independently-tunable phases First text is processed by several NLP tasks including Semantic Role Labeling (SRL) whose results are integrated in an RDF graph of mentions ie snippets of text denoting some entity/fact Then the mention graph is processed with SPARQL-like rules using a specifically created mapping resource from NomBank/PropBank/FrameNet annotations to FrameBase concepts producing a knowledge graph whose content is linked to DBpedia and organized around semantic frames ie prototypical descriptions of events and situations A single RDF/OWL representation is used where each triple is related to the mentions/tools it comes from We implemented the approach in PIKES an open source tool that combines two complementary SRL systems and provides a working online demo We evaluated PIKES on a manually annotated gold standard assessing precision/recall in (i) populating FrameBase ontology and (ii) extracting semantic frames modeled after standard predicate models for comparison with state-of-the-art tools for the Semantic Web We also evaluated (iii) sampled precision and execution times on a large corpus of 110 K Wikipedia-like pages',NLP
'One of the most significant recent advances in health information systems has been the shift from paper to electronic documents While research on automatic text and image processing has taken separate paths there is a growing need for joint efforts particularly for electronic health records and biomedical literature databases This work aims at comparing text-based versus image-based access to multimodal medical documents using state-of-the-art methods of processing text and image components A collection of 180 medical documents containing an image accompanied by a short text describing it was divided into training and test sets Content-based image analysis and natural language processing techniques are applied individually and combined for multimodal document analysis The evaluation consists of an indexing task and a retrieval task based on the gold standard codes manually assigned to corpus documents The performance of text-based and image-based access as well as combined document features is compared Image analysis proves more adequate for both the indexing and retrieval of the images In the indexing task multimodal analysis outperforms both independent image and text analysis This experiment shows that text describing images can be usefully analyzed in the framework of a hybrid text/image retrieval system',NLP
'Transformation Based Learning (TBL) is an intensively Machine Learning algorithm frequently used in Natural Language Processing TBL uses rule templates to identify error-correcting patterns A critical requirement in TBL is the availability of a problem domain expert to build these rule templates In this work we propose an evolutionary approach based on Genetic Algorithms to automatically implement the template selection process We show some empirical evidence that our approach provides template sets with almost the same quality as human built templates',NLP
'By analyzing the properties of Chinese words and sentences a new syntax analyzer CWDS for Chinese language is proposed This system put the emphasis on the central verb Using the meaning of sentence and the Chinese word-formation rule this system does the analysis and distinguishing word and automatically transfer the natural sentence into the concept structure representation of the sentence This method integrates the process of distinguishing word and the process of syntactic analysis',NLP
'This paper describes a universal concept representational mechanism called E-HowNet to handle difficulties caused by unknown words in natural language processing Semantic structures and sense disambiguation of unknown words are discovered by analogy We intend to achieve that any concept can be defined by E-HowNet and the representation is near-canonical The design for easy semantic composition and decomposition makes the automation of semantic processing for unknown words phrases and even sentences possible',NLP
'Extend an ontology is a complex task which require a considerable amount of decision makings This paper will study the possibility of develop an automatic ontology-driven system which will be able to extend an ontology with a satisfactory recall and precision levels extracting information from semi-structured XML texts',NLP
'System requirements specification describes technical concerns of a system and is used throughout the project life-cycle Requirements specification helps sharing the system vision among its stakeholders as well facilitating the communication project management and system development processes For an effective communication everyone communicates by means of a common language and natural language provides the foundations for such language Although natural language is the most common and preferred form of requirements representation it also exhibits intrinsic characteristics that often present themselves as the root cause of many requirements quality problems such as incorrectness inconsistency incompleteness and ambiguousness This paper presents the RSL (short name for Requirements Specification Language) which is a language to improve the production of requirements specifications in a more systematic rigorous and consistent way RSL includes constructs logically arranged into views according to the specific requirement engineering concerns they address These constructs are defined as linguistic patterns and are represented textually by multiple linguistic styles Due to space constraints this paper focuses only on its business level constructs and views namely on glossary terms stakeholders business goals processes events and flows RSL can be used and applied by different types of users such as requirement engineers business analysts or domain experts They can produce system requirements specifications with RSL at different level of detail considering different writing styles and different types of requirements (eg business goals system goals functional requirements quality requirements constraints user stories and use cases) In addition they can use other types of constructs (eg terms stakeholders actors data entities) that in spite of not being requirements are important to complement and enrich the specification of such requirements Based on a simple running example we also show how RSL users (ie requirements engineers and business analysts) can produce requirements specifications in a more systematic and rigorous way',NLP
'The quality and maintainability of a knowledge graph are determined by the process in which it is created There are different approaches to such processes; extraction or conversion of available data in the web (automated extraction of knowledge such as DBpedia from Wikipedia) community-created knowledge graphs often by a group of experts and hybrid approaches where humans maintain the knowledge graph alongside bots We focus in this work on the hybrid approach of human edited knowledge graphs supported by automated tools In particular we analyse the editing of natural language data ie labels Labels are the entry point for humans to understand the information and therefore need to be carefully maintained We take a step toward the understanding of collaborative editing of humans and automated tools across languages in a knowledge graph We use Wikidata as it has a large and active community of humans and bots working together covering over 300 languages In this work we analyse the different editor groups and how they interact with the different language data to understand the provenance of the current label data',NLP
'In this paper we present an automatic text simplification system for Spanish which intends to make texts more accessible for users with cognitive disabilities This system aims at reducing the structural complexity of Spanish sentences in that it converts complex sentences in two or more simple sentences and therefore reduces reading difficulty',NLP
'The increasing amount of valuable unstructured textual information poses a major challenge to extract value from those texts We need to use NLP (Natural Language Processing) techniques most of which rely on manually annotating a large corpus of text for its development and evaluation Creating a large annotated corpus is laborious and requires suitable computational support There are many annotation tools available but their main weaknesses are the absence of data management features for quality control and the need for a commercial license As the quality of the data used to train an NLP model directly affects the quality of the results the quality control of the annotations is essential In this paper we introduce ERAS a novel web-based text annotation tool developed to facilitate and manage the process of text annotation ERAS includes not only the key features of current mainstream annotation systems but also other features necessary to improve the curation process such as the inter-annotator agreement self-agreement and annotation log visualization for annotation quality control ERAS also implements a series of features to improve the customization of the users annotation workflow such as: random document selection re-annotation stages and warmup annotations We conducted two empirical studies to evaluate the tools support to text annotation and the results suggest that the tool not only meets the basic needs of the annotation task but also has some important advantages over the other tools evaluated in the studies ERAS is freely available at https://githubcom/grosmanjs/era (C) 2020 Elsevier Ltd All rights reserved',NLP
'Previous works in social media processing during crisis management highlight a paradox: citizens are extensively sharing data from the field of the crisis while decision-makers are looking for information about the emerging risks they need to address Several tools already exist to help taking advantage of this new important source of data However few made their way to decision-makers mainly because they remain resource-consuming That is why the question of a tool able to process social media in near-real time to deliver actionable information from the field is still pending Based on a state of the art of the Natural Language Processing tools and systems dedicated to the use of social media data to improve the situational awareness of the decision-makers this paper aims to describe a way to provide them with a first comprehensive system which asset is to completely address the challenge from the collection of the data to their interpretation and understanding and finally offer situational models In this sense the paper focuses on the thorough detail of the business and consequent technical challenges that are raised and a work in progress proposal to address them in a comprehensive manner',NLP
'Text summarization is one of the leading problem of natural language processing and deep learning in recent years Text summarization contains a condensed short note on a large text document Our purpose is to create an efficient and effective abstractive Bengali text summarizer what can generate an understandable and meaningful summary from a given Bengali text document To do this we have collected various texts such as newspaper articles Facebook posts etc and to generate summary from those text we will be using our model Our model works with bi-directional RNNs with LSTM in encoding layer and attention model at decoding layer Our model works as sequence to sequence model to generate summary There are some challenges we have faced while building this model such as text pre-processing vocabulary counting missing words counting word embedding unknown words find out and so on In this model our main goal was to make an abstractive summarizer and reduce the train loss of that During our research experiment we have successfully reduced the train loss to 0008 and able to generate a fluent short summary note from a given text',NLP
'The behavior attributes and properties of a software system is represented in a set of requirements that are written in structured natural language and are usually ambiguous In large development projects different modeling techniques are used to create and manage these requirements which aid in the analysis of the problem domain Requirements are later used in the development process to create test cases which is still mainly a manual process To automate this process we plan to use several of the techniques used in model-driven software development and Natural Language Processing(NLP) The approach under consideration is to use a model-to-model transformation to convert requirements into test cases with the support of Stanford CoreNLP techniques Key to this transformation process is the use of meta-modeling for requirements and test cases In this paper we focus on creating a comprehensive meta-model for requirements that can represent both use cases and user stories and performing preliminary analysis of the requirements using NLP In later work we will develop a set of transformation rules to convert requirements into partial test cases To show the feasibility of our approach we develop a prototype that can accept a cross-section of requirements written as both use cases and user stories',NLP
'Speech sound acoustic properties vary largely across speakers and accents When perceiving speech adult listeners normally disregard non-linguistic variation caused by speaker or accent differences in order to comprehend the linguistic message eg to correctly identify a speech sound or a word Here we tested whether the process of normalizing speaker and accent differences facilitating the recognition of linguistic information is found at the level of neural processing and whether it is modulated by the listeners native language In a multi-deviant oddball paradigm native and nonnative speakers of Dutch were exposed to naturally-produced Dutch vowels varying in speaker sex accent and phoneme identity Unexpectedly the analysis of mismatch negativity (MMN) amplitudes elicited by each type of change shows a large degree of early perceptual sensitivity to non-linguistic cues This finding on perception of naturally-produced stimuli contrasts with previous studies examining the perception of synthetic stimuli wherein adult listeners automatically disregard acoustic cues to speaker identity The present finding bears relevance to speech normalization theories suggesting that at an unattended level of processing listeners are indeed sensitive to changes in fundamental frequency in natural speech tokens (C) 2017 Elsevier Inc All rights reserved',NLP
'With the process of economic globalization and political multi-polarization accelerating it is especially important to predict policy change in the United States While current research has not taken advantage of the rapid advancement in the natural language processing and the relationship between news media and policy change we propose a BERT-based model to predict policy change in the United States using news published by the New York Times Specifically we propose a large-scale news corpus from the New York Times covers the period from 2006 to 2018 Then we use the corpus to fine-tune the pre-trained BERT language model to determine whether the news is on the front page which corresponds to the policy priority We propose a BERT-based Policy Change Index (BPCI) for the United States to predict the policy change in the future short period of time Experimental results in the New York Times Corpus demonstrate the validity of the proposed method',NLP
'Vector space embedding models like word2vec GloVe fastText and ELMo are extremely popular representations in natural language processing (NLP) applications We present Magnitude a fast lightweight tool for utilizing and processing embeddings Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings Magnitude performs common operations up to 60 to 6000 times faster than Gensim Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups',NLP
'In this paper we consider a machine translation (MT) system based on the tree adjoining grammar (TAG) formalism We have successfully carried out sentence level parallelization and its parallel implementations on a multicore machine with varying number of cores and a computing cluster with multicore nodes Since our code is in Java we use MPJ Express for parallel implementations We have carried out experiments with these parallel implementations and their performance is analysed',NLP
'IBM Watson (TM) is a system created to demonstrate DeepQA technology by competing against human champions in a question-answering game designed for people The DeepQA architecture was designed to be massively parallel with an expectation that low latency response times could be achieved by doing parallel computation on many computers This paper describes how a large set of deep natural-language processing programs were integrated into a single application scaled out across thousands of central processing unit cores and optimized to run fast enough to compete in live Jeopardy!(TM) games',NLP
'We report on an experiment to use the natural language processing tools being developed in the SPECIALIST(TM) system to accurately identify terminology associated with the coronary arteries as expressed in coronary catheterization reports The ultimate goal is to map from any anatomically-oriented medical text to online images using the UMLS(R) as an intermediate knowledge source We describe some of the problems encountered when processing coronary artery terminology and report on the results of a formative evaluation of a tool for addressing these problems',NLP
'The linguistic rules of medical terminology assist in gaining acquaintance with rare/complex clinical and biomedical terms The medical language follows a Greek and Latin-inspired nomenclature This nomenclature aids the stakeholders in simplifying the medical terms and gaining semantic familiarity However natural language processing models misrepresent rare and complex biomedical words In this study we present MedTCS-a lightweight post-processing module-to simplify hybridized or compound terms into regular words using medical nomenclature MedTCS enabled the word-based embedding models to achieve 100% coverage and enabled the BiowordVec model to achieve high correlation scores (0641 and 0603 in UMNSRS similarity and relatedness datasets respectively) that significantly surpass the n-gram and sub-word approaches of FastText and BERT In the downstream task of named entity recognition (NER) MedTCS enabled the latest clinical embedding model of FastText-OA-All-300d to improve the F1-score from 045 to 080 on the BC5CDR corpus and from 059 to 081 on the NCBI-Disease corpus respectively Similarly in the drug indication classification task our model was able to increase the coverage by 9% and the F1-score by 1% Our results indicate that incorporating a medical terminology-based module provides distinctive contextual clues to enhance vocabulary as a post-processing step on pre-trained embeddings We demonstrate that the proposed module enables the word embedding models to generate vectors of out-of-vocabulary words effectively We expect that our study can be a stepping stone for the use of biomedical knowledge-driven resources in NLP',NLP
'Archaeological records are a central part of this discipline research It is essential that the records include the relationship between its parts to allow better understanding among man and machine Here we present the challenges of applying NLP and Information Extraction to Archaeological data with a case study: Megalithism',NLP
'Spelling errors when writing Quranic verses require special handling in terms of detection and correction This paper aims to demonstrate the significance of the character level errors correction approach for written Quranic text It concentrates mainly on discussing the Algorithm and the implementation of the system',NLP
'Presents the semantic portal project of the center for teaching and research in the Social Sciences and Contemporary History (CPDOC) of the Fundacao Getulio Vargas Rio de Janeiro This project involves the use of semantic and visualization technologies and natural language processing techniques to allow enhanced ways to access the CPDOC collections',NLP
'The paper presents a framework for handling uncertainty in data fusion systems based on possibility theory Information on units and observations is represented by a possibility distribution The correlation problem is addressed and the fusion process is described Lastly we present a linguistic interface to translate possibility distributions into a natural language form which can be understood by the operator',NLP
'Developing performant mobile expert systems consists an emerging problem taking into account the wide use of mobile phones and the need to access from such devices decision-assistance tools This paper proposes a Hybrid Intelligent Agent based Expert System for Android which uses a positioning system to enhance its performance',NLP
'An extraction of significant information from Internet sources is an important task of pharmacovigilance due to the need for post-clinical drugs monitoring This research considers the task of end-to-end recognition of pharmaceutically significant named entities and their relations in texts in natural language The meaning of end-to-end  is that both of the tasks are performed within a single process on the raw  text without annotation The study is based on the current version of the Russian Drug Review Corpus-a dataset of 3800 review texts from the Russian segment of the Internet Currently this is the only corpus in the Russian language appropriate for research of the mentioned type We estimated the accuracy of the recognition of the pharmaceutically significant entities and their relations in two approaches based on neural-network language models The first core approach is to sequentially solve tasks of named-entities recognition and relation extraction (the sequential approach) The second one solves both tasks simultaneously with a single neural network (the joint approach) The study includes a comparison of both approaches along with the hyperparameters selection to maximize resulting accuracy It is shown that both approaches solve the target task at the same level of accuracy: 52-53% macro-averaged F1-score which is the current level of accuracy for end-to-end  tasks on the Russian language Additionally the paper presents the results for English open datasets ADE and DDI based on the joint approach and hyperparameter selection for the modern domain-specific language models The result is that the achieved accuracies of 842% (ADE) and 733% (DDI) are comparable or better than other published results for the datasets',NLP
'Objective: To assess clinical documentation variations across health care institutions using different electronic medical record systems and investigate how they affect natural language processing (NLP) system portability Materials and Methods: Birth cohorts from Mayo Clinic and Sanford Childrens Hospital (SCH) were used in this study (n = 298 for each) Documentation variations regarding asthma between the 2 cohorts were examined in various aspects: (1) overall corpus at the word level (ie lexical variation) (2) topics and asthma-related concepts (ie semantic variation) and (3) clinical note types (ie process variation) We compared those statistics and explored NLP system portability for asthma ascertainment in 2 stages: prototype and refinement Results: There exist notable lexical variations (word-level similarity = 0669) and process variations (differences in major note types containing asthma-related concepts) However semantic-level corpora were relatively homogeneous (topic similarity = 0944 asthma-related concept similarity = 0971) The NLP system for asthma ascertainment had an F-score of 0937 at Mayo and produced 0813 (prototype) and 0908 (refinement) when applied at SCH Discussion: The criteria for asthma ascertainment are largely dependent on asthma-related concepts Therefore we believe that semantic similarity is important to estimate NLP system portability As the Mayo Clinic and SCH corpora were relatively homogeneous at a semantic level the NLP system developed at Mayo Clinic was imported to SCH successfully with proper adjustments to deal with the intrinsic corpus heterogeneity',NLP
'Scientific research teams have immense valuable knowledge that need to be managed Organizing scientific contributions of team members constitutes a major challenge for the monitoring of knowledge evolution team members competences discovery and facilitating information retrieval processes However performing manual annotations is often time consuming and labor-intensive task especially in case of complex annotation schemas Currently existing knowledge management systems focus on ensuring the scientific knowledge creation sharing organization and evaluation but dont provide a way for helping researchers in the classification task In this paper we introduce a knowledge management system that offers an annotation service for researchers contributions by including some natural language processing techniques The provided service process comprises four phases: (1) the semantic enrichment of domain ontology based on the extraction of background data from Babelnet knowledge base (2) the automatic generation of candidate categories using the enriched domain ontology (3) the forwarding of the pre-annotated papers to our web-based system to interact with researchers and finally (4) the human revision of the generated annotations Evaluation results show its advantage not only in reducing human effort and time consumption during the annotation task but also in improving annotations quality',NLP
'Companies aiming at online data sources for extracting business insights often experience the issue of managing different collections of datasets It is because online business-related data is growing over time This increasing data demands more storage But saving the irrelevant data will cost more to companies To overcome the problems of data integration and scalability the idea of Virtual Knowledge Graphs (VKGs) is exploited Moreover text processing methods based on Natural Language Processing (NLP) are utilized for improving the identification of relevant business news data The contribution of this work is twofold that is presented using a business case study First we have applied the concept of VKG to expand the company database The expansion is required for adding more business-related data from the news articles to improve the process of identifying its relevance as per the end-users interests Second we have proposed the text relevance resolution matrix and its working is shown through NLP techniques for identifying relevant news data Populating the modeled VKG with relevant news data will save database space and prevent manual labor Moreover it will also help business analysts to understand business data temporally spatially and thematically',NLP
'In the recent past e-commerce sites have made rapid growth There are thousands of products and various websites sell these products Massive growth in the number of reviews and their availability along with the advent of opinion-rich review forums for the products sold online choosing the right one from a large number of products has become difficult for the users HELP-ME- BUY APP is an android application that assists buyers in online shopping It is imminent for buyers to verify for genuineness and quality of products What better way is there than to ask people who have already bought the product? This is when customer reviews come into picture The major hitch here is popular products have thousands of reviews-we do not have the time or patience to read all thousands of them Hence our application eases this task by analyzing and summarizing all reviews which will help the user decide what other buyers have experienced on buying this product We carry out this process by a number of modules that include feature extraction and opinion extraction which improves the process of analysis and helps in the formation of an efficient summary',NLP
'Data modelling is a complex process that requires knowledge and experience of designers where the quality of the data model significantly affects the quality of further phases in the development of the information system The paper presents an architecture proposal for knowledge-based (KB) system that serves as a support for creating data models based on verbalisation The system will use verbalized expert knowledge which will be formalized using the methods of the theory of formal languages Through the methodology for developing system architecture system parts are described and Purpose Context Inputs Activities Outputs Effects for KB system are given through a variant of a logic model Also the functionalities which the verbalisation-based KB system for data modelling should have are identified along with all the potential beneficiaries The paper presents the application of the theory of formal languages for translating business descriptions expressed in natural language into a data model expressed using the entity-relationship (ER) method Example of the translation process is also given',NLP
'Named Entity Recognition (NER) is an essential task in Natural Language Processing (NLP) By using NER it is possible to create associations in a text to recognize real-world entities The data indexing process is also considered a vital resource as it makes it easier to find texts in a set of documents When we analyze a search engine we aim at the ease of the users search process Indexing recognized entities could help the search engine find data with a high semantic index therefore more accurate This study aims to investigate the automatic transformation of annotated entities as indexes for a search engine The recognition of entities used the hybrid model CRF+LG Search engines usually work with keyword localization (tokens) However this research aimed to use a semantic search as it improves the quality of the results by understanding the users intention using enricher meta factors besides the keyword We performed ten experiments using P@{5 10 and 20} and the search engine with a high semantic index achieved accuracy of 100% correctly returning all results The search engine without NER was confused when producing results for person and organization categories mainly',NLP
'This review of medical imaging informaties is a survey of current developments in an exciting field The focus is on informatics issues rather than traditional data processing and information systems such as picture archiving and communications systems (PACS) and image processing and analysis systems In this review we address imaging informatics issues within the requirements of an informatics system defined by the American Medical Informatics Association With these requirements as a framework we review in four sections: (1) Methods to present imaging and associated data without causing an overload including image study summarization content-based medical image retrieval and natural language processing of text data (2) Data modeling techniques to represent clinical data with focus on an image data model including general-purpose time-based multimedia data models healthcare-specific data models knowledge models and problem-centric data models (3) Methods to integrate medical data information from heterogeneous clinical data sources Advances in centralized databases and mediated architectures are reviewed along with a discussion on our efforts at data integration based on peer-to-peer networking and shared file systems (4) Visualization schemas to present imaging and clinical data: the large volume of medical data presents a daunting challenge for an efficient visualization paradigm In this section we review current multimedia visualization methods including temporal modeling problem-specific data organization including our problem-centric context and user-specific visualization interface',NLP
'Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks However the unsupervised sub-word tokenization methods commonly used in these models (eg byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages Even given a morphological analyzer naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities We address these challenges by proposing a simple yet effective twotier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality Despite the success of BERT most of its evaluations have been conducted on high-resource languages obscuring its applicability on low-resource languages We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language naming the proposed model architecture KinyaBERT A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 43% in average score of a machine-translated GLUE benchmark KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise(1)',NLP
'Automatic speech recognition is a critical component of human language technologies It concerns the translation of speech into textual data which can be processed by computers Thus it offers the creation of an intimate link allowing humans to interact with machines on a completely natural level A variety of open-source toolkits exist for the development of these systems These toolkits have been successfully implemented and tested for use on well-resourced languages However the same level of testing has not been performed for South African languages This investigation sets out to evaluate popular open-source tools for South African languages and identify optimal toolkit configurations for each language and toolkit The NCHLT corpora were used to set up automatic speech recognition systems for English and isiXhosa using Kaldi CMU Sphinx and HTK The word error rates achieved during this investigation showed that the best configurations from this investigation achieved better performance than those which were reported by the developers of the NCHLT corpus',NLP
'Action verbs have many meanings covering actions in different ontological types Moreover each language categorizes action in its own way One verb can refer to many different actions and one action can be identified by more than one verb The range of variations within and across languages is largely unknown causing trouble for natural language processing tasks IMAGACT is a corpus-based ontology of action concepts derived from English and Italian spontaneous speech corpora which makes use of the universal language of images to identify the different action types extended by verbs referring to action in English Italian Chinese and Spanish This paper presents the infrastructure and the various linguistic information the user can derive from it IMAGACT makes explicit the variation of meaning of action verbs within one language and allows comparisons of verb variations within and across languages Because the action concepts are represented with videos extension into new languages beyond those presently implemented in IMAGACT is done using competence-based judgments by mother-tongue informants without intense lexicographic work involving underdetermined semantic description',NLP
'Domain engineering can be seen as the process of identifying a domain API providing its semantics and structuring it The domain API is a natural framework for software product line requirements and development Magnolia is an integrated programming and algebraic specification language As such it gives a strong focus on API design The Magnolia way to domain engineering can be scaled from a lightweight to a formalistic heavyweight approach Defined APIs can easily be extended as more of the domain is investigated This paper summarises the Magnolia domain engineering process',NLP
'This paper presents a computational model for auxiliary verb orderings in Thai It aims at the pre-processing step of an efficient Thai parser by preliminarily percolating allowable scrambling patterns Orderings schemes achieved by corpus observation are modelled by finite-state transducer in which auxiliary verbs are categorised into groups This results in an improvement of speed efficiency of the parser as it tremendously reduces backtracking step in derivations',NLP
'This research is directed towards automating the Web Site summarization task To achieve this objective an approach which applies machine learning and natural language processing techniques is employed The automatically generated summaries are compared to manually constructed summaries from DMOZ Open Directory Project The comparison is performed via a formal evaluation process involving human subjects Statistical evaluation of the results demonstrates that the automatically generated summaries axe as informative as human authored DMOZ summaries and significantly more informative than home page browsing or time limited site browsing',NLP
'In the domain of high-energy physics (HEP) general-purpose query languages have found little adoption in analysis This is surprising regarding SQL-based systems as HEP data analysis matches SQLs processing model well: the data is fully structured and makes use of predominantly standard operators To better understand the situation we select six general-purpose query engines from both the SQL and NoSQL domain and analyze their performance scalability and usability in HEP analysis employing standard HEP tools as baseline We also identify a set of core language features needed to support HEP data analysis Our results reveal an interesting and complex picture: several query languages provide a rich and natural query development experience while others fall short In terms of performance our results reveal that many of the database systems are one or two orders of magnitude slower than the standard HEP analysis tools while others manage to scale and perform well These conclusions suggest that while the existing data processing systems are viable candidates for HEP analysis there is still work to be done to improve their performance and ability to succinctly express HEP queries',NLP
'The language used in social media is often characterized by the abundance of informal and non-standard writing The normalization of this non-standard language can be crucial to facilitate the subsequent textual processing and to consequently help boost the performance of natural language processing tools applied to social media text In this paper we present a benchmark for lexical normalization of social media posts specifically for tweets in Spanish language We describe the tweet normalization challenge we organized recently analyze the performance achieved by the different systems submitted to the challenge and delve into the characteristics of systems to identify the features that were useful The organization of this challenge has led to the production of a benchmark for lexical normalization of social media including an evaluation framework as well as an annotated corpus of Spanish tweets-TweetNorm_es- which we make publicly available The creation of this benchmark and the evaluation has brought to light the types of words that submitted systems did best with and posits the main shortcomings to be addressed in future work',NLP
'The increase in Alzheimers disease is due to the aging of the population and is the first cause of neurodegenerative disorders Progressive development of cognitive emotional and behavior troubles leads to the loss of autonomy and to dependency of people which corresponds to the dementia phase Language disorders are among the first clinical cognitive signs of the disease Our objective is to study verbal communication of people affected by the Alzheimers disease at early to moderate stages One particularity of our approach is that we work in ecological conversation situation: people are faced to persons they know We study verbal productions of five people affected by the Alzheimers disease and of five control people The conversations are transcribed and processed with the NLP methods and tools Over thirty features grouped in four categories are studied Our results indicate that the Alzheimers patients present lexical and semantic deficit and that in several ways their conversation is notably poorer than the conversation of the control people',NLP
'Language processing has a large practical potential in intelligent interfaces if we take into account multiple modalities of communication Multimodality refers to the perception of different coordinated media used in delivering a message as well as the combination of various attitudes in relation to communication In particular the integration of natural language processing and hypermedia allows each modality to overcome the constraints of the other resulting in a novel class of integrated environments for complex exploration and information access Information presentation is a key element of such environments; generation techniques can contribute to their quality by producing texts ex novo or flexibly adapting existing material to the current situation A great opportunity arises for intelligent interfaces and language technology of this kind to play an important role for individual-oriented cultural tourism In the article reference is made to some prototypes developed at IRST that were conceived for this specific area A recent project concentrated on the combination of two forms of navigation taking place at the same time-one in information space the other in physical space Collaboration an important topic for intelligent interfaces is also discussed',NLP
'Measuring the semantic similarity of short texts is a noteworthy problem since short texts are widely used on the Internet in the form of product descriptions or captions image and webpage tags news headlines etc This paper describes a methodology which can be used to create a software system capable of determining the semantic similarity of two given short texts The proposed LInSTSS approach is particularly suitable for application in situations when no large publicly available electronic linguistic resources can be found for the desired language We describe the basic working principles of the system architecture we propose as well as the stages of its construction and use Also we explain the procedure used to generate a paraphrase corpus which is then utilized in the evaluation process Finally we analyze the evaluation results obtained from a system created for the Serbian language and we discuss possible improvements which would increase system accuracy (C) 2013 Elsevier BV All rights reserved',NLP
'One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs) MWEs pose a huge problem to a precise language processing due to their idiosyncratic nature and diversity in lexical syntactical and semantic properties The semantic of a MWE can be expressed transparently or opaquely after combining the semantic of its constituents This paper deals with the identification of Nominal Multiword Expressions in the Bengali text using Conditional Random Field (CRF) machine learning technique Bengali is highly agglutinative and morphologically rich language Thus the selection of features such as surrounding words POS tag prefix suffix length etc are proved to be very effective for running the CRF tool for the identification of Nominal MWEs Compared to the statistical system built in Bengali language for compound noun MWEs identification our proposed system shows higher accuracy in terms of precision recall and F-score We also conclude that with the identification of Reduplicated MWEs (RMWEs) and considering it as a feature makes reasonable improvement compared to the earlier system',NLP
'Natural Language Processing continues to grow in popularity in a range of research and commercial applications yet managing the wide array of potential NLP components remains a difficult problem This paper describes CURATOR an NLP management framework designed to address some common problems and inefficiencies associated with building NLP process pipelines; and EDISON an NLP data structure library in Java that provides streamlined interactions with CURATOR and offers a range of useful supporting functionality',NLP
'Metamorphic testing involves reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation a strategy particularly fitting to testing scientific software However such software typically has large input-output spaces and the fundamental prerequisite extracting variables of interest is an arduous and non-scalable process when performed manually To this end we devise a workflow around an autoregressive transformerbased Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software Our end-toend approach besides a prompt specification consisting of fewshot examples by a human user is fully automated in contrast to current practice requiring human intervention We showcase our LLM workflow over a real case and compare variables extracted to ground truth manually labelled by experts Our preliminary results show that our LLM-based workflow achieves an accuracy of 087 while successfully deriving 618% of variables as partial matches and 347% as exact matches',NLP
'Sentence representation is one of the foundational tasks in natural language processing and long short term memory (LSTM) is a widely used tool to deal with the variable length sentence In this paper a new LSTM-based sentence representation model is proposed for sentence classification task By introducing a self-supervised method in the process of learning the hidden representation of the sentence the proposed model automatically capture the syntactic and semantic information from the context and used as additional language information to learn better contextual hidden representation Moreover instead of using the final hidden representation of LSTM or the max (or average) pooling of the hidden representations over all the time step we propose to generate the global representation of the sentence by combining all contextual hidden representations in an element-wise attention manner We evaluate our model on three sentence classification tasks: sentiment classification question type classification and subjectivity classification Experimental results show that the proposed model improves the accuracy of sentence classification compared to other sentence representation methods in all of the three tasks',NLP
'The ability to express feelings and needs to share ideas or to establish relations is fundamental in different stages of human life During language acquisition a child develops several intellectual psychological and emotional skills that support many other brain functions On those grounds speech-language therapy is a fundamental process for those people who suffer from communication disorders However currently in several developing countries there is a lack of rehabilitation services and personnel of many centers overwork In this paper we present an expert system able to design therapy plans for patients suffering from disabilities related with communication The system uses an approach based on natural language processing and multi-label classification to determine which strategies must be carried out in a therapy session This approach has been tested on a database consisting on 1345 strategies and 53 real cases of children with disabilities In order to evaluate the generated plans we have measured accuracy and quality obtaining encouraging results',NLP
'Open Source Intelligence (OSINT) is an intelligence gathering discipline that involves collecting information from open sources and analyzing it to produce usable intelligence The international Intelligence Communities have seen open sources grow increasingly easier and cheaper to acquire in recent years But up to 80% of electronic data is textual and most valuable information is often hidden and encoded in pages which are neither structured nor classified The process of accessing all these raw data heterogeneous in terms of source and language and transforming them into information is therefore strongly linked to automatic textual analysis and synthesis which are greatly related to the ability to master the problems of multilinguality This paper describes a content enabling system that provides deep semantic search and information access to large quantities of distributed multimedia data for both experts and general public STALKER provides with a language independent search and dynamic classification features for a broad range of data collected from several sources in a number of culturally diverse languages',NLP
'The understanding of information in a text description can be improved by visually accompanying it with images or videos This opportunity is particularly relevant for books and other traditional instructional material Videos or more in general (interactive) graphics contents can help to increase the effectiveness of this material by providing eg an animated representation of the steps to be performed to carry out a given procedure The generation of 3D animated contents however is still very labor-intensive and time-consuming Systems able to speed up this process offering flexible and easy-to-use interfaces are becoming of paramount importance Hence this paper describes a system designed to automatically generate a computer graphics video by processing a text description and a set of associated images The system combines Natural Language Processing and image analysis for extracting information needed to visually represent the procedure depicted in an instruction manual using 3D animations It relies on a database of 3D models and preconfigured animations that are activated according to the information extracted from the said input Moreover by analyzing the images the system can also generate new animations from scratch Promising results have been obtained assessing the system performance in a specific use case focused on printers maintenance',NLP
'This paper presents an architecture for Big Data Analytics regarding unstructured content The architecture is proposed as an industrial solution in a real setting In particular the paper focuses on BD (Big Data) for Smart Companies and on Enterprise Content Management for extraction of information from textual BD It presents the process architecture and discusses some experiments done as part of a larger BD Analytics project',NLP
'Requirement engineering is a fundamental step in the production of high quality software Many attempts have been conducted to automate some aspects of the requirements engineering process In this paper we present a framework that provides the requirements engineers with an environment which accepts English natural language requirements as input and automatically generates the corresponding UML class diagram designs Moreover the framework can highlight the possibility of specification reusability through a reverse engineering process which saves the requirements engineers both time and efforts',NLP
'Image captioning is a popular topic in the domains of computer vision and natural language processing (NLP) Recent advancements in deep learning (DL) models have enabled the improvement of the overall performance of the image captioning approach This study develops a metaheuristic optimization with a deep learning-enabled automated image captioning technique (MODLE-AICT) The proposed MODLE-AICT model focuses on the generation of effective captions to the input images by using two processes involving encoding unit and decoding unit Initially at the encoding part the salp swarm algorithm (SSA) with a HybridNet model is utilized to generate effectual input image representation using fixed-length vectors showing the novelty of the work Moreover the decoding part includes a bidirectional gated recurrent unit (BiGRU) model used to generate descriptive sentences The inclusion of an SSA-based hyperparameter optimizer helps in attaining effectual performance For inspecting the enhanced performance of the MODLE-AICT model a series of simulations were carried out and the results are examined under several aspects The experimental values suggested the betterment of the MODLE-AICT model over recent approaches',NLP
'Software is everywhere and the productivity of Software Engineers has increased radically with the advent of new specifications design and programming paradigms and languages The main objective of the DECODER project is to introduce radical solutions to increase productivity by increasing the abstraction level at specification stage using requirements engineering techniques to integrate more complete specifications into the development process and formal methods to reduce the time and efforts for integration testing DECODER project will develop a methodology and tools to improve the productivity of the software development process for medium-criticality applications in the domains of IoT Cloud Computing and Operating Systems by combining Natural Language Processing techniques modelling techniques and Formal Methods A radical improvement is expected from the management and transformation of informal data into material (herein called knowledge) that can be assimilated by any party involved in a development process The project expects an average benefit of 20% in terms of efforts on several use cases belonging to the beforehand mentioned domains and will provide recommendations on how to generalize the approach to other medium-critical domains',NLP
'In this paper we present QuASIt a Question Answering System for the Italian language and the underlying cognitive architecture The term cognitive is meant in the procedural semantics perspective which states that the interpretation and/or production of a sentence requires the execution of some cognitive processes over both a perceptually grounded model of the world and a linguistic knowledge acquired previously We attempted to model these cognitive processes with the aim to make an artificial agent able both to understand and produce natural language sentences The agent runs these processes on its inner domain representation using the linguistic knowledge also In this sense QuASIt is both a rule-based and ontology-based question answering system In the model rules are aimed at understanding the query in terms of the linguistic typology of the question and enabling its semantic processing as regards the search for the answer in the structured knowledge from DBPedia Italian project Also the free explicative text in support of the query is analyzed if available QuASIt attempts to answer for both multiple choice and essay questions The model is presented the implementation of the system is detailed and some experiments are discussed',NLP
'As a robust similarity measurement Earth Movers Distance (EMD) has been widely adopted in many real-world applications such as machine learning computer vision and natural language processing In this paper we study the problem of EMD-based similarity search which aims at finding all histogram objects from a dataset whose EMD is within a pre-defined threshold from the given query Since the time complexity of computing EMD is rather high it is essential to devise effective techniques to accelerate the query processing To this end we propose a filter-and-verification framework: In the filter step we devise three effective strategies to prune dissimilar objects in batch by sharing the computation between multiple objects In the verification step we develop novel flow adjustment techniques to incrementally calculate the EMD of candidates and enable early termination We justify our proposed framework by conducting both theoretical analysis and extensive experiments The results on four real world datasets show that our proposed techniques achieve up to an order of magnitude performance gain than state-of-the-art approaches',NLP
'Sentence semantic matching (SSM) is a fundamental research in solving natural language processing tasks such as question answering and machine translation The latest SSM research benefits from deep learning techniques by incorporating attention mechanism to semantically match given sentences However how to fully capture the semantic context without losing significant features for sentence encoding is still a challenge To address this challenge we propose a deep feature fusion model and integrate it into the most popular deep learning architecture for sentence matching task The integrated architecture mainly consists of embedding layer deep feature fusion layer matching layer and prediction layer In addition we also compare the commonly used loss function and propose a novel hybrid loss function integrating MSE and cross entropy together considering confidence interval and threshold setting to preserve the indistinguishable instances in training process To evaluate our model performance we experiment on two real world public data sets: LCQMC and Quora The experiment results demonstrate that our model outperforms the most existing advanced deep learning models for sentence matching benefited from our enhanced loss function and deep feature fusion model for capturing semantic context',NLP
'This paper describes a customized scaffolding approach to improve learners conceptual understandings through knowledge visualization We propose a framework to use natural language processing and graph based algorithms to automatic visualize individual learners prior knowledge states domain knowledge new encountered concepts and to reveal the semantic relationships between them Thus we are able to help learners to solve their uncertainties of new merging ideas and concepts in their learning process in order to integrate new knowledge with their preconceptions',NLP
'Structural analysis of compound words is necessary and an important process in natural language processing Proposed here is a corpus- and statistics- based method for the structural analysis of compound words in Japanese We determine the structure of a compound word by using Internet corpus and calculating the strength of word association among its constituent words Experiments with 5 6 7 and 8 kanji compound words show that our method works well and its performance is better than those of other comparable studies',NLP
'Over the last few years the exponential rise of social media particularly Twitter is becoming a major source of news sharing and consumption among users These platforms allow users to publish author and distribute content These environments may be used to report and spread gossip and false news whether accidentally or maliciously Fake news and inaccurate machine-generated text are serious issues affecting societies worldwide particularly the Arab world This motivates efforts to identify fake and distorted news This paper aims to introduce a robust prediction model to identify fake news in Arabic Tweets Several Natural Language Processing (NLP) feature selection and advanced ML algorithms were exploited to achieve this purpose NLP techniques were used to process and transform the given tweets into structured form The recursive feature elimination (RFE) technique was employed to eliminate uninformative features ML methods were used to build the prediction model Experimental results revealed the superiority of the Logistic Regression (LR) classifier among other algorithms Moreover RFE proved its ability to enhance the overall performance of the LR classifier Overall the proposed model provided an acceptable prediction accuracy of 82%',NLP
'Attention is the fundamental element of effective learning memory and interaction Learning however with the evolvement of technologies in the modern digital age has surpassed traditional learning systems to more convenient online or e-learning systems Nevertheless unlike in the traditional learning systems attention detection of a student in an e-learning environment remains one of the barely explored areas in Human Computer Interaction This study proposes a multimodal ensemble solution to detect the level of attentiveness of a student in an e-learning environment with the use of computer vision natural language processing and deep learning to overcome the barriers in identifying user attention in e-learning The proposed multimodal captures processes and predicts user attentiveness levels of individual students which are subsequently aggregated through an ensemble model to derive an overall outcome of better accuracy than individual model outcomes The final outcome of the ensemble model produces a range of percentages within which the attentiveness level of the student lies during a single online lesson This range is consequently delivered to the users through an Application Programming Interface',NLP
'Constructed emergency response scenarios provide a basis for decision makers to make management decisions and the development of such scenarios considers earlier historical cases Over the decades the development of emergency response scenarios has mainly implemented the elements of historic cases to describe the grade and influence of an accident This paper focuses on scenario construction and proposes a corresponding framework based on natural language processing (NLP) using text reports of marine oil spill accidents For each accident the original textual reports are first divided into sentence sets corresponding to the temporal evolution Each sentence set is regarded as a textual description of a marine oil spill scenario A method is proposed in this paper based on parsing named entity recognition (NER) and open information extraction (OpenIE) to process the relation triples that are extracted from the sentence sets Finally the relation triples are semantically clustered into different marine oil spill domains to construct scenarios The research results are validated and indicate that the proposed scenario construction framework can be effectively used in practical applications',NLP
'Natural-language processing is well positioned to help stakeholders study the dynamics of ambiguous Climate Change-related (CC) information Recently deep neural networks have achieved good results on a variety of NLP tasks depending on high-quality training data and complex and exquisite frameworks This raises two dilemmas: (1) the networks are highly reliant on powerful hardware devices and processing is time-consuming which is not only inconducive to execution on edge devices but also leads to resource consumption (2) Obtaining large-scale effective annotated data is difficult and laborious especially when it comes to a special domain such as CC In this paper we propose a CC-domain-adapted BERT distillation and reinforcement ensemble (DARE) model for tackling the problems above Specifically we propose a novel data-augmentation strategy which is a Generator-Reinforced Selector collaboration network for countering the dilemma of CC-related data scarcity Extensive experimental results demonstrate that our proposed method outperforms baselines with a maximum of 2683% on SoTA and 5065x inference time speed-up Furthermore as a remedy for the lack of CC-related analysis in the NLP community we also provide some interpretable conclusions for this global concern',NLP
'Big Data is becoming a prominent trend in our society Ever larger amounts of data including sensitive and personal information are being loaded into NoSQL and other Big Data technologies for analysis and processing However current security approaches do not take into account the special characteristics of these technologies leaving sensitive and personal data unprotected thereby risking severe monetary losses and brand damage In this paper we focus on assuring document databases proposing a framework that considers three stages: (1) The source data set is analysed by using Natural Language Processing techniques and ontological resources in order to detect sensitive data (2) We define a metamodel for document databases that allows designers to specify both structural and security aspects (3) This model is automatically implemented into a specific document database tool MongoDB Finally we apply the proposed framework to a case study with a data set from the medical domain The great advantages of our framework are that: (1) the effort required to secure the data is reduced as part of the process is automated (2) it can be easily applied to other NoSQL technologies by adapting the metamodel and transformations and (3) it is aligned with existing standards thus facilitating the application of recommendations and best practices',NLP
'Because huge amount of scientific papers have been published at an accelerating rate it is beneficial to do intelligent paper classification especially fine-grained classification However existing natural language processing techniques are mostly coarse-grained Some characteristics of fine-grained scientific paper classification needs special attention One is that the number of data may well be quite limited Number of papers in the lower level sub-fields inevitably becomes less Meanwhile emerging sub-fields with new discoveries will have few papers nevertheless these sub-fields can be important Furthermore fine-grained labeling of scientific papers requires high expertise and is time consuming Another aspect of scientific papers is that they contain multi-modal information To address the above two issues we propose a multi-modal hierarchical fusion network (MHFNet) for fine-grained paper classification We treat paper abstract features image features and paper title features as three modalities The MobileNetV2 model and the ALBERT model are combined in the proposed model to encode multi-modal information Comparison results with baseline methods on both sufficiently large datasets and number-limited datasets show improvements even more on number-limited datasets',NLP
'This paper conducts a sentiment analysis of Twitters posts between late October 2020 and late April 2021 regarding COVID-19 vaccination campaign in Mexico through several machine learning models such as Logistic Regression Neuronal Network Naive Bayes and Support Vector Machine To prepare data Natural Language Processing techniques were used such as tokenization stemming n-grams and stop-words The best performance was achieved by Logistic Regression with an accuracy score of 8342% while classifying tweets according to a positive or negative sense This work suggests that sentiment analysis with Twitter information allows to witness a relevant part of the public discussion around specific topics For this study the tweets analyzed showed a similar behavior to other search and reference electronic tools such as Google Trends regarding conversation around COVID In addition the present analysis allows the classification and tendency of public opinion Furthermore this study shows that measuring peoples opinion through machine learning and natural language processing techniques can generate significant benefits for institutions and businesses given that obtaining information on Twitter is less expensive and can be processed and analyzed faster than other opinion analysis techniques such as surveys or focus groups',NLP
'GloVe representations of words as vector embeddings in continuous spaces are learned from matrix factorization of the words co-occurrences matrix constructed from large corpora Due to their high quality as textual features GloVe embeddings have been extensively utilized for many text mining and natural language processing tasks with considerable success Further improvements of these word representations can be obtained by also taking into account the valuable information of the semantic properties of the words and the complex relationships between them as provided by semantic lexicons In this paper we adopt optimization techniques from the domain of machine learning with constrained optimization in order to leverage the relational knowledge between words and we propose an efficient algorithm that produces word embeddings enhanced by the semantic information The proposed algorithm outperforms other related approaches that utilize semantic information either during training or as a post-processing step Our claims are validated by experiments on popular text mining and natural language processing tasks including word similarities word analogies and sentiment analysis which demonstrate that our proposed model can significantly improve the quality of word vector representations (C) 2020 Elsevier BV All rights reserved',NLP
'Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society-forecasting weather recognizing faces detecting fraud and deciphering genomics AIs future role in medical practice remains an unanswered question Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms) Correcting algorithm mistakes (training) adds to AI predictive model confidence AI is being successfully applied for image analysis in radiology pathology and dermatology with diagnostic speed exceeding and accuracy paralleling medical experts While diagnostic confidence never reaches 100% combining machines plus physicians reliably enhances system performance Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records In this and other ways AI may optimize the care trajectory of chronic disease patients suggest precision therapies for complex illnesses reduce medical errors and improve subject enrollment into clinical trials (c) 2018 Elsevier Inc All rights reserved',NLP
'People tend to pay bribes to get their work done or get rid of any issue People are unaware of the existing laws because of which they are defrauded The illiteracy about laws has become advantageous to the deceiver Hence proper education should be given to the people regarding the laws of the Indian Constitution The mobile application serves the purpose of providing valuable information about laws based on the question posted by the user An information retrieval system is designed to retrieve relevant answers about laws The keywords from laws of Indian Constitution are indexed and used to build a store of indexed keywords The user query is processed using natural language processing Referring with the indexed keywords multiple laws are identified The laws are ranked in line with the reward gained The system is further modeled to master based on the positive and negative feedback from the user The reward is revised in accordance with the feedback Thus the system learns from the user too Considering everything the system is designed to deliver possible solution based on the user feedback Overall the system solves the problem of lack of awareness about laws of the Indian constitution',NLP
'In the pharmaceutical industry efficiently mining pharmacological data from the rapidly increasing scientific literature is very crucial for many aspects of the drug discovery process such as target validation tool compound selection etc A quick and reliable way is needed to collect literature assertions of selected compounds biological and pharmacological effects in order to assist the hypothesis generation and decision-making of drug developers INFUSIS the text mining system presented here extracts data on chemical compounds from PubMed abstracts It involves an extensive use of customized natural language processing besides a co-occurrence analysis As a proof-of-concept study INFUSIS was used to search in abstract texts for several obesity/diabetes related pharmacological effects of the compounds included in a compound dictionary The system extracts assertions regarding the pharmacological effects of each given compound and scores them by the relevance For each selected pharmacological effect the highest scoring assertions in 100 abstracts were manually evaluated ie 800 abstracts in total The overall accuracy for the inferred assertions was over 90 percent',NLP
'This paper presents an overview of a research project aiming to create a new product design methodology for small and medium sized enterprises This methodology combines structured innovation approach of TRIZ with the modern market analysis techniques in order to help design innovative and competitive products TRIZ which is an abbreviation of Theory of Solving Inventive Problems is a methodology of coping with engineering problems formulated by Genrich Altshuller almost fifty years ago and developed since It is a proven engineering methodology used in order to solve production or technical problems very effectively Coupled with modern market analysis tools it could be - as the project demonstrates - also a very effective method of devising improvements in existing products - or even in coming up with new product lines - that respond to market needs Along with the methodology a knowledge management tool is being created that will support methodology users with knowledge gathering and organization The software tool will utilize natural language processing technology in order to help statistically analyse the documentation repositories describing technology state of the art - such as patent databases It will also support analysts by guiding them in a systematic way through current product - and its production process - description and analysis',NLP
'Generating natural language descriptions of visual content is an intriguing task which has wide applications such as assisting blind people The recent advances in image captioning stimulate further study of this task in more depth including generating natural descriptions for videos Most works of video description generation focus on visual information in the video However audio provides rich information for describing video contents as well In this paper we propose to generate video descriptions in natural sentences via multimodal processing which refers to using both audio and visual cues via unified deep neural networks with both convolutional and recurrent structure Experimental results on the Microsoft Research Video Description (MSVD) corpus prove that fusing audio information greatly improves the video description performance We also investigate the impact of image amount vs caption amount on the image caption performance and see the trend that when limited amount of training is available number of various captions is more important than number of various images This will guide us to investigate in the future how to improve the video description system via increasing amount of training data',NLP
'Prepositional phrase attachment is a common source of ambiguity in natural language processing We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus we use an iterative process to extract training data from an automatically parsed corpus Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words',NLP
'in this paper I present an environment and algorithm for lazy (incremental) construction of multigram profile models as part of IR (information retrieval) training and exploatation processes N-grams are traditionally used for natural language text models but they can be also successfully used for domain independent document classification I am demonstrating results of a prototype utility which proves these ideas',NLP
'We describe a Web-services based conversational agent architecture that combines logical inferences from the WordNet knowledge base and Web content extraction using Googles Web-search API Our agents interact with users over the Web as voice-enabled animated characters and can reach wireless devices through Instant Messenger protocol adapters',NLP
'This article introduces Xiao-Shih the first intelligent question answering bot on Chinese-based massive open online courses (MOOCs) Question answering is critical for solving individual problems However instructors on MOOCs must respond to many questions and learners must wait a long time for answers To address this issue Xiao-Shih integrates many novel natural language processing and machine learning approaches to achieve state-of-the-art performance Furthermore Xiao-Shih has a built-in self-enriched mechanism for expanding the knowledge base through open community-based question answering This article proposes a novel approach known as spreading question similarity (SQS) which iterates similar keywords on our keyword networks to find duplicate questions Compared with BERT an advanced neural language model the results showed that SQS outperforms BERT on recall and accuracy above a prediction probability threshold of 08 After training Xiao-Shih achieved a perfect correct rate Furthermore Xiao-Shih outperforms Jill Watson 10 which is a noted question answering bot on answer rate with the self-enriched mechanism',NLP
'The issue of explainability for autonomous systems is becoming increasingly prominent Several researchers and organisations have advocated the provision of a Why did you do that? button which allows a user to interrogate a robot about its choices and actions We take previous work on debugging cognitive agent programs and apply it to the question of supplying explanations to end users in the form of answers to why-questions These previous approaches are based on the generation of a trace of events in the execution of the program and then answering why-questions using the trace We implemented this framework in the agent infrastructure layer and in particular the Gwendolen programming language it supports - extending it in the process to handle the generation of applicable plans and multiple intentions In order to make the answers to why-questions comprehensible to end users we advocate a two step process in which first a representation of an explanation is created and this is subsequently converted into natural language in a way which abstracts away from some events in the trace and employs application specific predicate dictionaries in order to translate the first-order logic presentation of concepts within the cognitive agent program in natural language A prototype implementation of these ideas is provided',NLP
'Advances in the Natural Language Processing (NLP) and machine learning fields have led to the development of automated methods for the recognition of personality traits from text available from social media and similar sources Systems of this kind exploit the close relation between lexical knowledge and personality models - such as the well-known Big Five model - to provide information about the author of an input text in a non-intrusive fashion and at a low cost Although now a well-established research topic in the field the computational recognition of personality traits from text still leaves a number of research questions worth further exploration In particular this paper attempts to shed light on three main issues: (i) whether we may develop psycholinguistics-motivated models of personality recognition when such knowledge sources are not available for the target language under consideration; (ii) whether the use of psycholinguistic knowledge may be still superior to contemporary word vector representations; and (iii) whether we may infer certain personality facets from a corpus that does not explicitly convey this information In this paper these issues are dealt with in a series of individual experiments of personality recognition from Facebook text whose initial results should aid the future development of more robust systems of this kind',NLP
'In this paper we describe how a major retailers recommender system contextualizes the information that is passed to it to provide real-time in-store recommendations at a high level We specifically focus on the data pre-processing ideas that were necessary for the model to learn The paper describes the ideas and reasoning behind crucial data transformations and then illustrates a learning model inspired by the work done in Natural Language Processing',NLP
'Artificial intelligence will have a great impact on the rather traditional field of anatomy Techniques of artificial intelligence can advance anatomical research in a wide range of applications Fuzzy logic is especially useful in facilitating the use of natural language in the mathematical description of structures or functions Examples presented demonstrate the broad use of information technology in anatomical applications including description and classification knowledge representation image processing and three-dimensional anatomical atlases (C) 2001 Elsevier Science BV All rights reserved',NLP
'We explore the evolution of digital career advising companions for the rapidly growing knowledge economies to enable continuous evaluation and re-skilling of workforce in a wide range of domains These companions deal with a variety of unstructured data sources to glean actionable insights We present our experiences from building one such companion and describe interesting natural language processing and machine learning challenges and open problems',NLP
'The study of semantic role labeling is a hotspot in the field of Natural Language Processing This paper together with rationalism and empiricism with the principle of pragmatism from the perspective of semantic information processing poses an approach to label the semantic role of Korean The approach theoretically based on the level-framework of Korean verbs syntax and semantic assisted with feature vector-based approach combined with the classification marked database of category and concept is used to test marked corpus for semantic role labeling study',NLP
'Digital maps greatly support storytelling about territories especially when enriched with data describing cultural societal and ecological aspects conveying emotional messages that describe the territory as a whole Story maps are interactive online digital narratives that can describe a territory beyond its map by enriching the map with text pictures videos and other multimedia information This paper presents a semi-automatic workflow to produce story maps from textual documents containing territory data An expert first assembles one territory-contextual document containing text and images Then automatic processes use natural language processing and Wikidata services to (i) extract key concepts (entities) and geospatial coordinates associated with the territory (ii) assemble a logically-ordered sequence of enriched story-map events and (iii) openly publish online story maps and an interoperable Linked Open Data semantic knowledge base for event exploration and inter-story correlation analyses Our workflow uses an Open Science-oriented methodology to publish all processes and data Through our workflow we produced story maps for the value chains and territories of 23 rural European areas of 16 countries Through numerical evaluation we demonstrated that territory experts considered the story maps effective in describing their territories and appropriate for communicating with citizens and stakeholders',NLP
'Clinical documentation in electronic health records contains crucial narratives and details about patients and their care Natural language processing (NLP) can unlock the information conveyed in clinical notes and reports and thus plays a critical role in real-world studies The NLP Working Group at the Observational Health Data Sciences and Informatics (OHDSI) consortium was established to develop methods and tools to promote the use of textual data and NLP in real-world observational studies In this paper we describe a framework for repre-senting and utilizing textual data in real-world evidence generation including representations of information from clinical text in the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) the workflow and tools that were developed to extract transform and load (ETL) data from clinical notes into tables in OMOP CDM as well as current applications and specific use cases of the proposed OHDSI NLP solution at large consortia and individual institutions with English textual data Challenges faced and lessons learned during the process are also discussed to provide valuable insights for researchers who are planning to implement NLP so-lutions in real-world studies',NLP
'Information Extraction is an important task in Natural Language Processing consisting of finding a structured representation for the information expressed in natural language text Two key steps in information extraction are identifying the entities mentioned in the text and the relations among those entities In the context of Information Extraction for the World Wide Web unsupervised relation extraction methods also called Open Relation Extraction (ORE) systems have become prevalent due to their effectiveness without domain-specific training data In general these systems exploit part-of-speech tags or semantic information from the sentences to determine whether or not a relation exists and if so its predicate This paper discusses some of the issues that arise when even moderately complex sentences are fed into ORE systems A process for re-structuring such sentences is discussed and evaluated The proposed approach replaces complex sentences by several others that together convey the same meaning and are more amenable to extraction by current ORE systems The results of an experimental evaluation show that this approach succeeds in reducing the processing time and increasing the accuracy of the state-of-the-art ORE systems',NLP
'3D object detection is playing a key role in the perception process of autonomous driving and industrial robots automation Inherent characteristics of point cloud raise an enormous challenge to both spatial representation and association analysis Unordered point cloud spatial data structure and density variations caused by gradually varying distances to LiDAR make accurate and robust 3D object detection even more difficult In this paper we present a novel transformer network POAT-Net for 3D point cloud object detection Transformer is credited with the great success in Natural Language Processing (NLP) and exhibiting inspiring potentials in point cloud processing Our method POAT-Net is inherently insensitive to element permutations within the unordered point cloud The associations between local points contribute significantly to 3D object detection or other 3D tasks Parallel offset-attention is leveraged to highlight and capture subtle associations between local points To overcome the non-uniform density distribution of different objects we exploit Normalized multi-resolution Grouping (NMRG) strategy to enhance the non-uniform density adaptive ability for POAT-Net Quantitative experimental results on KITTI3D dataset demonstrate our method achieves the state-of-the-art performance',NLP
'Malware is a term that refers to any malicious software used to harm or exploit a device service or network The presence of malware in a system can disrupt operations and the availability of information in networks while also jeopardizing the integrity and confidentiality of such information which poses a grave issue for sensitive and critical operations Traditional approaches to malware detection often used by antivirus software are not robust in detecting previously unseen malware As a result they can often be circumvented by finding and exploiting vulnerabilities of the detection system This study involves using natural language processing techniques considering the recent advancements made in the field in recent years to analyze the strings present in the executable files of malware Specifically we propose a topic modeling-based approach whereby the strings of a malwares executable file are treated as a language abstraction to extract relevant topics which can then be used to improve a classifiers detection performance Finally through experiments using a publicly available dataset the proposed approach is demonstrated to be superior in performance to traditional techniques in its detection ability specifically in terms of performance measures such as precision and accuracy',NLP
'Currently personal data collection and processing are widely used while providing digital services within mobile sensing networks for their operation personalization and improvement Personal data are any data that identifiably describe a person Legislative and regulatory documents adopted in recent years define the key requirements for the processing of personal data They are based on the principles of lawfulness fairness and transparency of personal data processing Privacy policies are the only legitimate way to provide information on how the personal data of service and device users is collected processed and stored Therefore the problem of making privacy policies clear and transparent is extremely important as its solution would allow end users to comprehend the risks associated with personal data processing Currently a number of approaches for analyzing privacy policies written in natural language have been proposed Most of them require a large training dataset of privacy policies In the paper we examine the existing corpora of privacy policies available for training discuss their features and conclude on the need for a new dataset of privacy policies for devices and services of the Internet of Things as a part of mobile sensing networks The authors develop a new technique for collecting and cleaning such privacy policies The proposed technique differs from existing ones by the usage of e-commerce platforms as a starting point for document search and enables more targeted collection of the URLs to the IoT device manufacturers privacy policies The software tool implementing this technique was used to collect a new corpus of documents in English containing 592 unique privacy policies The collected corpus contains mainly privacy policies that are developed for the Internet of Things and reflect the latest legislative requirements The paper also presents the results of the statistical and semantic analysis of the collected privacy policies These results could be further used by the researchers when elaborating techniques for analysis of the privacy policies written in natural language targeted to enhance their transparency for the end user',NLP
'Object-oriented programming (OOP) is a collection of concepts tools and styles that are supposed to help programmers build more complex programs and support them easily However some OOP benefits become disadvantages The Natural OOP (NOOP) in Elica does not use any specialized syntax to utilize OOP The evolutional metaphor applied to NOOP changes the focus of OOP by treating classes and objects as species and individual creatures The paper confronts activities done in a NOOP-based program with evolutionary processes like inheritance birth mutation life and death',NLP
'In this study chatbots creation technology was developed The main aspects of using the necessary components have been analyzed and suggested to apply structural and architectural principles as in global modern software solutions The difference between chatbots of use in server-client systems and intelligent chatbots has been reviewed A prototype of the program was developed for the research based on the created technology with the necessary control of efficiency and effectiveness of use',NLP
'Extraction and integration of entities from textual data and linking them to knowledgebases (for further information or processing) is useful for many applications in natural language processing However a major problem in this process is disambiguation named entities might refer to different things In this work we propose a novel method to disambiguate named entities Our method is a combination of search engine results and knowledgebase repository mining results We obtained 84% correct disambiguation rates with the data sets we used',NLP
'Informedia research at Carnegie Mellon University combines speech recognition image processing and natural language processing to automatically index a digital video library This engineering report focuses on the contribution of speech analysis for transcript generation and alignment and the use of these features in library interface development By deepening the automated analysis such as using named entity extraction to identify people and place names in the audio transcript better summaries and visualizations can be produced to navigate through video libraries holding thousands of hours of material',NLP
'Social networking websites are now considered to be the best platforms for the dissemination of news articles However information sharing in social media platforms leads to explosion of fake news Traditional detection methods were focusing on content analysis while the current researchers examining social features of the news In this work we proposed a novel artificial intelligence (AI)-assisted fake news detection with deep natural language processing (NLP) model The proposed work is characterized in four layers: publisher layer social media networking layer enabled edge layer and cloud layer In this work four steps were carried out: 1) data acquisition; 2) information retrieval (IR); 3) NLP-based data processing and feature extraction; and 4) deep learning-based classification model that classifies news articles as fake or real using credibility score of publishers users messages headlines and so on Three datasets such as Buzzface FakeNewsNet and Twitter were used for evaluation of the proposed model and simulation results were computed This proposed model obtained an average accuracy of 9972% and an $F1$ score of 9833% which outperforms other existing methods',NLP
'This paper explores opinion mining using supervised learning algorithms to find the polarity of the student feedback based on pre-defined features of teaching and learning The study conducted involves the application of a combination of machine learning and natural language processing techniques on student feedback data gathered from module evaluation survey results of Middle East College Oman In addition to providing a step by step explanation of the process of implementation of opinion mining from student comments using the open source data analytics tool Rapid Miner this paper also presents a comparative performance study of the algorithms like SVM Naive Bayes K Nearest Neighbor and Neural Network classifier The data set extracted from the survey is subjected to data preprocessing which is then used to train the algorithms for binomial classification The trained models are also capable of predicting the polarity of the student comments based on extracted features like examination teaching etc The results are compared to find the better performance with respect to various evaluation criteria for the different algorithms',NLP
'Due to an exponential growth in the generation of web data the need for tools and mechanisms for automatic summarization of Web documents has become very critical Web data can be accessed from multiple sources for e g on different Web pages which makes searching for relevant pieces of information a difficult task Therefore an automatic summarizer is vital towards reducing human effort Text summarization is an important activity in the analysis of a high volume text documents and is currently a major research topic in Natural Language Processing It is the process of generation of the summary of an input document by extracting the representative sentences from it In this paper we present a novel technique for generating the summarization of domain-specific text from a single Web document by using statistical NLP techniques on the text in a reference corpus and on the web document The summarizer proposed generates a summary based on the calculated Sentence Weight (SW) the rank of a sentence in the documents content the number of terms and the number of words in a sentence and using term frequency in the input corpus',NLP
'Once a bug in software is reported developers have to determine which source files are related to the bug This process is referred to as bug localization and an automatic way of bug localization is important to improve developers productivity This paper proposes an approach called DrewBL to efficiently localize faulty files for a given bug report using a natural language processing tool word2vec In DrewBL we first build a vector space model named semantic-VSM which represents a distributed representation of words in the bug report and source code files and next compute the relevance between them by feeding the constructed model to word2vec We also present an approach called CombBL to further improve the accuracy of bug localization which employs not only the proposed DrewBL but also existing bug localization techniques such as BugLocator based on textual similarity and Bugspots based on bug-fixing history in a combinational manner This paper gives our early experimental results to show the effectiveness and efficiency of the proposed approaches using two open source projects',NLP
'The teaching and learning of algebraic word problems is a basic component of elementary education Recently to facilitate its learning a few approaches for automatically solving algebraic and arithmetic word problems have been proposed These systems generally use either natural language processing (NLP) or a combination of NLP and machine learning However they have low accuracy due to their large feature sets extracted using limited preprocessing techniques In this research work we propose a template-based approach that was developed by following a two-step process In the first step we predict an equation template from a training dataset using NLP and a classification mechanism The next step is to instantiate the predicted template with nouns and numbers through reasoning To validate the proposed methodology a prototype system was implemented We then compared the proposed system with the existing systems using their respective datasets and the proposed dataset The experimental results show improvement in accuracy with an average precision of 806% and average recall of 835%',NLP
'People use questions to elicit information from other people in their everyday lives and yet the most common method of obtaining information from a search engine is by posing keywords There has been research that suggests users are better at expressing their information needs in natural language however the vast majority of work to improve document retrieval has focused on queries posed as sets of keywords or Boolean queries This paper focuses on improving document retrieval for the subset of natural language questions asking about how something is done We classify questions as asking either for a description of a process or asking for a statement of fact with better than 90% accuracy Further we identify non-content features of documents relevant to questions asking about a process Finally we demonstrate that we can use these features to significantly improve the precision of document retrieval results for questions asking about a process Our approach based on exploiting the structure of documents shows a significant improvement in precision at rank one for questions asking about how something is done (c) 2006 Elsevier Ltd All rights reserved',NLP
'The technology development cycle continues to accelerate and novelty analysis is becoming increasingly important for R & D planning as well as in the patent application process Recently thanks to significant advances in both text mining and natural language processing researchers started to look into AI-assisted novelty analysis of technical content including patents However existing language models do not take into account the unique characteristics of technical elements in patent documents nor do they provide any explanation of their decisions including which technical elements represent a novelty Therefore we developed an eXplainable AI (XAI) model that evaluates novelty takes into account the claim structure of a patent and provides an explanation The proposed framework of an XAI model for patent novelty consists of the following three parts: (1) dataset construction (2) model architecture and (3) inference process The training dataset for patent novelty is constructed using a full-text dataset provided by the European Patent Office (EPO) A self-explainable novelty classification model is proposed and investigated Using the fitted model the inference results are then analyzed by extracting patents in the field of vehicle communication networks The inference process is done by applying the fitted model to patents in the vehicle communication network field and can be expanded to address potential applications The performance of the proposed model is then verified by comparing it with the results of similar studies We also discuss practical applications from the perspective of patent examiners and technical planning practitioners This study involves both an academic contribution that uses a novel approach to technology management via an XAI model and a practical contribution that can be used for patent analysis',NLP
'These days a lot of crime-related events take place all over the world Most of them are reported in news portals and social media Crime-related event extraction from the published texts can allow monitoring analysis and comparison of police or criminal activities in different countries or regions Existing approaches to event extraction mainly suggest processing texts in English French Chinese and some other resource-rich and well-annotated languages This paper presents a parallel corpus-based approach that follows a closed-domain event extraction methodology to event extraction from web news articles in low-resource languages To identify the event its arguments and the arguments roles in the source-language part of the corpus we utilize an enhanced pattern-based method that involves the multilingual synonyms dictionary with knowledge about crime-related concepts and logic-linguistic equations The event extraction from the target-language part of the corpus uses a cross-lingual crime-related event extraction transfer technique that is based on supplementary knowledge about the semantic similarity patterns of the considered pair of languages The presented approach does not require a preliminarily annotated corpus for training making it more attractive to low-resource languages and allows extracting TRANSFER CRIME and POLICE types of events and their seven subtypes from various topics of news articles simultaneously Implementation of our approach for the Russian-Kazakh parallel corpus of news portals articles allowed obtaining the F1-measure of crime-related event extraction of over 82% for the source language and 63% for the target language',NLP
'Background: The Institute of Medicine has identified patient safety as a key goal for health care in the United States Detecting vaccine adverse events is an important public health activity that contributes to patient safety Reports about adverse events following immunization (AEFI) from surveillance systems contain free-text components that can be analyzed using natural language processing To extract Unified Medical Language System ( UMLS) concepts from free text and classify AEFI reports based on concepts they contain we first needed to clean the text by expanding abbreviations and shortcuts and correcting spelling errors Our objective in this paper was to create a UMLS-based spelling error correction tool as a first step in the natural language processing (NLP) pipeline for AEFI reports Methods: We developed spell checking algorithms using open source tools We used de-identified AEFI surveillance reports to create free-text data sets for analysis After expansion of abbreviated clinical terms and shortcuts we performed spelling correction in four steps: ( 1) error detection ( 2) word list generation ( 3) word list disambiguation and ( 4) error correction We then measured the performance of the resulting spell checker by comparing it to manual correction Results: We used 12056 words to train the spell checker and tested its performance on 8131 words During testing sensitivity specificity and positive predictive value (PPV) for the spell checker were 74% ( 95% CI: 74 - 75) 100% ( 95% CI: 100 - 100) and 47% ( 95% CI: 46% - 48%) respectively Conclusion: We created a prototype spell checker that can be used to process AEFI reports We used the UMLS Specialist Lexicon as the primary source of dictionary terms and the WordNet lexicon as a secondary source We used the UMLS as a domain-specific source of dictionary terms to compare potentially misspelled words in the corpus The prototype sensitivity was comparable to currently available tools but the specificity was much superior The slow processing speed may be improved by trimming it down to the most useful component algorithms Other investigators may find the methods we developed useful for cleaning text using lexicons specific to their area of interest',NLP
'We do not know how humans reason whether they reason using natural language (NL) or not and we are not interested in proving or disproving such a proposition Nonetheless it seems that a very expressive transparent medium humans communicate with state their problems in and justify how they solve these problems is NL Hence we wished to use NL as a Knowledge Representation(KR) in NL knowledge-based (KB) sytems However NL is full of ambiguities In addition there axe syntactic and semantic processing complexities associated with NL Hence we consider a quasi-NL KR with a tractable inference relation We believe that such a representation bridges the gap between an expressive semantic representation (SR) sought by the Natural Language Processing (NLP) community and an efficient KR sought by the KR community In addition to being a KR we use the quasi-NL language as a SR for a subset of English that it defines Also it is capable of a general-purpose domain-independent inference component which is according to semanticists all what it takes to test a semantic theory in any NLP system This paper gives only a flavour for this quasi-NL KR and its capabilities (for a detailed study see [14])',NLP
'Modeling increases the importance of processes significantly but also imposes higher requirements for the accuracy of process specifications since an error in the design of a process may only be discovered after it already produces large cumulative losses We believe that modeling tools can help build better models in a shorter time This inevitably results in the need to build formal models that can be theoretically verified A category as well as a model is a mixture of graphical information and algebraic operations Therefore category language seems to be the most general to describe the models The category theory offers an integrated vision of the concepts of a model and also provides mechanisms to combine models mechanisms to migrate between models and mechanisms to build bridges between models So category theory simplifies how we think and use models In this paper we will use the language offered by the category theory to formalize the concept of Modeling Method with the demonstration of the Categorical Modeling Method concept for the Petri Net grammar',NLP
'Developing resources which can be used for Natural Language Processing is an extremely difficult task for any language but is even more so for less privileged (or less computerized) languages One way to overcome this difficulty is to adapt the resources of a linguistically close resource rich language In this paper we discuss how the cost of such adaption can be estimated using subjective and objective measures of linguistic similarity for allocating financial resources time manpower etc Since this is the first work of its kind the method described in this paper should be seen as only a preliminary method indicative of how better methods can be developed Corpora of several less computerized languages had to be collected for the work described in the paper which was difficult because for many of these varieties there is not much electronic data available Even if it is it is in non-standard encodings which means that we had to build encoding converters for these varieties The varieties we have focused on are some of the varieties spoken in the South Asian region',NLP
'Entity linking in knowledge-based question answering (KBQA) is intended to construct a mapping relation between a mention in a natural language question and an entity in the knowledge base Most research in entity linking focuses on long text but entity linking in open domain KBQA is more concerned with short text Many recent models have tried to extract the features of raw data by adjusting the neural network structure However the models only perform well with several datasets We therefore concentrate on the data rather than the model itself and created a model DME (Domain information Mining and Explicit expressing) to extract domain information from short text and append it to the data The entity linking model will be enhanced by training with DME-processed data Besides we also developed a novel negative sampling approach to make the model more robust We conducted experiments using the large Chinese open source benchmark KgCLUE to assess model performance with DME-processed data The experiments showed that our approach can improve entity linking in the baseline models without the need to change their structure and our approach is demonstrably transferable to other datasets',NLP
'This study analyzed perceptions of Indians regarding COVID-19 booster dose vaccines using natural language processing techniques particularly sentiment analysis and topic modeling We analyzed tweets generated by Indian citizens for this study In late July 2022 the Indian government hastened the process of COVID-19 booster dose vaccinations Understanding the emotions and concerns of the citizens regarding the health policy being implemented will assist the government health policy officials and policymakers implement the policy efficiently so that desired results can be achieved Seventy-six thousand nine hundred seventy-nine tweets were used for this study The sentiment analysis study revealed that out of those 76979 tweets more than half (n = 40719 tweets (528%) had negative sentiments 24242 tweets (315%) had neutral sentiments and 12018 tweets (156%) had positive sentiments Social media posts by Indians on the COVID-19 booster doses have focused on the feelings that younger people do not need vaccines and that vaccinations are unhealthy',NLP
'Over the past few years various word-level textual attack approaches have been proposed to reveal the vulnerability of deep neural networks used in natural language processing Typically these approaches involve an important optimization step to determine which substitute to be used for each word in the original input However current research on this step is still rather limited from the perspectives of both problem-understanding and problem-solving In this paper we address these issues by uncovering the theoretical properties of the problem and proposing an efficient local search algorithm (LS) to solve it We establish the first provable approximation guarantee on solving the problem in general cases Extensive experiments involving 5 NLP tasks 8 datasets and 26 NLP models show that LS can largely reduce the number of queries usually by an order of magnitude to achieve high attack success rates Further experiments show that the adversarial examples crafted by LS usually have higher quality exhibit better transferability and can bring more robustness improvement to victim models by adversarial training',NLP
'With the wide application of attention mechanism in multitudinous field of natural language processing (NLP) to date various deep neural networks based on this mechanism have been introduced and developed However a major problem with this kind of application is that a long time will be consumed due to the current networks still need to rely on their own ability to form attention values from scratch during the training In this paper we propose an auxiliary method called the Guided Attention Mechanism (GAM) which utilizes the prior knowledge to guide the network to form attention values in NLP field thereby shortening the network training time and making the attention values more accurate This work designed two sets of prior knowledge generation processes based on the regularization method and the deep learning method respectively And the prior knowledge is used to guide the attention values of the original network in terms of values and angles The experimental results show that compared with the original network the classification accuracy of the network using GAM is improved by about 2% and the training time is reduced by 5 similar to 9%',NLP
'Despite the fact it has been recognised since Aristotle that ethos and credibility play a critical role in many types of communication these facts are rarely studied in linguistically oriented AI which has enjoyed such success in processing complex features as sentiment opinion and most recently arguments This paper shows how a text analysis pipeline of structural and statistical approaches to natural language processing (NLP) can be deployed to tackle ethos by mining linguistic resources from the political domain We summarise a coding scheme for annotating ethotic expressions; present the first openly available corpus to support further comparative research in the area; and report results from a system for automatically recognising the presence and polarity of ethotic expressions Finally we hypothesise that in the political sphere ethos analytics - including recognising who trusts whom and who is attacking whose reputation - might act as a powerful toolset for understanding and even anticipating the dynamics of governments By exploring several examples of correspondence between ethos analytics in political discourse and major events and dynamics in the political landscape we uncover tantalising evidence in support of this hypothesis',NLP
'The last few years have seen tremendous growth in human microbiome research with a particular focus on the links to both mental and physical health and disease Medical and experimental settings provide initial sources of information about these links but individual studies produce disconnected pieces of knowledge bounded in context by the perspective of expert researchers reading full-text publications Building a knowledge base (KB) consolidating these disconnected pieces is an essential first step to democratize and accelerate the process of accessing the collective discoveries of human disease connections to the human microbiome In this article we survey the existing tools and development efforts that have been produced to capture portions of the information needed to construct a KB of all known human microbiome-disease associations and highlight the need for additional innovations in natural language processing (NLP) text mining taxonomic representations and field-wide vocabulary standardization in human microbiome research Addressing these challenges will enable the construction of KBs that help identify new insights amenable to experimental validation and potentially clinical decision support',NLP
'When used effectively reflective writing tasks can deepen learners understanding of key concepts help them critically appraise their developing professional identity and build qualities for lifelong learning As such reflecting writing is attracting substantial interest from universities concerned with experiential learning reflective practice and developing a holistic conception of the learner However reflective writing is for many students a novel genre to compose in and tutors may be inexperienced in its assessment While these conditions set a challenging context for automated solutions natural language processing may also help address the challenge of providing real time formative feedback on draft writing This paper reports progress in designing a writing analytics application detailing the methodology by which informally expressed rubrics are modelled as formal rhetorical patterns a capability delivered by a novel web application This has been through iterative evaluation on an independently human-annotated corpus showing improvements from the first to second version We conclude by discussing the reasons why classifying reflective writing has proven complex and reflect on the design processes enabling work across disciplinary boundaries to develop the prototype to its current state',NLP
'The advances achieved in Natural Language Processing make it possible to automatically mine information from electronically created documents Many Natural Language Processing methods that extract information from texts make use of annotated corpora but these are scarce in the clinical domain due to legal and ethical issues In this paper we present the creation of the IxaMed-GS gold standard composed of real electronic health records written in Spanish and manually annotated by experts in pharmacology and pharmacovigilance The experts mainly annotated entities related to diseases and drugs but also relationships between entities indicating adverse drug reaction events To help the experts in the annotation task we adapted a general corpus linguistic analyzer to the medical domain The quality of the annotation process in the IxaMed-GS corpus has been assessed by measuring the inter-annotator agreement which was 9053% for entities and 8286% for events In addition the corpus has been used for the automatic extraction of adverse drug reaction events using machine learning (C) 2015 Elsevier Inc All rights reserved',NLP
'Many organizations and healthcare providers are concerned on implementing new tools and applications to improve the quality of the provided services and to speed up the processing time Discharge summary report (DSR) is one of the curtail reports that consumes the physician efforts and time to integrate it Only limited researches exist on automating the integration of DSR which is based on combining various patient data from his/her Hospital Medical Reports (HMR) The intention for this paper is to construct a formative model for constructing Intelligent Discharge System (IDS) automatically without human intervention Method: DSR integration composed of text summarization technique which provides the ability to extract an important data from a large size of text that collected from several parts of patients HMR In addition to Natural Language Processing (NLP) techniques such as Segmentation Stemming and Stopword Filtering in order to reduce the time for generating the DSR Result: The produced DSR output contains all the required fields that should be included in the report which has been approved by the physicians The system showed an impressive improvement in DSR automated production',NLP
'Software documents are commonly processed by natural language processing (NLP) libraries to extract information The libraries provide similar functional APIs to achieve NLP tasks numerous toolkits result in a problem of selection In this work we propose a method to combine the strengths of different NLP libraries to avoid the subjective selection of a specific NLP library The combined usage is conducted through two steps ie document-level selection of primary NLP library and sentence-level overwriting The primary NLP library is determined according to the overlap degree of the results The highest overlap degree indicated the most effective NLP library on a specific NLP task Through sentence-level overwriting the possible fine-gained improvements from other libraries are extracted to overwrite the outputs of primary library We evaluate the combined method with six widely used NLP libraries and 200 documents from three different sources The results show that the combined method can generally outperform all the studied NLP libraries in terms of accuracy The finding means that our combined method can be used instead of individual NLP library for more effective results',NLP
'There exist a high demand to provide explainability to artificial intelligence systems where decision making models are included This paper focuses on crowd decision making using natural language evaluations from social media with the aim to provide explainability We present the Explainable Crowd Decision Making based on Subgroup Discovery and Attention Mechanisms (ECDM-SDAM) methodology as an a posteriori explainable process that captures the wisdom of crowds that is naturally provided in social media opinions It extracts the opinions from social media texts using a deep learning based sentiment analysis approach called Attention based Sentiment Analysis Method The methodology includes a backward process that provides explanations to justify its sense-making procedure by applying mainly the attention mechanism on texts and subgroup discovery on opinions We evaluate the methodology in the real case study of the TripR-2020Large dataset for restaurant choice The results show that the ECDM-SDAM methodology provides easy understandable explanations that elucidates the key reasons that support the output of the decision process',NLP
'It is widely recognized that the ability to exploit Natural Language Processing (NLP) text mining strategies has the potential to increase productivity and innovation in the sciences by orders of magnitude by enabling scientists to pull information from research articles in scientific disciplines such as genomics and biomedicine The Language Applications (LAPPS) Grid is an infrastructure for rapid development of natural language processing applications (NLP) that provides an ideal platform to support mining scientific literature Its Galaxy interface and the interoperability among tools together provide an intuitive and easy-to-use platform and users can experiment with and exploit NLP tools and resources without the need to determine which are suited to a particular task and without the need for significant computer expertise The LAPPS Grid has collaborated with the developers of PubAnnotation to integrate the services and resources provided by each in order to greatly enhance the users ability to annotate scientific publications and share the results This poster/demo shows how the LAPPS Grid can facilitate mining scientific publications including identification and extraction of relevant entities relations and events; iterative manual correction and evaluation of automatically-produced annotations and customization of supporting resources to accommodate specific domains',NLP
'Health information technology has been used in long-term care to improve outcomes and reduce cost In Tiger Pace an aging in place facility from Columbia MO we deployed sensor networks together with an electronic health record (EHR) system to provide early illness recognition In this paper we describe a methodology for early illness based on non-wearable sensor data and concepts extracted from nursing notes using Natural Language Processing (NLP) The methodology is inspired from genomic sequence annotation using BLAST First we extract a set of Unified Medical Language System (UMLS) concepts from each nursing note using Metamap a NLP tool provided by UMLS Then we associate each daily sensor sequence with the medical concepts related to the nursing notes issued that day for that patient Finally to infer the health concepts for an unknown day we compute the similarity between its sensor sequence and those available in the database The challenges presented by this method are finding the most suitable multi-attribute time sequence similarity and aggregation of the retrieved concepts On a pilot dataset from three Tiger Place residents with a total of 1685 sensor days and 358 nursing records we obtained an average precision of 034 and a recall of 052',NLP
'Word embeddings have been successfully used in several natural language processing tasks (NLP) and speech processing Different approaches have been introduced to calculate word embeddings through neural networks In the literature many studies focused on word embedding evaluation but for our knowledge there are still some gaps This paper presents a study focusing on a rigorous comparison of the performances of different kinds of word embeddings These performances are evaluated on different NLP and linguistic tasks while all the word embeddings are estimated on the same training data using the same vocabulary the same number of dimensions and other similar characteristics The evaluation results reported in this paper match those in the literature since they point out that the improvements achieved by a word embedding in one task are not consistently observed across all tasks For that reason this paper investigates and evaluates approaches to combine word embeddings in order to take advantage of their complementarity and to look for the effective word embeddings that can achieve good performances on all tasks As a conclusion this paper provides new perceptions of intrinsic qualities of the famous word embedding families which can be different from the ones provided by works previously published in the scientific literature',NLP
'Readability is a linguistic feature that indicates how difficult it is to read a text Traditional readability formulas were made for the English language This study evaluates their adequacy to the Portuguese language We applied the traditional formulas in 10 parallel corpora We verified that the Portuguese language had higher grade scores (less readability) in the formulas that use the number of syllables per words or number of complex words per sentence Formulas that use letters by words instead of syllables by words output similar grade scores Considering this we evaluated the correlation of the complex words in 65 Portuguese school books of 12 schooling years We found out that the concept of complex word as a word with 4 or more syllables instead of 3 or more syllables as originally used in traditional formulas applied to English texts is more correlated with the grade of Portuguese school books In the end for each traditional readability formula we adapted it to the Portuguese language performing a multiple linear regression in the same dataset of school books',NLP
'Data augmentation (DA) is a universal technique to reduce overfitting and improve the robustness of machine learning models by increasing the quantity and variety of the training dataset Although data augmentation is essential in vision tasks it is rarely applied to text datasets since it is less straightforward Some studies have concerned text data augmentation but most of them are for the majority languages such as English or French There have been only a few studies on data augmentation for minority languages eg Korean This study fills the gap by demonstrating several common data augmentation methods and Korean corpora with pre-trained language models In short we evaluate the performance of two text data augmentation approaches known as text transformation and back translation We compare these augmentations among Korean corpora on four downstream tasks: semantic textual similarity (STS) natural language inference (NLI) question duplication verification (QDV) and sentiment classification (STC) Compared to cases without augmentation the performance gains when applying text data augmentation are 224% 219% 066% and 008% on the STS NLI QDV and STC tasks respectively',NLP
'Visual question answering (VQA) is a complicated Turing-AI task which needs not only to understand the multi-modality inputs but also reason to provide correct answer Nowadays there are complicated and sophisticated modules for reasoning in popular works However the language representation which is frequently treated as the guider of VQA hasnt been fully explored in current researches leading to insufficient reasoning and unsatisfactory answer In this work two types of method including VieAns and Repeat-Padding which focus on language processing are proposed to balance the sentence by cropping and padding the question where the language information is transformed to different expressions and further pushes the language model to grab more representative features for further boosting the accuracy of predicted answers Experiments on the benchmark COCO-QA and VQA20 datasets are conducted to demonstrate the effectiveness of the proposed method Particularly the proposed RepeatPadding method is more suitable for different language models (C) 2020 Elsevier Inc All rights reserved',NLP
'Natural language processing has been split into two sub-fields: semantic and syntactic analysis Semantic processing deals with creating vector representations for each word The goal is for the vectors to be dense and that relationships between words are made clear by their vectors Word2Vec Skip-Gram model is one such semantic word embedding model that processes the words into vectors based on the co-occurrences of the words within a fixed context window Syntactic processing on the other hand deals with gleaning textual information from the structure of the text A popular method of syntactic processing is dependency parsing where connections are explicitly drawn between different pairs of words in a phrase or clause where one word modifies the other Human beings make use of both semantic and syntactic information when processing text As such combining semantic and syntactic processing is important for medical text analysis The dependencies between words define specific medical concepts such as diseases and symptoms The aim of this research paper is to investigate an altered word vector model-the SD Skip-Gram model which incorporates the dependencies between words The SD Skip-Gram model is compared against the basic Skip-Gram model for analyzing common words and medical disease and symptom concepts The returned results demonstrate that the SD Skip-Gram model can identify the relationships between the common words as well as the basic Skip-Gram model and perform better than the basic Skip-Gram model in identifying the related disease and symptom concepts',NLP
'We propose a system for automated essay grading using ontologies and textual entailment The process of textual entailment is guided by hypotheses which are extracted from a domain ontology Textual entailment checks if the truth of the hypothesis follows from a given text We enact textual entailment to compare students answer to a model answer obtained from ontology We validated the solution against various essays written by students in the chemistry domain',NLP
'This contribution addresses generation of natural language descriptions for human actions and behaviour observed in video streams The work starts with implementation of conventional image processing techniques to extract high-level features from video Because human is often the most important and also interesting feature description focuses on humans and their activities Although feature extraction processes are erroneous at various levels we explore approaches to put them together to produce a coherent description Evaluation is made by calculating the overlap similarity score between human authored and machine generated descriptions',NLP
'The techniques for organizing and retrieving the artifacts from digital libraries (DLs) semantically are discussed which include letting the taxonomies and semantic relations work in tandem to index the artifacts in DLs; integrating the techniques used in natural language processing and taxonomies to help users to start their retrieval processes; and ranking scientific papers on similarity in terms of contents or ranking the relevant papers on multi-factors These techniques are verified through the design and implementation of a prototype of DLs for scientific paper management',NLP
'Sentiment analysis is gaining high importance as a field of study in academia and industry It becomes one of the most active research field in Artificial Intelligence and web mining Most of the work on sentiment analysis has focused on the English language On the other hand few studies are done on other languages that also dominate the Internet including Arabic which is currently ranked as the fourth language used in the Internet Research on this language remains weak and still suffers from several limitations and challenges Moreover people in several social media tend to use other kinds of data to express their opinions mood and sentiment including images videos and audios Thus it is crucial to mine understand and identify sentiments from different modalities In this paper we first present an outline of the progress of unimodal and multimodal sentiment analysis most notably the Arabic language and then we present the different limits and challenges of the Arabic in this domain',NLP
'Documenting analyzing and designing architecture are activities that can be linked to art and technique The humanistic technical and conjectural training of architects has this ambiguity between artistic and technical aspects which are intrinsically linked to the use of a language The language that makes the architectural project possible is the architectural graphic language and its tool is architectural drawing in its different manifestations and modalities Drawing is responsible for acquiring knowledge analysis and experience of the built environment from its conceptual phases to the final ones linked to the constructive representative and communicative processes It is our job as agents involved in this process to know how to adapt the graphic tools of thought to the appropriate graphic modality Analog and digital media transgress their boundaries in this process of adaptation which is highly productive and should be assumed by architects in a natural way in their architectural production procedures supported by drawing as a means of knowledge demonstrating the ability to adapt to the technological and social changes of recent decades',NLP
'This paper describes AQUA an experimental question answering system AQUA combines Natural Language Processing (NLP) Ontologies Logic and Information Retrieval technologies in a uniform framework AQUA makes intensive use of an ontology in several parts of the question answering system The ontology is used in the refinement of the initial query the reasoning process and in the novel similarity algorithm The similarity algorithm is a key feature of AQUA It is used to find similarities between relations used in the translated query and relations in the ontological structures',NLP
'Human language appears to be unique among natural communication systems and such uniqueness impinges on both nature and nurture Human babies are endowed with cognitive abilities that predispose them to learn language and this process cannot operate in an impoverished environment To be effectively complete the acquisition of human language in human children requires highly socialised forms of learning scaffolded over years of prolonged and intense caretaker-child interactions How genes and environment operate in shaping language is unknown These two components have traditionally been considered as independent and often pitted against each other in terms of the nature versus nurture debate This perspective article considers how innate abilities and experience might instead work together In particular it envisages potential scenarios for research in which early caregiver verbal and non-verbal attachment practices may mediate or moderate the expression of human genetic systems for language (C) 2017 Elsevier BV All rights reserved',NLP
'In recent years the use of social media has rapidly increased and developed significant influence on its users In the study of the behavior reactions approval and interactions of social media users detecting the polarity (positive negative neutral) of news posts is of considerable importance This proposed research aims to collect data from Arabic social media pages with the posts comprising the main unit in the dataset and to build a corpus of manually-processed data for training and testing Applying Natural Language Processing to the data is crucial for the computer to understand and easily manipulate the data Therefore Stop-Word removal Stemming and Normalization are applied Several classifiers such as Support Vector Machine Na?ve Bayes K-Nearest Neighbor Random Frost and Decision Tree are used to train the dataset and their accuracy is determined by data testing These two steps are carried out using the open-source WEKA tool As a result each post is categorized into three different classes: positive negative and neutral This research concludes that among the classifiers SVM reaches the highest level of accuracy with a percentage of 83% for the F1-measure',NLP
'The rapid growth of Android malware results in a large body of approaches devoted to malware analysis by leveraging machine learning algorithms However the effectiveness of these approaches primarily depends on the manual feature engineering process which is time-consuming and labor-intensive based on expert knowledge and intuition In this paper we propose an automatic approach that engineers informative features from a corpus of Android malware related technical blogs which are written in a way that mirrors the human feature engineering process However there are two main challenges First it is difficult to recognize useful knowledge in the magnanimity information of thousands of blogs To this end we leverage natural language processing techniques to process the blogs and extract a set of sensitive behaviors that might do harmful activities to users potentially Second there exists a semantic gap between the extracted sensitive behaviors and the programming language To this end we propose two semantic matching rules to match the behaviors with concrete code snippets such that the apps can be tested experimentally We design and implement a system called CTDroid for malware analysis including malware detection (MD) and familial classification (FC) After the evaluation of CTDroid on a large scale of real malware and benign apps the experimental results demonstrate that CTDroid can achieve 958% true positive rate with only 1% false positive rate for MD and 979% accuracy for FC Furthermore our proposed features are more informative than those of state-of-the-art approaches',NLP
'Synonyms dictionaries are useful resources for natural language processing Unfortunately their availability in digital format is limited as publishing companies do not release their dictionaries in open digital formats Dicionario-Aberto (Simoes and Farinha 2010) is an open and free digital synonyms dictionary for the Portuguese language It is under public domain and in textual digital format which makes it usable for any task Synonyms dictionaries are commonly used for the extraction of relations between words the construction of complex structures like ontologies or thesaurus (comparable to WordNet (Miller et al 1990)) or just the extraction of lists of words of specific type This article will present Dicionario-Aberto discussing how it was created its main characteristics the type of information present on it and the formats in which it is available Follows the description of an API designed specifically to help Dicionario-Aberto processing without the need to tackle with the dictionary format Finally we will analyze the results on some data extraction experiments extracting lists of words from a specific class and extracting relationships between words',NLP
'Word sentence and document embeddings have become the cornerstone of most natural language processing-based solutions The training of an effective embedding depends on a large corpus of relevant documents However such corpus is not always available especially for specialized heavy industries such as oil mining or steel To address the problem this paper proposes a semi-supervised learning framework to create document corpus and embedding starting from an industry taxonomy along with a very limited set of relevant positive and negative documents Our solution organizes candidate documents into a graph and adopts different explore and exploit strategies to iteratively create the corpus and its embedding At each iteration two metrics called Coverage and Context Similarity are used as proxy to measure the quality of the results Our experiments demonstrate how an embedding created by our solution is more effective than the one created by processing thousands of industry-specific document pages We also explore using our embedding in downstream tasks such as building an industry specific classification model given labeled training data as well as classifying unlabeled documents according to industry taxonomy terms',NLP
'The image of the tolerant religion of Islam has been distorted by extremists in the last two decades in many ways such as luring teenagers into terrorist acts Nowadays millions of users socialize and share ideas using social media platforms such as Twitter Typically the ideas shared on Twitter (tweets) reach and influence many people who could simply retweet them and make them even spread faster Unfortunately some of these ideas are posted by extremists who share hateful Arabic content Thus it is very important to automate the process of controlling and monitoring hateful Arabic tweets given that Arabic is the most widely used language in the Islamic world In this paper we provide a manually labeled and curated dataset of 3000 Arabic tweets that contain hateful and non-hateful tweets To automate the process of detecting hateful tweets we utilize advanced Machine Learning (ML) techniques and perform sentiment analysis to capture the meaning of the Arabic words in a proper word embedding (Word2Vec) Also we used the proposed model to classify and analyze 100000 tweets of the last decade The outcome of this work promotes future research on analyzing Arabic hateful speech by providing a manually labeled Arabic dataset and the trained model (achieved 92% accuracy) which can be used as an underlying tool by governments Internet service providers and social media applications to detect any inflammatory tweets before they spread to a wider audience',NLP
'Machine reading comprehension (MRC) of text data is a challenging task in Natural Language Processing (NLP) with a lot of ongoing research fueled by the release of the Stanford Question Answering Dataset (SQuAD) and Conversational Question Answering (CoQA) It is considered to be an effort to teach computers how to understand a text and then to be able to answer questions about it using deep learning However until now large-scale training on private text data and knowledge sharing has been missing for this NLP task Hence we present FedQAS a privacy-preserving machine reading system capable of leveraging large-scale private data without the need to pool those datasets in a central location The proposed approach combines transformer models and federated learning technologies The system is developed using the FEDn framework and deployed as a proof-of-concept alliance initiative FedQAS is flexible language-agnostic and allows intuitive participation and execution of local model training In addition we present the architecture and implementation of the system as well as provide a reference evaluation based on the SQuAD dataset to showcase how it overcomes data privacy issues and enables knowledge sharing between alliance members in a Federated learning setting',NLP
'Sentence matching is crucial to many natural language processing (NLP) tasks Generally the degree of matching is measured from either of the two perspectives: topic-based match or semantic-based match The former is to investigate if two sentences discuss the same topic and the latter performs a deep level semantic matching of texts which is currently the highlight in research Deep semantic matching requires adequate modeling from the internal structure of the language objects as well as their interactions To achieve this goal this paper proposes a multiple-perspective semantics-crossover (MPSC) model for modeling the semantic-based match of two sentences The model extracts the matching information of two sentences from the semantic interaction information generated from different angles so as to calculate the matching degree of the two sentences The MPSC model not only captures rich matching patterns at different levels but also acquires interactive features from different semantic angles It can be used to address some important issues in NLP fields such as information matching in text retrieval question-answer matching in the Q&A system and so on The experimental results show that our proposed model of MPSC has better effectiveness than some popular semantic matching approaches',NLP
'Named entity recognition (NER) is a fundamental part of other natural language processing tasks such as information retrieval question answering systems and machine translation Progress and success have already been achieved in research on the English NER systems However the Urdu NER system is still in its infancy due to the complexity and morphological richness of the Urdu language Existing Urdu NER systems are highly dependent on manual feature engineering and word embedding to capture similarity Their performance lags if the words are previously unknown or infrequent The feature-based models suffer from complicated feature engineering and are often highly reliant on external resources To overcome these limitations in this study we present several deep neural approaches that automatically learn features from the data and eliminate manual feature engineering Our extension involved convolutional neural network to extract character-level features and combine them with word embedding to handle out-of-vocabulary words The study also presents a tweets dataset in Urdu annotated manually for five named entity classes The effectiveness of the deep learning approaches is demonstrated on four benchmarks datasets The proposed method demonstrates notable progress upon current state-of-the-art NER approaches in Urdu The results show an improvement of 626% in the F1 score',NLP
'Securing sufficient data to enable automatic sign language translation modeling is challenging The data insufficiency issue exists in both video and text modalities; however fewer studies have been performed on text data augmentation compared to video data In this study we present three methods of augmenting sign language text modality data comprising 3052 Gloss-level Korean Sign Language (GKSL) and Word-level Korean Language (WKL) sentence pairs Using each of the three methods the following number of sentence pairs were created: blank replacement 10654 sentence paraphrasing 1494 and synonym replacement 899 Translation experiment results using the augmented data showed that when translating from GKSL to WKL and from WKL to GKSL Bi-Lingual Evaluation Understudy (BLEU) scores improved by 0204 and 0170 respectively compared to when only the original data was used The three contributions of this study are as follows First we demonstrated that three different augmentation techniques used in existing Natural Language Processing (NLP) can be applied to sign language Second we propose an automatic data augmentation method which generates quality data by utilizing the Korean sign language gloss dictionary Lastly we publish the Gloss-level Korean Sign Language 13k dataset (GKSL13k) which has verified data quality through expert reviews',NLP
'Grammars might be used for various other aspects than just to represent a language Grammar inference is a large field which main goal is the construction of grammars from various sources Written text might be analysed indirectly with the use of such inferred grammars Grammars acquired from processed text might grow into large structures as the inference process could he continuous We present a method to decompose and store grammars into a non-redundant set of lambda calculus supercombinators Grammars decomposition is based on their structure and each distinct element is stored only once in such a structure We present a method that can create such a set from any context-free grammar To prove this and to show the possible applications in the field of natural language processing we present a case study performed on samples from two books Those samples are the entire Book of Genesis from The King James Bible and the first 24 chapters of War and peace by Tolstoy We obtain context-free grammars with the Sequitur algorithm and then we process them with our method The results show significant decline in the number of grammar elements in all cases',NLP
'Ellipsis is a cross-linguistic phenomenon which can be commonly seen in Chinese Although eliding some of the elements in the sentence that could be understood from the context makes no difference for human beings it is a great challenge for machine in the procedure of natural language understanding In order to promote ellipsis-related researches in Chinese language we propose an application-oriented definition of ellipsis specifically for researches in the realm of Chinese natural language processing At the same time we build and release a multi-domain dataset for sentence-level Chinese ellipsis resolution following the new definition we propose In addition we define a new task: sentence-level Chinese ellipsis resolution and model it with two subprocedures: 1) Elliptic position detection; 2) Ellipsis resolution We propose several baseline methods based on pre-trained language models as they have obtained state-of-the-art results on related tasks Besides it is also worth noticing that to our knowledge this is the first study that apply the extractive method for question answering to Chinese ellipsis resolution The results of the experiments show that it is possible for machine to understand ellipsis within our new definition',NLP
'The computer support of teaching and learning processes is still a large challenge for ICT Namely the actual level of human-computer interaction does not enable teachers or students to support the educational processes and automate them in a way that is natural for humans (eg solutions in natural language are missing) If they should be computerized the crucial problem is that these processes by the nature are characterized by a continuous knowledge flow between teachers and students Because knowledge is closely linked to mental processes of humans they are uncertain and characterized by use of unstructured data The state-of-the-art is prevailing by technology-driven approaches This is more centered on how to create a technological infrastructure or tools and how to have access to data on the internet rather than how to process the knowledge flow However from a teachers point of view computer support is more about processes how to computerize teaching in the classroom communication feedback how to write student works the creation of teaching texts and hundreds of other issues respectively Thus more sophisticated IT solutions and an innovative approach for knowledge abstraction and representation is needed for the automation of teaching processes This paper deals with how these issues were solved in natural language within a long-term participatory action research on technology-enhanced learning when teaching bachelor students The research outcomes - the technology-driven and educational-driven approach in-house educational software with a personalized supportive system and innovative approach to the abstraction of knowledge are briefly presented as examples from the practice The abstraction of the virtual knowledge unit is a part of the registered utility model 7340/2015 (the patent classification IPC G06F17/30 G06Q 10/10)',NLP
'In this article we introduce a knowledge-based approach to medical text understanding From an in-depth consideration of deep sentence and text understanding we distill basic requirements for an adequate knowledge representation framework These requirements are then matched with currently available medical ontologies (thesauri terminologies etc) A fundamental trade-off is recognized between large-scale conceptual coverage on the one hand and formal mechanisms for integrity preservation and conceptual expressiveness on the ether hand We discuss various shortcomings of the most wide-spread ontologies to capture medical knowledge in-the-large As a result we argue for the need of a formally sound and expressive model along the lines of KL-ONE-style terminological representation systems in the format of description logics These provide an adequate methodology for designing more sophisticated flexible medical ontologies serving the needs of deep knowledge applications which are by no means restricted to medical language processing (C) 1999 Elsevier Science BV All rights reserved',NLP
'The stigma related to mental health continues to be present in online newspapers where mental diseases are often used metaphorically to refer to entities or situations outside the clinical of mental health This project explores the implementation of Artificial Intelligence and Natural Language Processing techniques for the task of automatically classifying stigmatizing articles with references to the mental disorders of schizophrenia and psychosis This work is implemented in Portuguese online news articles collected from the Arquivopt repository a public repository of archived Portuguese web pages and can be adapted to other languages or similar problems Nine machine and deep learning algorithms were implemented most of them yielding results with a precision above 90% In addition the automatic detection of articles topics was also performed through topic modeling with the top2vec model which allowed concluding that the stigmatization of mental health occurs essentially in Economics and Politics related news The results confirm the existence of stigma in Portuguese newspapers (52% of the 978 articles collected) and the effectiveness of the use of Artificial Intelligence models to detect it Additionally a set of 978 articles collected and manually classified with the classes [stigmatizing  literal] is obtained',NLP
'The efficient indexing of large and sparse N-gram datasets is crucial in several applications in Information Retrieval Natural Language Processing and Machine Learning Because of the stringent efficiency requirements dealing with billions of N-grams poses the challenge of introducing a compressed representation that preserves the query processing speed In this paperwe study the problem of reducing the space required by the representation of such datasets maintaining the capability of looking up for a given N-gram within micro seconds For this purpose we describe compressed exact and lossless data structures that achieve at the same time high space reductions and no time degradation with respect to state-of-the-art software packages In particular we present a trie data structure in which each word following a context of fixed length k i e its preceding k words is encoded as an integer whose value is proportional to the number of words that follow such context Since the number of words following a given context is typically very small in natural languages we are able to lower the space of representation to compression levels that were never achieved before Despite the significant savings in space we show that our technique introduces a negligible penalty at query time',NLP
'Social media has affected peoples information sources Since most of the news on social media is not verified by a central authority it may contain fake news for various reasons such as advertising and propaganda Considering an average of 500 million tweets were posted daily on Twitter alone in the year of 2020 it is possible to control each share only with smart systems In this study we use Natural Language Processing methods to detect fake news for Turkish-language posts on certain topics on Twitter Furthermore we examine the follow/follower relations of the users who shared fake-real news on the same subjects through social network analysis methods and visualization tools Various supervised and unsupervised learning algorithms have been tested with different parameters The most successful F1 score of fake news detection was obtained with the support vector machines algorithm with 09 People who share fake/true news can help in the separation of subgroups in the social network created by people and their followers The results show that fake news propagation networks may show different characteristics in their own subject based on the follow/follower network',NLP
'Objective To investigate the performance of Dual-AI Deep Learning Platform in detecting unreported pulmonary nodules that are 6 mm or greater comprising computer-vision (CV) algorithm to detect pulmonary nodules with positive results filtered by natural language processing (NLP) analysis of the dictated report Methods Retrospective analysis of 5047 chest CT scans and corresponding reports Cases which were both CV algorithm positive (nodule >= 6 mm) and NLP negative (nodule not reported) were outputted for review by 2 chest radiologists Results The CV algorithm detected nodules that are 6 mm or greater in 1830 (363%) of 5047 cases Three hundred fifty-five (194%) were unreported by the radiologist as per NLP algorithm Expert review determined that 139 (392%) of 355 cases were true positives (28% of all cases) One hundred thirty (367%) of 355 cases were unnecessary alerts-vague language in the report confounded the NLP algorithm Eighty-six (242%) of 355 cases were false positives Conclusions Dual-AI platform detected actionable unreported nodules in 28% of chest CT scans yet minimized intrusion to radiologists workflow by avoiding alerts for most already-reported nodules',NLP
'Planned Preventive Maintenance (PPM) and Unplanned Maintenance (UPM) are the most common types of facility maintenance This paper analyzes current trends and status of Facility Management (FM) practice at higher education institutions by proposing a systematic data-driven methodology using Natural Language Process (NLP) approaches statistical analysis risk-profile analysis and outlier analysis This study utilizes a descriptive database entitled Facility Management Unified Classification Database (FMUCD) to conduct the systematic data-driven analyses The 5-year data from 2015 to 2019 was collected from eight universities in North America A preprocessing step included but was not limited to identifying common data attributes cleaning noisy data and removing unnecessary data The outcomes of this study can facilitate the decision-making process by providing an understanding of various aspects of educational facility management trends and risks The methodology developed gives decision makers of higher education institutions including facility managers and institution administrators effective strategies to establish long-term budgetary goals which will lead to the enhancement of the asset value of the institutions',NLP
'This paper presents a framework for the semiautomated development of textual requirements for the building construction industry a domain in which quality of textual requirements has a direct bearing on the avoidance of unnecessary project losses and failures The proposed framework is novel in three respects First it employs a combination of natural language processing (NLP) and template matching to restrict the range of ways in which textual requirements can be stated Second knowledge-based models work hand-in-hand with the NLP and templates to help designers fill in the details of a model that might be missing Third we envision the proposed approach being part of a multidomain semantic modeling and reasoning framework that includes the building construction domain and its connections to processes for project management and governance To demonstrate the potential of the proposed approach to the practical development and construction of building systems we developed a prototype software tool-the framework for linking ontology objects and textual requirements (FLOOR)-and procedures for the validation of individual textual requirements and groups of textual requirements related to construction tasks',NLP
'Threat modeling is a process by which security designers and researchers analyze the security of a system against known threats and vulnerabilities There is a myriad of threat intelligence and vulnerability databases that security experts use to make important day-to-day decisions Security experts and incident responders require the right set of skills and tools to recognize attack consequences and convey them to various stakeholders In this paper we used natural language processing (NLP) and deep learning to analyze text descriptions of cyberattacks and predict their consequences This can be useful to quickly analyze new attacks discovered in the wild help security practitioners take requisite actions and convey attack consequences to stakeholders in a simple way In this work we predicted the multilabels (availability access control confidentiality integrity and other) corresponding to each text description in MITREs CWE dataset We compared the performance of various CNN and LSTM deep neural networks in predicting these labels The results indicate that it is possible to predict multilabels using a LSTM deep neural network with multiple output layers equal to the number of labels LSTM performance was better when compared to CNN models',NLP
'Information technology outsourcing (ITO) has grown significantly in recent decades and is now over a USD trillion-dollar industry Service provider organisations are striving to improve the efficiencies of their service deliveries Natural language processing (NLP) provides an opportunity to bring efficiencies through automation in understanding and processing information Since information security risk management (ISRM) in ITO is a growing concern of both client and service provider organisations they are adopting to improve ISRM in ITO using NLP This paper explores those ISRM improvement scenarios It also investigates the information security risks (ISRs) that result from the use of NLP in ITO and proposes strategies to manage those ISRs To gain insights into the problem a qualitative research approach is followed using the case study method Six semi-structured interviews were conducted from participants in three organisations in the ICT industry engaged in an ITO relationship To the best of our knowledge it is the first study to investigate the use of NLP for enhancing ISRM in ITO',NLP
'The objective of this study was to develop a medical language processing (MLP) system which consisted of MedLEE and a set of inference rules to identify 19 Charlson comorbidities from discharge summaries and chest x-ray reports We used 233 cases to learn the patterns that were indicative of comorbidities for developing the inference rules We then used an independent data set of 3662 pneumonia patients to identify comorbidities by MLP compared with administrative data (ICD-9 codes) A stratified random sample of 190 records from disagreement cases was manually reviewed The sensitivity specificity and accuracy for the MLP system/ICD-9 codes in this testing set were 084/016 070/030 and 077/023 respectively Thirteen of the 19 comorbidities studied were underreported in the administrative data The kappa values ranged from 019 for peptic ulcer to 070 for lymphoma We conclude that comorbidities derived from natural language processing of medical records can improve ICD-9-based approaches',NLP
'Semantic role labeling (SRL) is one of the important tasks in natural language processing Current end-to-end SRL compared with traditional pipeline SRL has achieved competitive performance via graph-based neural models However these are all first-order methods where decisions for detecting predicate-argument pairs are made in isolation with local short-term features thus being error-prone Besides they may potentially suffer from long-distance dependency and perform poorly on the data-scarce scenario In this paper we explore a second-order end-to-end SRL model that considers simultaneously two pairs of predicate-argument when making scoring We next propose a structural refinement mechanism to further model higher-order interactions at a global scope After iterations of refinement the underlying token representations are refined to be more informative Experimental results show that our model significantly outperforms the state-of-the-art baselines on multiple benchmarks (ie CoNLL08 CoNLL09 and CoNLL12) Further analysis demonstrates that our framework is able to learn latent SRL constraints being more robust on data-scarce settings meanwhile relieving the long-range dependency issue',NLP
'Sentiment analysis tools often rely on counts of sentiment-carrying words ignoring structural aspects of content Natural Language Processing has been fruitfully exploited in text mining but advanced discourse processing is still nonpervasive for mining opinions Some studies however extracted opinions based on the discursive role of text segments The merits of such computationally intensive analyses have thus far been assessed in very specific small-scale scenarios In this paper we investigate the usefulness of Rhetorical Structure Theory in various sentiment analysis tasks on different types of information sources First we demonstrate how to perform a large-scale ranking of individual blog posts in terms of their overall polarity by exploiting the rhetorical structure of a few key evaluative sentences In order to further validate our findings we additionally explore the potential of Rhetorical Structure Theory in sentence-level polarity classification of news and product reviews Our most valuable polarity classification features turn out to capture the way in which polar terms are used rather than the sentiment-carrying words per se (C) 2014 Elsevier BV All rights reserved',NLP
'Sentiment analysis has become a flourishing field of text mining and natural language processing Sentiment analysis aims to determine whether the text is written to express positive negative or neutral emotions about a certain domain Most sentiment analysis researchers focus on English texts with very limited resources available for other complex languages such as Arabic In this study the target was to develop an initial model that performs satisfactorily and measures Arabic Twitter sentiment by using machine learning approach Naive Bayes and Decision Tree for classification algorithms The datasets used contains more than 2000 Arabic tweets collected from Twitter We performed several experiments to check the performance of the two algorithms classifiers using different combinations of text-processing functions We found that available facilities for Arabic text processing need to be made from scratch or improved to develop accurate classifiers The small functionalities developed by us in a Python language environment helped improve the results and proved that sentiment analysis in the Arabic domain needs lot of work on the lexicon side',NLP
'It is important to learn directly from original texts in natural language processing (NLP) Many deep learning (DP) models needing a large number of manually annotated data are not effective in deriving much information from corpora with few annotated labels Existing methods using unlabeled language information to provide valuable messages consume considerable time and cost Our provided sentence representation based on quantum computation (called Model I) needs no prior knowledge except word2vec To reduce some semantic noise caused by the tensor product on the entangled words vector two improved models (called Model II and Model III) are proposed to reduce the dimensions of the sentence embedding stimulated by Model I The provided models are evaluated in the STS tasks of 2012 2014 2015 and 2016 for a total of 21 corpora Experimental results show that using quantum entanglement and dimensionality reduction in sentence embedding yields state-of-the-art performances on semantic relations and syntactic structures Compared to the Pearson correlation coefficient (Pcc) and mean squared error (MSE) the results of 16 out of 16 corpora are better than the results of the comparative methods',NLP
'Using statistical approaches beside the traditional methods of natural language processing could significantly improve both the quality and performance of several natural language processing (NLP) tasks The effective usage of these approaches is subject to the availability of the informative accurate and detailed corpora on which the learners are trained This article introduces a bootstrapping method for developing annotated corpora based on a complex and rich linguistically motivated elementary structure called supertag To this end a hybrid method for supertagging is proposed that combines both of the generative and discriminative methods of supertagging The method was applied on a subset of Wall Street Journal (WSJ) in order to annotate its sentences with a set of linguistically motivated elementary structures of the English XTAG grammar that is using a lexicalised tree-adjoining grammar formalism The empirical results confirm that the bootstrapping method provides a satisfactory way for annotating the English sentences with the mentioned structures The experiments show that the method could automatically annotate about 20% of WSJ with the accuracy of F-measure about 80% of which is particularly 12% higher than the F-measure of the XTAG Treebank automatically generated from the approach proposed by Basirat and Faili [(2013) Bridge the gap between statistical and hand-crafted grammars Computer Speech and Language 27 1085-1104]',NLP
'We present Korbit a large-scale open-domain mixed-interface dialogue-based intelligent tutoring system (ITS) Korbit uses machine learning natural language processing and reinforcement learning to provide interactive personalized learning online Korbit has been designed to easily scale to thousands of subjects by automating standardizing and simplifying the content creation process Unlike other ITS a teacher can develop new learning modules for Korbit in a matter of hours To facilitate learning across a wide range of STEM subjects Korbit uses a mixed-interface which includes videos interactive dialogue-based exercises question-answering conceptual diagrams mathematical exercises and gamification elements Korbit has been built to scale to millions of students by utilizing a state-of-the-art cloud-based micro-service architecture Korbit launched its first course in 2019 and has over 7 000 students have enrolled Although Korbit was designed to be open-domain and highly scalable A/B testing experiments with real-world students demonstrate that both student learning outcomes and student motivation are substantially improved compared to typical online courses',NLP
'Electronic Medical Record (EMR) systems store patients medical information in either structured or unstructured free-text format such as clinical reports Pathology notes are a type of clinical reports that may store cancer related information such as diagnoses and description of tissue sample Data in clinical documents can provide up to 20% of knowledge in addition to structured data stored in discrete fields The process of extracting information from documents can be time-consuming and non-trivial We evaluated several natural language processing (NLP) open source tools to extract terms of interest from pathology documents and to incorporate with data already stored in the institutional data warehouse (EDW) Many of the evaluated NLP software tools provide various features but none suites our immediate need of extracting key pathology terms This paper discusses our in-house developed framework to identify and extract pathology data points from pathology documents curate and load in the EDW The performance of the proposed model was evaluated and extracted terms were validated with data stored in the institutional electronic medical record system',NLP
'Nowadays the automatic processing of digitalized documents is crucial to cope with the increasing amount of information available This issue is addressed from the natural language processing (NLP) research field One of the tasks required for many NLP applications is temporal information processing It involves the automatic extraction and interpretation of temporal expressions events and their relations Specifically the identification and the categorization of temporal relations are the most complex subtasks yet to solve judging from the results reported in the latest international evaluation exercise Temporal relation identification has been addressed by very few approaches and the current categorization approaches are still not a definitive solution This paper presents a system that approaches temporal relation identification and categorization The former is approached with a knowledge-driven strategy and the later with data-driven strategy based on different machine-learning techniques Our proposal has been empirically evaluated over the currently available English data sets annotated with temporal information (TimeBank and AQUAINT) in a 10-fold cross-validated experiment The results obtained support that the presented approach achieves a high performance It improves the baseline F1 by 46% and outperforms the state of the art (c) 2012 Wiley Periodicals Inc',NLP
'Dialog System is an important research area in natural language processing This paper presents a research work on the Multi-Talk dialog system which is supported by a large scaled knowledge base called Pangu Our experience in the design improvement xible and natural dialog modularized knowledge evaluation of the Multi-Talk system are introduced is presented in this paper as well as the experimental results To realize flebase formalized knowledge representation various efficient reasoners and assessment of system were used Finally this paper ends with some detailed conclusions and looks into the future research directions',NLP
'Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages Unfortunately it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race Because most studies have focused on MLMs in English the bias of MLMs in other languages has rarely been investigated Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators Moreover the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (eg He/She is a nurse) We propose Multilingual Bias Evaluation (MBE) score to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias',NLP
'Natural language processing has witnessed remarkable progress with the advent of deep learning techniques Text summarization along other tasks like text translation and sentiment analysis used deep neural network models to enhance results The new methods of text summarization are subject to a sequence-to-sequence framework of encoder-decoder model which is composed of neural networks trained jointly on both input and output Deep neural networks take advantage of big datasets to improve their results These networks are supported by the attention mechanism which can deal with long texts more efficiently by identifying focus points in the text They are also supported by the copy mechanism that allows the model to copy words from the source to the summary directly In this research we are re-implementing the basic summarization model that applies the sequence-to-sequence framework on the Arabic language which has not witnessed the employment of this model in the text summarization before Initially we build an Arabic data set of summarized article headlines This data set consists of approximately 300 thousand entries each consisting of an article introduction and the headline corresponding to this introduction We then apply baseline summarization models to the previous data set and compare the results using the ROUGE scale',NLP
'Aspect-based sentiment analysis (ABSA) is a powerful way of predicting the sentiment polarity of text in natural language processing However understanding human emotions and reasoning from text like a human continues to be a challenge In this paper we propose a model named Attention-based Sentiment Reasoner (AS-Reasoner) to alleviate the problem of how to capture precise sentiment expressions in ABSA for reasoning AS-Reasoner assigns importance degrees to different words in a sentence to capture key sentiment expressions towards a specific aspect and transfers them into a sentiment sentence representation for reasoning in the next layer To obtain appropriate importance degree values for different words in a sentence two attention mechanisms we designed: intra attention and global attention Specifically intra attention captures the sentiment similarity between any two words in a sentence to compute weights and global attention computes weights by a global perspective Experiments on all four English and four Chinese datasets show that the proposed model achieves state-of-the-art accuracy and macro-F1 results for aspect term level sentiment analysis and obtains the best accuracy for aspect category level sentiment analysis The experimental results also indicate that AS-Reasoner is language-independent',NLP
'Since BERT appeared Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks Recently some works geared towards pre-training specially-crafted models for particular domains such as scientific papers medical documents user-generated texts among others These domain-specific models have been shown to improve performance significantly in most tasks; however for languages other than English such models are not widely available In this work we present RoBERTuito a pre-trained language model for user-generated text in Spanish trained on over 500 million tweets Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish In addition to this our model has some cross-lingual abilities achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks To facilitate further research we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it',NLP
'Emotions play an important role in human intelligence and behaviour and are a major vehicle for communication Therefore the integration of emotions in computational models can improve the human-computer interaction systems In this paper we present a study of different machine learning approaches to automatically recognise emotions in messages written in Spanish on social media Although the computational treatment of emotion is more difficult than other sentiment analysis tasks the baseline of some machine learning algorithms achieve an acceptable accuracy showing that it is possible to tackle the problem using some basic natural language processing techniques In this study we have experimented with the integration of knowledge from different affective lexical resources We conclude that the incorporation of lexical affective features leads to improvement over most baseline figures with significant improvement Indeed we observe that the use of resources generated particularly for emotion recognition in other languages than English is a promising approach to enhance basic machine learning systems Particularly we used a Spanish lexical resource and we notice that it always improves the results In the best case it improves 615% of the results obtained using the Naive Bayes classifier (C) 2019 Elsevier BV All rights reserved',NLP
'Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP) However despite their widespread application little is known about the structure and properties of these spaces To gain insights into the relationship between words the NLP community has begun to adapt high-dimensional visualization techniques In particular researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (eg word analogies) respectively Unfortunately these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings Here we introduce new embedding techniques for visualizing semantic and syntactic analogies and the corresponding tests to determine whether the resulting views capture salient structures Additionally we introduce two novel views for a comprehensive study of analogy relationships Finally we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation Combined the different views address a number of domain-specific tasks difficult to solve with existing tools',NLP
'Offensive content such as verbal attacks demeaning comments or hate speech has become widespread on social media Automatic detection of this content is considered an important and challenging task Although several research works have been proposed to address this challenge for high-resource languages research on detecting offensive content in Dialectal Arabic (DA) remains under-explored Recently the detection of offensive language in DA has gained increasing interest among researchers in Natural Language Processing (NLP) However only a limited number of annotated datasets have been introduced for single or multiple coarse-grained dialects In this paper we introduce Offensive Moroccan Comments Dataset (OMCD) the first dataset for offensive language detection for the Moroccan dialect First we present the data collection steps the statistical analysis and the annotation guidelines of the introduced dataset Then we evaluate several state-of-the-art Machine Learning (ML) and Deep Learning (DL) based models on the OMCD dataset Finally we highlight the impact of emojis on the evaluated models for offensive language detection',NLP
'Correctly identifying multiword expressions (MWEs) is an important task for most natural language processing systems since their misidentification can result in ambiguity and misunderstanding of the underlying text In this work we evaluate the performance of the mBERT model for MWE identification in a multilingual context by training it on all 14 languages available in version 12 of the PARSEME corpus We also incorporate lateral inhibition and language adversarial training into our methodology to create language-independent embeddings and improve its capabilities in identifying multiword expressions The evaluation of our models shows that the approach employed in this work achieves better results compared to the best system of the PARSEME 12 competition MTLB-STRUCT on 11 out of 14 languages for global MWE identification and on 12 out of 14 languages for unseen MWE identification Additionally averaged across all languages our best approach outperforms the MTLB-STRUCT system by 123% on global MWE identification and by 473% on unseen global MWE identification',NLP
'Recent innovations in self-supervised representation learning have led to remarkable advances in natural language processing That said in the speech processing domain self-supervised representation learning-based systems are not yet considered state-of-the-art We propose leveraging recent advances in self-supervised-based speech processing to create a common speech analysis engine Such an engine should be able to handle multiple speech processing tasks using a single architecture to obtain state-of-the-art accuracy The engine must also enable support for new tasks with small training datasets Beyond that a common engine should be capable of supporting distributed training with client in-house private data We present the architecture for a common speech analysis engine based on the HuBERT self-supervised speech representation Based on experiments we report our results for language identification and emotion recognition on the standard evaluations NIST-LRE 07 and IEMOCAP Our results surpass the state-of-the-art performance reported so far on these tasks We also analyzed our engine on the emotion recognition task using reduced amounts of training data and show how to achieve improved results',NLP
'Information extraction from unstructured texts plays a vital role in the field of natural language processing Although there has been extensive research into each information extraction task (ie entity linking coreference resolution and relation extraction) data are not available for a continuous and coherent evaluation of all information extraction tasks in a comprehensive framework Given that each task is performed and evaluated with a different dataset analyzing the effect of the previous task on the next task with a single dataset throughout the information extraction process is impossible This paper aims to propose a Korean information extraction initiative point and promote research in this field by presenting crowdsourcing data collected for four information extraction tasks from the same corpus and the training and evaluation results for each task of a state-of-the-art model These machine learning data for Korean information extraction are the first of their kind and there are plans to continuously increase the data volume The test results will serve as an initiative result for each Korean information extraction task and are expected to serve as a comparison target for various studies on Korean information extraction using the data collected in this study',NLP
'Nowadays the number of huge companies and corporations has in their disposition various non-structured texts documents and other data but most of this data is still just text documents with different subject matters and content The work-flow organization on this data format is complicated because of their characteristics and requires modern tools for processing and analysis Possible problem solution is machine learning algorithms and natural language processing methods envolving with existing clustering and classification algorithms improvement For document classification we propose a proprietary approach based on the us-age of a semantic map as a feature reduction tool In this paper we are going to investigate the impact of this approach on the quality of classification of documents and describe its application to the implementation of the document categorization (C) 2018 The Authors Published by Elsevier BV This is an open access article under the CC BY-NC-ND license (https://creativecommonsorg/licenses/by-nc-nd/40/) Peer-review under responsibility of the scientific committee of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures',NLP
'A robot as a coworker or a cohabitant is becoming mainstream day-by-day with the development of low-cost sophisticated hardware However an accompanying software stack that can aid the usability of the robotic hardware remains the bottleneck of the process especially if the robot is not dedicated to a single job Programming a multi-purpose robot requires an on the fly mission scheduling capability that involves task identification and plan generation The problem dimension increases if the robot accepts tasks from a human in natural language Though recent advances in NLP and planner development can solve a variety of complex problems their amalgamation for a dynamic robotic task handler is used in a limited scope Specifically the problem of formulating a planning problem from natural language instructions is not studied in details In this work we provide a non-trivial method to combine an NLP engine and a planner such that a robot can successfully identify tasks and all the relevant parameters and generate an accurate plan for the task Additionally some mechanism is required to resolve the ambiguity or missing pieces of information in natural language instruction Thus we also develop a dialogue strategy that aims to gather additional information with minimal question-answer iterations and only when it is necessary This work makes a significant stride towards enabling a human-like task understanding capability in a robot',NLP
'With the development of social network services various phenomena can be shared easily and rapidly through human natural language including not only natural but also social cultural phenomena Consequently analyses of social media have appreciated in value for understanding human behaviors to grasp public interests or sentiments as both the medium and outcome of human experiences From the state of the art psychology and neuroscience human behaviors regarding both physical and linguistic aspects are mostly dependent on sensory perceptions under the realm of the subconscious Even though sensation is the most fundamental element to understand human behaviors the rack of background resources make it hard to study the social sensation comparing with the sentimental or opinion mining This paper focuses on building sensation knowledges to obtain useful human perceptual experiences in natural language expressions as a requisite for the social sensation analysis We try to approach the constructing lexicons as a sensation knowledge from two viewpoints such as a deep learning and lexicon based methods Then we classify social media text based on the lexicons with considering a part of speech as well as semantic meanings of each word Finally we identify which knowledge has a good performance to distinguish sensation expressions from social media data in terms of accuracy and and F-score',NLP
'Corpus tagging is an important basic work of named entity recognition and relation extraction based on deep learning and it is also a core task in natural language processing Aiming at the problems of error propagation information loss and entity redundancy in the pipeline method this paper proposes a text annotation method E+R+BIES for joint extraction of domain entity relations based on the characteristics of specific domain corpus The entity relationship extraction problem is transformed into a sequence labeling problem The entity and its relationship information are included in a labeling process the triples are directly modeled and the BERT-BiLSTM+CRF end-to-end model is used for label prediction Experiments show that the joint extraction effect of domain entity relations based on this annotation method and the BERT-BiLSTM+CRF model is better than the general frontier model in which the accuracy rate is increased by 5 to 7 percentage points the recall rate is increased by 9 to 11 percentage points and the Fl value is increased by 8 -9 percentage points reaching 9060%',NLP
'Purpose Clear and accurate genetic information should be available to health-care consumers at an individualized level of comprehension The objective of this study is to evaluate the complexity of common online resources and to simplify text content using automated text processing tools Methods We extracted all text from Genetics Home Reference and MedlinePlus in bulk and analyzed content using natural language processing We applied custom tools to improve the readability and compared readability before and after text optimization Results Commonly used educational materials were more complex than the recommended reading level for the general public Genetic health information entries from Genetics Home Reference (n = 1279) were written at a median 130 grade level MedlinePlus entries which are not exclusively genetic (n = 1030) had a median grade level of 77 When we optimized text for the 59 actionable conditions by prioritizing medical details using a standard structure the average reading grade level improved Conclusion Factors that increase complexity are long sentences and difficult words Future strategies to reduce complexity include prioritizing relevant details and using more illustrations Simplifying and providing standardized online health resources would benefit diverse consumers and promote inclusivity',NLP
'Bug-tracking systems are widely used by software developers to manage bug reports Since it is time-consuming and costly to fix all the bugs developers usually pay more attention to the bugs with higher impact such as security bugs (ie vulnerabilities) which can be exploited by malicious users to launch attacks and cause great damages However manually identifying security bug reports from millions of reports in bug-tracking systems is difficult and error-prone Furthermore existing automated identification approaches to security bug reports often incur many false negatives causing a hidden danger to the computer system To address this important problem we present an automatic security bug reports identification model via multitype features analysis dubbed Security Bug Report Identifier (SBRer) Specifically we make use of multiple kinds of information contained in a bug report including meta features and textual features to automatically identify the security bug reports via natural language processing and machine learning techniques The experimental results show that SBRer with imbalanced data processing can successfully identify the security bug reports with a much higher precision of 994% and recall of 799% compared to existing work',NLP
'Most languages of the world are taken to result from a combination of a vertical transmission process from older to younger generations of speakers or signers and (mostly) gradual changes that accumulate over time In contrast creole languages emerge within a few generations out of highly multilingual societies in situations where no common first language is available for communication (as for instance in plantations related to the Atlantic slave trade) Strikingly creoles share a number of linguistic features (the creole profile) which is at odds with the striking linguistic diversity displayed by non-creole languages1-4 These common features have been explained as reflecting a hardwired default state of the possible grammars that can be learned by humans(1) as straightforward solutions to cope with the pressure for efficient and successful communication(5) or as the byproduct of an impoverished transmission process(6) Despite their differences these proposals agree that creoles emerge from a very limited and basic communication system (a pidgin) that only later in time develops the characteristics of a natural language potentially by innovating linguistic structure Here we analyse 48 creole languages and 111 non-creole languages from all continents and conclude that the similarities (and differences) between creoles can be explained by genealogical and contact processes(78) as with non-creole languages with the difference that creoles have more than one language in their ancestry While a creole profile can be detected statistically this stems from an over-representation of Western European and West African languages in their context of emergence Our findings call into question the existence of a pidgin stage in creole development and of creole-specific innovations In general given their extreme conditions of emergence they lend support to the idea that language learning and transmission are remarkably resilient processes',NLP
'Software often produces biased outputs In particular machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs Such unfair program behavior can be caused by societal bias In the last few years Amazon Microsoft and Google have provided software services that produce unfair outputs mostly due to societal bias (eg gender or race) In such events developers are saddled with the task of conducting fairness testing Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases We propose a grammar-based fairness testing approach (called Astraea) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems Using probabilistic grammars Astraea also provides fault diagnosis by isolating the cause of observed software bias Astraeas diagnoses facilitate the improvement of ML fairness Astraea was evaluated on 18 software systems that provide three major natural language processing (NLP) services In our evaluation Astraea generated fairness violations at a rate of about 18% Astraea generated over 573K discriminatory test cases and found over 102K fairness violations Furthermore Astraea improves software fairness by about 76% via model-retraining on average',NLP
'Due to limited time budget or resources a team is prone to introduce code that does not follow the best software development practices This code that introduces instability in the software projects is known as Technical Debt (TD) Often TD intentionally manifests in source code which is known as Self-Admitted Technical Debt (SATD) This paper presents DebtHunter a natural language processing (NLP)- and machine learning (ML)- based approach for identifying and classifying SATD in source code comments The proposed classification approach combines two classification phases for differentiating between the multiple debt types Evaluations over 10 open source systems containing more than 259k comments showed that the approach was able to improve the performance of others in the literature The presented approach is supported by a tool that can help developers to effectively manage SATD The tool complements the analysis over Java source code by allowing developers to also examine the associated issue tracker DebtHunter can be used in a continuous evolution environment to monitor the development process and make developers aware of how and where SATD is introduced thus helping them to manage and resolve it',NLP
'We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text We describe and evaluate our system the clinical Text Analysis and Knowledge Extraction System (cTAKES) released open-source at http://wwwohnlporg The cTAKES builds on existing open-source technologies the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit Its components specifically trained for the clinical domain create rich linguistic and semantic annotations Performance of individual components: sentence boundary detector accuracy=0949; tokenizer accuracy=0949; part-of-speech tagger accuracy=0936; shallow parser F-score=0924; named entity recognizer and system-level evaluation F-score=0715 for exact and 0824 for overlapping spans and accuracy for concept mapping negation and status attributes for exact and overlapping spans of 0957 0943 0859 and 0580 0939 and 0839 respectively Overall performance is discussed against five applications The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text',NLP
'Most of existing studies on parsing natural language (NL) for constructing structured query language (SQL) do not consider the complex structure of database schema and the gap between NL and SQL query In this paper we propose a schema-aware neural network with decomposing architecture namely HSRNet which aims to address the complex and cross-domain Text-to-SQL generation task The HSRNet models the relationship of the database schema with a hierarchical schema graph and employs a graph network to encode the information into sentence representation Instead of end-to-end generation the HSRNet decomposes the generation process into three phases Given an input question and schema we first choose the column candidates and generate the sketch grammar of the SQL query Then a detail completion module fills the details based on the column candidates and the corresponding sketch We demonstrate the effectiveness of our hierarchical schema representation by incorporating the information into different baselines We further show that the decomposing architecture significantly improves the performance of our model Evaluation of Spider benchmark shows that the hierarchical schema representation and decomposing architecture improves our parser result by 145% and 43% respectively',NLP
'One approach to the transcription of written text into sounds (phonetization) is to use a set of well-defined language-dependent rules which are in most situations augmented by a dictionary of exceptional words that constitute their on rules The process of transcribing into sounds starts by pre-processing the text into lexical items to which the rules are applicable The rules can be segregated into phonemic and phonetic rules Phonemic rules operate on the graphemes to convert them into phonemes Phonetic rules operate onto the phonemes and convert them into phones or actual sounds Converting from written text into actual sounds and developing a comprehensive set of rules for any language is marked by several problems that have their origins in the relative lack of correspondence between the spelling of the lexical items and their sound contents For standard Arabic (SA) these problems are not as severe as they are for English or French but they do exist This paper presents a detailed investigation into all aspects of the phonetization of SA for the purpose of developing a comprehensive system for letter-to-sound conversion for the standard Arabic language and assessing the quality of the letter-to-sound transcription system In particular the paper deals with the following issues: (1) investigation of the spelling and other problems of SA writing system and their impact on converting graphemes into phonemes (2) The development of a comprehensive set of rules to be used in the transcription of graphemes into phonemes and (3) investigations of the important contextual phonetic variations of SA phonemes so as to determine viable variants (phones) of the phonemes (4) The development of a set of rules to be used in the transcription of phonemes into phones (5) The formulation of the rules for grapheme to phoneme and the phoneme to phone transcriptions into algorithms that lend themselves to computer-based processing (6) An objective evaluation of the performance of the process of converting SA text into actual sounds Phonetization of text is an important component in any natural language processing (NLP) domain that envisages text-to-speech (TTS) conversion and has applications beyond speech synthesis such as acoustic modeling for speech recognition and other natural language processing applications (C) 2003 Elsevier Ltd All rights reserved',NLP
'We propose a novel deliberation-based approach to end-to-end (E2E) spoken language understanding (SLU) where a streaming automatic speech recognition (ASR) model produces the first-pass hypothesis and a second-pass natural language understanding (NLU) component generates the semantic parse by conditioning on both ASRs text and audio embeddings By formulating E2E SLU as a generalized decoder our system is able to support complex compositional semantic structures Furthermore the sharing of parameters between ASR and NLU makes the system especially suitable for resource-constrained (on-device) environments; our proposed approach consistently outperforms strong pipeline NLU baselines by 060% to 065% on the spoken version of the TOPv2 dataset (STOP) We demonstrate that the fusion of text and audio features coupled with the systems ability to rewrite the first-pass hypothesis makes our approach more robust to ASR errors Finally we show that our approach can significantly reduce the degradation when moving from natural speech to synthetic speech training but more work is required to make text-to-speech (TTS) a viable solution for scaling up E2E SLU',NLP
'Relation classification is a significant task within the field of natural language processing Its objective is to extract and identify relations between two entities in a given text Within the scope of this paper we construct an artificial dataset (CS13K) for relation classification in the realm of cybersecurity and propose two models for processing such tasks For any sentence containing two target entities we first locate the entities and fine-tune the pre-trained BERT model Next we utilize graph attention networks to iteratively update word nodes and relation nodes A new relation classification model is constructed by concatenating the updated vectors of word nodes and relation nodes Our proposed model achieved exceptional performance on the SemEval-2010 task 8 dataset surpassing previous approaches with a remarkable F1 value of 923% Additionally we propose the integration of a ranking-based voting mechanism into the existing model Our best results are an F1 value of 925% on the SemEval-2010 task 8 dataset and a value 946% on the CS13K dataset These findings highlight the effectiveness of our proposed models in tackling relation classification tasks',NLP
'With the successful development and rapid advancement of social networking technology people tend to exchange and share information via online social networks such as Facebook and LINE Massive amounts of information are aggregated promptly and circulated quickly among people However with the enormous volume of human-interactions various types of swindles via online social networks have been launched in recent years Effectively detecting fraudulent activities on social networks has taken on increased importance and is a topic of ongoing interest In this paper we develop a fraud analysis and detection system based on real-time messaging communications which constitute one of the most common human-interacted services of online social networks An integrated platform consisting of various text-mining techniques such as natural language processing matrix processing and content analysis via a latent semantic model is proposed In the system implementation we first collect a series of fraud events all of which happened in Taiwan to construct analysis modules for detecting such fraud events An Android-based application is then built for alert notification when dubious logs and fraud events happen',NLP
'When we read printed text we are continuously predicting upcoming words to integrate information and guide future eye movements Thus the Predictability of a given word has become one of the most important variables when explaining human behaviour and information processing during reading In parallel the Natural Language Processing (NLP) field evolved by developing a wide variety of applications Here we show that using different word embeddings techniques (like Latent Semantic Analysis Word2Vec and FastText) and N-gram-based language models we were able to estimate how humans predict words (cloze-task Predictability) and how to better understand eye movements in long Spanish texts Both types of models partially captured aspects of predictability On the one hand our N-gram model performed well when added as a replacement for the cloze-task Predictability of the fixated word On the other hand word embeddings were useful to mimic Predictability of the following word Our study joins efforts from neurolinguistic and NLP fields to understand human information processing during reading to potentially improve NLP algorithms',NLP
'Nowadays Artificial Intelligent (AI) technologies are applied widely in many different areas to assist knowledge gaining and decision-making tasks Especially health information system can get most benefits from the AI advantages In particular symptoms based disease prediction research and production became increasingly popular in the healthcare sector recently Various researchers and organizations have turned their interest in using modern computational techniques to analyze and develop new approaches that can efficiently predict diseases with reasonable accuracy In this paper we propose a framework to evaluate the efficiency of applying both Machine Learning (ML) and Nature Language Processing (NLP) technologies for disease prediction system As an example we scraped a disease-symptom dataset with NLP features from one of the UK most trustable National Health Service (NHS) website In addition we will exam our data in depth having symptom frequency similarity and clustering analysis As result we can see that the prediction can have a very positive efficient rate but still open issues need to be addressed',NLP
'Many natural language processing applications rely on the availability of domain-specific terminologies containing synonyms To that end semi-automatic methods for extracting additional synonyms of a given concept from corpora are useful especially in low-resource domains and noisy genres such as clinical text where nonstandard language use and misspellings are prevalent In this study prototype embeddings based on seed words were used to create representations for (i) specific urinary tract infection (UTI) symptoms and (ii) UTI symptoms in general Four word embedding methods and two phrase detection methods were evaluated using clinical data from Karolinska University Hospital It is shown that prototype embeddings can effectively capture semantic information related to UTI symptoms Using prototype embeddings for specific UTI symptoms led to the extraction of more symptom terms compared to using prototype embeddings for UTI symptoms in general Overall 142 additional UTI symptom terms were identified yielding a more than 100% increment compared to the initial seed set The mean average precision across all UTI symptoms was 051 and as high as 086 for one specific UTI symptom This study provides an effective and cost-effective solution to terminology expansion with small amounts of labeled data',NLP
'Diacritic restoration is a challenging problem in natural language processing (NLP) With diacritic restoration one can text faster and easier Diacritic restoration is also helpful in making use of diacritic-missing texts which are normally discarded in many NLP applications This paper deals with the diacritic restoration problem for Vietnamese text Three state-of-the-art deep learning models including Gated Recurrent Unit Bidirectional Long-short Term Memory and Bidirectional Gated Recurrent Unit have been examined for the problem and the last one turned out to be the best among them Besides deep learning models it was found in this paper that word tokenization which is the final pre-processing step applied on the data before feeding it to deep learning models also have influences on the final accuracy Between two examined word tokenization methods: morpheme-based tokenization and phrase-based tokenization the former yield better results regardless of the applied deep learning models The experimental results show that the combination of morpheme-based tokenization and Bidirectional-GRU achieve the best performance of diacritic restoration with the Bleu-score of 8806%',NLP
'In response to the COVID-19 pandemic the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset ( CORD-19) containing over 51000 scholarly articles including over 40000 with full text about COVID-19 SARS-CoV-2 and related coronaviruses Medical professional including physicians frequently seek answers to specific questions to improve guidelines and decisions The huge resource of medical literature is important sources to generate new insights that can help medical communities to provide relevant knowledge and overall fight against the infectious disease There are ongoing attempts to develop intelligent systems to automatically extract relevant knowledge from many unstructured documents In this paper we propose an efficient question answering framework based on automatically analyzing thousands of articles to generate both long text answers (sections/ paragraphs) in response to the questions that are posed by medical communities In the process of developing the framework we explored natural language processing techniques like query expansion data preprocessing and vector space models early We show the initial results of an example query answering for the incubation period',NLP
'Causality analysis is one of the emerging topics of importance in fault diagnosis network reconstruction and in performance analysis of control systems Almost all current day literature in the engineering arena makes use of numerical data However many processes generate insightful textual data that is left unanalysed In this work we present a novel method for causal discovery from textual data using context and dependency information The proposed BiLSTM and Node2Vec architecture uses word-level embeddings linguistic features to gather contextual information and dependencyparse tree representations to gauge how entities in a sentence are related The extracted phrases in each sentence are labelled as causes effects or causal connectives Our work is evaluated on datasets such as SemEval BECAUSE 20 and CausalBank and compared against a baseline Conditional Random Field (CRF) based supervised BiLSTM model We demonstrate that the presence of both context and dependency information significantly improves performance and also extend our analysis to operator logs exhibiting its effectiveness in extracting causal events in process logs The work hopes to generate immense value for the technology envisaged in Industry 40',NLP
'We present a novel natural language query interface the AggChecker aimed at text summaries of relational data sets The tool focuses on natural language claims that translate into an SQL query and a claimed query result Similar in spirit to a spell checker the AggChecker marks up text passages that seem to be inconsistent with the actual data At the heart of the system is a probabilistic model that reasons about the input document in a holistic fashion Based on claim keywords and the document structure it maps each text claim to a probability distribution over associated query translations By efficiently executing tens to hundreds of thousands of candidate translations for a typical input document the system maps text claims to correctness probabilities This process becomes practical via a specialized processing back-end avoiding redundant work via query merging and result caching Verification is an interactive process in which users are shown tentative results enabling them to take corrective actions if necessary We tested our system on 53 publicly available articles containing 392 claims Our tool revealed erroneous claims in roughly a third of test cases Also AggChecker compares favorably against several automated and semi-automated fact checking baselines',NLP
'Bilingualism is a natural laboratory for studying whether the brains structural connectome is influenced by different aspects of language experience However evidence on how distinct components of bilingual experience may contribute to structural brain adaptations is mixed The lack of consistency however may depend at least in part on methodological choices in data acquisition and processing Herein we adopted the Network Neuroscience framework to investigate how individual differences in second language (L2) exposure proficiency and age of acquisition (AoA) - measured as continuous between-subject variables - relate to whole-brain structural organization We observed that L2 exposure modulated the connectivity of two networks of regions subserving language comprehension and production L2 proficiency was associated with enhanced connectivity within a rostro-caudal network which supports language selection and word learning Moreover L2 AoA and exposure affected inter-hemispheric communication between control-related regions These findings expand mechanistic knowledge about particular environmental factors associated with specific variation in brain structure',NLP
'Narratives are paradigmatic examples of natural language where nouns represent a proxy of information Functional magnetic resonance imaging (fMRI) studies revealed the recruitment of temporal cortices during noun processing and the existence of a noun-specific network at rest Yet it is unclear whether in narratives changes in noun density influence the brain functional connectivity so that the coupling between regions correlates with information load We acquired fMRI activity in healthy individuals listening to a narrative with noun density changing over time and measured whole-network and node-specific degree and betweenness centrality Network measures were correlated with information magnitude with a time-varying approach Noun density correlated positively with the across-regions average number of connections and negatively with the average betweenness centrality suggesting the pruning of peripheral connections as information decreased Locally the degree of the bilateral anterior superior temporal sulcus (aSTS) was positively associated with nouns Importantly aSTS connectivity cannot be explained by changes in other parts of speech (eg verbs) or syllable density Our results indicate that the brain recalibrates its global connectivity as a function of the information conveyed by nouns in natural language Also using naturalistic stimulation and network metrics we corroborate the role of aSTS in noun processing',NLP
'At present the field of natural language will also introduce in-depth learning using the concept of word vector so that the neural network can also complete the work in the field of statistics It can be said that the neural network has begun to show its advantages in the field of natural language processing In this paper the author analyzes the multimedia English course based on fuzzy statistics and neural network clustering Different factors were classified and scores were classified according to the number of characteristics of different categories It can be seen that with the popularization of the Internet MOOC teaching meets the requirements of the current college English curriculum is a breakthrough in the traditional teaching mode improves students participation and enables students to learn independently It not only conforms to the characteristics of College students but also improves their learning effect In the automatic scoring stage the quantitative text features are extracted by the feature extractor in the pre-processing stage and then the weights of network connections obtained in the training stage are used to score the weights comprehensively This model can better reflect students autonomous learning ability and language application ability',NLP
'Historic dress artifacts are a valuable source for human studies In particular they can provide important insights into the social aspects of their corresponding era These insights are commonly drawn from garment pictures as well as the accompanying descriptions and are usually stored in a standardized and controlled vocabulary that accurately describes garments and costume items called the Costume Core Vocabulary Building an accurate Costume Core from garment descriptions can be challenging because the historic garment items are often donated and the accompanying descriptions can be based on untrained individuals and use a language common to the period of the items In this paper we present an approach to use Natural Language Processing (NLP) to map the free-form text descriptions of the historic items to that of the controlled vocabulary provided by the Costume Core Despite the limited dataset we were able to train an NLP model based on the Universal Sentence Encoder to perform this mapping with more than 90% test accuracy for a subset of the Costume Core vocabulary We describe our methodology design choices and development of our approach and show the feasibility of predicting the Costume Core for unseen descriptions With more garment descriptions still being curated to be used for training we expect to have higher accuracy for better generalizability',NLP
'With the growing use of Natural Language Processing (NLP) techniques for information extraction and concept indexing in the biomedical domain a method that quickly and efficiently assigns the correct sense of an ambiguous biomedical term in a given context is needed concurrently The current status of word sense disambiguation (WSD) in the biomedical domain is that handcrafted rules are used based on contextual material The disadvantages of this approach are (i) generating WSD rules manually is a time-consuming and tedious task (ii) maintenance of rule sets becomes increasingly difficult over time and (iii) handcrafted rules are often incomplete and perform poorly in new domains comprised of specialized vocabularies and different genres of text This paper presents a two-phase unsupervised method to build a WSD classifier for an ambiguous biomedical term W The first phase automatically creates a sense-tagged corpus for W and the second phase derives a classifier for W using the derived sense-tagged corpus as a training set A formative experiment was performed which demonstrated that classifiers trained on the derived sense-tagged corpora achieved an overall accuracy of about 97% with greater than 90% accuracy for each individual ambiguous term (C) 2001 Elsevier Science (USA)',NLP
'Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision Natural Language Processing and Embodied AL Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions there has not been a holistic analysis at the center of all three Moreover even when combinations of these topics are considered more focus is placed on describing eg current architectural methods as opposed to also illustrating high-level challenges and opportunities for the field In this survey paper we discuss Embodied Vision-Language Planning (EVLP) tasks a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches metrics simulated environments as well as the datasets used for EVLP tasks Finally we present the core challenges that we believe new EVLP works should seek to address and we advocate for task construction that enables model generalizability and furthers real-world deployment',NLP
'Epitopes are specific regions on an antigens surface that the immune system recognizes Epitopes are usually protein regions on foreign immune-stimulating entities such as viruses and bacteria and in some cases endogenous proteins may act as antigens Identifying epitopes is crucial for accelerating the development of vaccines and immunotherapies However mapping epitopes in pathogen proteomes is challenging using conventional methods Screening artificial neoepitope libraries against antibodies can overcome this issue Here we applied conventional sequence analysis and methods inspired in natural language processing to reveal specific sequence patterns in the linear epitopes deposited in the Immune Epitope Database (wwwiedborg) that can serve as building blocks for the design of universal epitope libraries Our results reveal that amino acid frequency in annotated linear epitopes differs from that in the human proteome Aromatic residues are overrepresented while the presence of cysteines is practically null in epitopes Byte pair encoding tokenization shows high frequencies of tryptophan in tokens of 5 6 and 7 amino acids corroborating the findings of the conventional sequence analysis These results can be applied to reduce the diversity of linear epitope libraries by orders of magnitude',NLP
'Sarcasm is a widespread phenomenon in social media such as Twitter or Instagram As a critical task of Natural Language Processing (NLP) sarcasm detection plays an important role in many domains of semantic analysis such as stance detection and sentiment analysis Recently pre-trained models (PTMs) on large unlabelled corpora have shown excellent performance in various tasks of NLP PTMs have learned universal language representations and can help researchers avoid training a model from scratch The goal of our paper is to evaluate the performance of various PTMs in the sarcasm detection task We evaluate and analyse the performance of several representative PTMs on four well-known sarcasm detection datasets The experimental results indicate that RoBERTa outperforms other PTMs and it is also better than the best baseline in three datasets DistilBERT is the best choice for sarcasm detection task when computing resources are limited However XLNet may not be suitable for sarcasm detection task In addition we implement detailed grid search for four hyperparameters to investigate their impact on PTMs The results show that learning rate is the most important hyperparameter Furthermore we also conduct error analysis by means of several sarcastic sentences to explore the reasons of detection failures which provides instructive ideas for future research',NLP
'This paper describes a method for processing natural text and assigning meaning to it without the need of any a priori linguistic knowledge The semiotic cognitive automaton is driven only by the observations it makes and operates based solely on grounded symbols An experiment of learning from one book in the English language is detailed where certain paradigms of words are first formed thanks to side effects occurring in samples of text and then connected to functions and structures part of the internal reality of the automaton by way of meta-reasoning Making use of the reflectivity of a functional language is envisaged to fully specify the reflection engine component of the automaton In wait of a complete software specification it is possible to carry out a mental experiment to validate the capabilities of the automaton The method is general enough to learn semantics in multiple domains and lies the foundation of Semiotic Artificial Intelligence (C) 2018 The Authors Published by Elsevier BV This is an open access article under the CC BY-NC-ND license (https://creativecommonsorg/licenses/by-nc-nd/40/) Peer-review under responsibility of the scientific committee of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures',NLP
'This paper presents a manned-vehicle/unmanned-aerial-vehicle (UAV) mission system that enables an operator in a manned aircraft to issue mission level commands to an autonomous aircraft in real time A natural language interface allows the manned and unmanned vehicle to communicate in languages understood by both agents A task scheduler transforms the commands into a dynamic mission plan consisting of task waypoints These are then given to a mixed-integer linear programming (MILP)-based trajectory optimizer which safely guides the vehicle through a partially known environment in real time The MILP trajectory planning formulation and its implementation are discussed in detail Integrated simulation and June 2004 flight-test results that used an F-15 and an autonomous T-33 equipped with Boeings unmanned combat air vehicle (UCAV) avionics package are presented These activities were part of the Capstone Demonstration of the Defense Advanced Research Projects Agency-sponsored Software Enabled Control effort The flight tests mark the first time that an onboard MILP-based guidance system was used to control a UAV They also mark the first time that a natural language interface was used by a manned vehicle to task a UAV in real time',NLP
'In todays era of big data huge amounts of Web contains many important information From the Web to extract domain-specific term is an indispensable part of the natural language processing Web and it also plays an important role in the domain ontology study Chinese text has no evident difference between words therefore the present stage in Web text extraction is difficult in the field of Chinese text This article will put forward to more accurately extract the Chinese text First by removing stop words Chinese word segmentation lexical analysis to extract the nouns and noun phrases as candidate field terms Then according to the candidate term in the field of subject in the field of distribution the distribution of the subject areas each page and terms in the distribution of other background areas Combination of subject areas and background areas using both TF-IDF and DR + DC algorithm terminology and implementing the term extraction in the field of subject based on the Chinese word segmentation system of Chinese Academy of Sciences (ICTCLAS) and Language Technology Platform Cloud of Harbin Institute of Technology (LTP) [15] two platform tools to implement the term extraction so that extract more accurate domain terminology',NLP
'Agile global software engineering challenges architectural knowledge (AK) management since face-to-face interactions are preferred over comprehensive documentation which causes AK loss over time The AK condensation concept was proposed to reduce AK losing using the AK shared through unstructured electronic media A crucial part of this concept is a classification mechanism to ease AK recovery in the future We developed a Slack complement as a classification mechanism based on social tagging which recommends tags according to a chat/message topic using natural language processing (NLP) techniques We evaluated two tagging modes: NLP-assisted versus alphabetical auto-completion in terms of correctness and time to select a tag Fifty-two participants used the complement emulating an agile and global scenario and gave us their complements perceptions about usefulness ease of use and work integration Messages tagged through NLP recommendations showed fewer semantic errors and participants spent less time selecting a tag They perceived the component as very usable useful and easy to be integrated into the daily work These results indicated that a tag recommendation system is necessary to classify the shared AK accurately and quickly We will improve the NLP techniques to evaluate AK condensation in a long-term test as future work',NLP
'Background: Controlled vocabularies such as the Unified Medical Language System (UMLS (R)) and Medical Subject Headings (MeSH (R)) are widely used for biomedical natural language processing (NLP) tasks However the standard terminology in such collections suffers from low usage in biomedical literature eg only 13% of UMLS terms appear in MEDLINE (R) Results: We here propose an efficient and effective method for extracting noun phrases for biomedical semantic categories The proposed approach utilizes simple linguistic patterns to select candidate noun phrases based on headwords and a machine learning classifier is used to filter out noisy phrases For experiments three NLP rules were tested and manually evaluated by three annotators Our approaches showed over 93% precision on average for the headwords gene protein disease cell and cells Conclusions: Although biomedical terms in knowledge-rich resources may define semantic categories variations of the controlled terms in literature are still difficult to identify The method proposed here is an effort to narrow the gap between controlled vocabularies and the entities used in text Our extraction method cannot completely eliminate manual evaluation however a simple and automated solution with high precision performance provides a convenient way for enriching semantic categories by incorporating terms obtained from the literature',NLP
'Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus while traditional methods use statistical measures In our approach we want to integrate the concepts of those two approaches We present a novel weakly supervised multiword expressions extraction method which focuses on their behaviour in various contexts Our method uses a lexicon of English multiword lexical units acquired from The Oxford Dictionary of English as a reference knowledge base and leverages neural language modelling with deep learning architectures In our approach we do not need a corpus annotated specifically for the task The only required components are: a lexicon of multiword units a large corpus and a general contextual embeddings model We propose a method for building a silver dataset by spotting multiword expression occurrences and acquiring statistical collocations as negative samples Sample representation has been inspired by representations used in Natural Language Inference and relation recognition Very good results (F1=08) were obtained with CNN network applied to individual occurrences followed by weighted voting used to combine results from the whole corpus The proposed method can be quite easily applied to other languages',NLP
'A spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes The system is trained by a show-and-tell procedure in which visual scenes are paired with natural language descriptions Learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure word classes and individual words Using these structures a planning algorithm integrates syntactic semantic and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes The system generates syntactically well-formed compound adjective noun phrases as well as relative spatial clauses The acquired linguistic structures generalize from training data enabling the production of novel word sequences which were never observed during training The output of the generation system is synthesized using word-based concatenative synthesis drawing from the original training speech corpus In evaluations of semantic comprehension by human judges the performance of automatically generated spoken descriptions was comparable to human-generated descriptions This work is motivated by our long-term goal of developing spoken language processing systems which grounds semantics in machine perception and action (C) 2002 Elsevier Science Ltd All rights reserved',NLP
'Paraphrases are alternative syntactic forms in the same language expressing the same semantic content Speakers of all languages are inherently familiar with paraphrases at different levels of granularity (lexical phrasal and sentential) For quite some time the concept of paraphrasing is getting a growing attention by the research community and its potential use in several natural language processing applications (such as text summarization and machine translation) is being investigated In this paper we present what is to our best knowledge the first Turkish paraphrase corpus The corpus is gleaned from four different sources and currently contains 1270 paraphrase pairs All paraphrase pairs are carefully annotated by native Turkish speakers with the identified semantic correspondences between paraphrases The work for expanding the corpus is still under way',NLP
'In the present paper we describe TectoMT a multi-purpose open-source NLP framework It allows for fast and efficient development of NLP applications by exploiting a wide range of software modules already integrated in TectoMT such as tools for sentence segmentation tokenization morphological analysis POS tagging shallow and deep syntax parsing named entity recognition anaphora resolution tree-to-tree translation natural language generation word-level alignment of parallel corpora and other tasks One of the most complex applications of TectoMT is the English-Czech machine translation system with transfer on deep syntactic (tectogrammatical) layer Several modules are available also for other languages (German Russian Arabic) Where possible modules are implemented in a language-independent way so they call be reused in many applications',NLP
'Reading acquisition involves learning to associate visual symbols with spoken language Multiple lines of evidence indicate that instruction on the relationship between spellings and sounds may be particularly important However it is unclear whether the effectiveness of this form of instruction depends on pre-existing oral language knowledge To investigate this issue we developed a series of computational models of reading incorporating orthographic phonological and semantic processing to simulate both artificial and natural orthographic learning conditions in adults and children We exposed the models to instruction focused on spelling-sound or spelling-meaning relationships and tested the influence of the models oral language proficiency on the effectiveness of these training regimes Overall the simulations indicated that oral language proficiency is a vital foundation for reading acquisition and may modulate the effectiveness of reading instruction These results provide a computational basis for the Simple View of Reading and emphasise the importance of both oral language knowledge and spelling-sound instruction in the initial stages of learning to read',NLP
'There exist very lucid explanations of the combinatorial origins of rational and algebraic functions in particular with respect to regular and context free languages In the search to understand how to extend these natural correspondences we find that the shuffle product models many key aspects of D-finite generating functions a class which contains algebraic We consider several different takes on the shuffle product shuffle closure and shuffle grammars and give explicit generating function consequences In the process we define a grammar class that models D-finite generating functions',NLP
'Terminology extraction is an essential stop in several fields of natural language processing such as dictionary and ontology extraction In this paper we present a novel graph-based approach to terminology extraction We use SIGNUM a general purpose graph-based algorithm for binary clustering on directed weighted graphs generated using a metric for multi-word extraction Our approach is totally knowledge-free and can thus be used on corpora written in any language Furthermore it is unsupervised making it suitable for use by non-experts Our approach is evaluated on the TREC-9 corpus for filtering against the MESH and the UMLS vocabularies',NLP
'Extracting precise geographical information from the textual content referred to as toponym recognition is fundamental in geographical information retrieval and crucial in a plethora of spatial analyses eg mining location-based information from social media news reports and surveys for various applications However the performance of existing toponym recognition methods and tools is deficient in supporting tasks that rely on extracting fine-grained geographic information from texts eg locating people sending help requests with addresses through social media during disasters The emerging pretrained language models have revolutionized natural language processing and understanding by machines offering a promising pathway to optimize toponym recognition to underpin practical applications In this paper TopoBERT a uniquely designed toponym recognition module based on a one-dimensional Convolutional Neural Network (CNN1D) and Bidirectional Encoder Representation from Transformers (BERT) is proposed and fine-tuned Three datasets are leveraged to tune the hyperparameters and discover the best strategy to train the model Another seven datasets are used to evaluate the performance TopoBERT achieves state-of-the-art performance (average f1-score = 0854) compared to the seven baseline models It is encapsulated into easy-to-use python scripts and can be seamlessly applied to diverse toponym recognition tasks without additional training',NLP
'The ecology of human language is face-to-face interaction comprising cues such as prosody co-speech gestures and mouth movements Yet the multimodal context is usually stripped away in experiments as dominant paradigms focus on linguistic processing only In two studies we presented video-clips of an actress producing naturalistic passages to participants while recording their electroencephalogram We quantified multimodal cues (prosody gestures mouth movements) and measured their effect on a well-established electroencephalographic marker of processing load in comprehension (N400) We found that brain responses to words were affected by informativeness of co-occurring multimodal cues indicating that comprehension relies on linguistic and non-linguistic cues Moreover they were affected by interactions between the multimodal cues indicating that the impact of each cue dynamically changes based on the informativeness of other cues Thus results show that multimodal cues are integral to comprehension hence our theories must move beyond the limited focus on speech and linguistic processing',NLP
'With the fast growth of information science and engineering a large number of textual data generated are valuable for natural language processing and its applications Particularly finding correct answers to natural language questions or queries requires spending tremendous time and effort in human life While using search engines to discover information users manually determine the answer to a given question on a range of retrieved texts or documents Question answering relies heavily on the capability to automatically comprehend questions in human language and extract meaningful answers from a single text In recent years such question-answering systems have become increasingly popular using machine reading comprehension techniques On the other hand high-resource languages (eg English and Chinese) have witnessed tremendous growth in question-answering methodologies based on various knowledge sources Besides powerful BERTology-based language models only encode texts with a limited length The longer texts contain more distractor sentences that affect the QA system performance Vietnamese has a variety of question words in the same question type To address these challenges we propose ViQAS a new question-answering system with multi-stage transfer learning using language models based on BERTology for a low-resource language such as Vietnamese Last but not least our QA system is integrated with Vietnamese characteristics and transformer-based evidence extraction techniques into an effective contextualized language model-based QA system As a result our proposed system outperforms our forty retriever-reader QA configurations and seven state-of-the-art QA systems such as DrQA BERTserini BERTBM25 XLMRQA ORQA COBERT and NeuralQA on three Vietnamese benchmark question answering datasets',NLP
'Algorithmic automatic item generation can be used to obtain large quantities of cognitive items in the domains of knowledge and aptitude testing However conventional item models used by template-based automatic item generation techniques are not ideal for the creation of items for non-cognitive constructs Progress in this area has been made recently by employing long short-term memory recurrent neural networks to produce word sequences that syntactically resemble items typically found in personality questionnaires To date such items have been produced unconditionally without the possibility of selectively targeting personality domains In this article we offer a brief synopsis on past developments in natural language processing and explain why the automatic generation of construct-specific items has become attainable only due to recent technological progress We propose that pre-trained causal transformer models can be fine-tuned to achieve this task using implicit parameterization in conjunction with conditional generation We demonstrate this method in a tutorial-like fashion and finally compare aspects of validity in human- and machine-authored items using empirical data Our study finds that approximately two-thirds of the automatically generated items show good psychometric properties (factor loadings above 40) and that one-third even have properties equivalent to established and highly curated human-authored items Our work thus demonstrates the practical use of deep neural networks for non-cognitive automatic item generation',NLP
'A report published in 2000 from the Institute of Medicine revealed that medical errors were a leading cause of patient deaths and urged the development of error detection and reporting systems The field of radiation oncology is particularly vulnerable to these errors due to its highly complex process workflow the large number of interactions among various systems devices and medical personnel as well as the extensive preparation and treatment delivery steps Natural language processing (NLP)-aided statistical algorithms have the potential to significantly improve the discovery and reporting of these medical errors by relieving human reporters of the burden of event type categorization and creating an automated streamlined system for error incidents In this paper we demonstrate text-classification models developed with clinical data from a full service radiation oncology center (test center) that can predict the broad level and first level category of an error given a free-text description of the error All but one of the resulting models had an excellent performance as quantified by several metrics The results also suggest that more development and more extensive training data would further improve future results',NLP
'Professionals as well as the general public need effective help to access understand and consume complex biomedical concepts The existence of an interaction environment capable of automatically processing such information - thus replacing human intervention - such as chatbots is however challenging In this paper we propose a method of utilizing chatbots in the domain of biomedicine In the implementation we choose to incorporate the BERT algorithm so as to adopt a modern technique for natural language processing tasks We use several pre-trained models (RoBERTa XLM-R BERT Large and BioBert) in order to evaluate their ability to back the chatbot infrastructure The data is retrieved from the PubMed repository with the final set being formed into full sentences or potential chatbot responses thus preserving their conceptual meaning Response selection is performed using similarity metrics and F-score The results create a ranking of the models placing related ones closely recognizing the ability to always answer each question and highlighting the importance of the training previously applied to them These are compared to the Count Vectorizer technique which appears to perform better but with several weaknesses as many questions could not be answered',NLP
'Recent research has focused on developing new automated construction robots However the evolution pathways of robotics technologies and their specific applications are yet to be explored analytically This study aimed to trace these evolution pathways using critical juncture timings to describe them First data were collected from the literature using a two-step literature selection method Robotic technologies and application terms were then extracted and labelled using natural language processing techniques Next the probability of the selected terms was calculated as importance weighting Finally the critical junctures were identified by clustering the weightings using the k-means clustering algorithm The proposed method revealed three critical junctures thereby identifying four development stages since 1983 It was found that the needs for construction robots emerged after 2015 The robot configuration evolved from large-scale machinery to smaller and movable vehicles Image-related sensor systems and processing algorithms such as 3D cameras and deep learning algorithms have become popular after 2009 These improvements allow for more accurate operation in unknown and complex environments Inspection works may be the golden chance to further advance robot implementation',NLP
'Named entity recognition (NER) is fundamental in several natural language processing applications It involves finding and categorizing text into predefined categories such as a persons name location and so on One of the most famous approaches to identify named entity is the rule-based approach This paper introduces a rule-based NER method that can be used to examine Classical Arabic documents The proposed method relied on triggers words patterns gazetteers rules and blacklists generated by the linguistic information about entities named in Arabic The method operates in three stages operational stage preprocessing stage and processing the rule application stage The proposed approach was evaluated and the results indicate that this approach achieved a 902% rate of precision an 893% level of recall and an F-measure of 895% This new approach was introduced to overcome the challenges related to coverage in rule-based NER systems especially when dealing with Classical Arabic texts It improved their performance and allowed for automated rule updates The grammar rules gazetteers blacklist patterns and trigger words were all integrated into the rule-based system in this way',NLP
'The COVID-19 pandemic has led to an unprecedented challenge to public health It resulted in global efforts to understand record and alleviate the disease This research serves the purpose of generating a relevant summary related to Coronavirus The research uses the COVID-19 Open Research Dataset (CORD-19) provided by Allen Institute for AI The dataset contains 236336 academic full-text articles as of July 19 2021 This paper introduces a web-based system to handle user questions over the Coronavirus full-text scholarly articles The system periodically runs backend services to process such large amount article with basic Natural Language Processing (NLP) techniques that include tokenization N-Grams extraction and part-of-speech (PoS) tagging It automatically identifies the keywords from the question and uses cosine similarity to summarize the associated content and present to the user This research will possibly benefit researchers health workers as well as other individuals Moreover the same service can be used to train with the datasets of different domains (eg education) to generate a relevant summary for other user groups (eg students)',NLP
'Objective: To establish a structured and integrated platform of clinical data and biobank data and a client to retrieve these data Study Design: Initially the hospital information system (HIS) and biobank information system (BIS) were integrated through the patients ID numbers Then natural language processing (NLP) was used to process the integrated unstructured clinical information A query interface was designed for this system which enabled researchers to retrieve clinical or biobank data Finally several queries were listed and manually checked to test the retrieval performance of the system Results: The construction of the biobank screening system (BSS) was completed and the data were structured The BSS took an average of 2 seconds to perform a search for target patients/samples The retrieval results were consistent with the HIS and BIS For complex queries we manually checked the retrieved patients/samples and the systems accuracy was 100% Conclusion: This NLP-based system improved biological sample screening and using of clinical data We will continue to improve this system enhance resource sharing and promote the development of translational medicine',NLP
'Knowledge extracted from the protein-protein interaction (PPI) network can help researchers reveal the molecular mechanisms of biological processes With the rapid growth in the volume of the biomedical literature manually detecting and annotating PPIs from raw literature has become increasingly difficult Hence automatically extracting PPIs by machine learning methods from raw literature has gained significance in the biomedical research In this paper we propose a novel PPI extraction method based on the residual convolutional neural network (CNN) This is the first time that the residual CNN is applied to the PPI extraction task In addition the previous state-of-the-art PPI extraction models heavily rely on parsing results from natural language processing tools such as dependence parsers Our model does not rely on any parsing tools We evaluated our model based on five benchmark PPI extraction corpora AIMed BioInfer HPRD50 IEPA and LLL The experimental results showed that our model achieved the best results compared with the previous kernel-based and CNN-based PPI extraction models Compared with the previous recurrent neural network-based PPI extraction models our model achieved better or comparable performance',NLP
'Background Various tasks within health care processes are repetitive and time-consuming requiring personnel who could be better utilized elsewhere The task of assigning clinical urgency categories to internal patient referrals is one such case of a time-consuming process which may be amenable to automation through the application of text mining and natural language processing (NLP) techniques Objective This article aims to trial and evaluate a pilot study for the first component of the task-determining reasons for referrals Methods Text is extracted from scanned patient referrals before being processed to remove nonsensical symbols and identify key information The processed data are compared against a list of conditions that represent possible reasons for referral Similarity scores are used as a measure of overlap in terms used in the processed data and the condition list Results This pilot study was successful and results indicate that it would be valuable for future research to develop a more sophisticated classification model for determining reasons for referrals Issues encountered in the pilot study and methods of addressing them were outlined and should be of use to researchers working on similar problems Conclusion This pilot study successfully demonstrated that there is potential for automating the assignment of reasons for referrals and provides a foundation for further work to build on This study also outlined a potential application of text mining and NLP to automating a manual task in hospitals to save time of human resources',NLP
'Social networks platforms such as Facebook are becoming one of the most powerful sources for information The produced and shared data are important in volume in velocity and in variety Processing these data in the raw state to extract useful information can be a very difficult task and a big challenge Furthermore the Arabic language under its modern standard or dialectal shape is one of the languages producing an important quantity of data in social networks and the least analyzed The characteristics and the specificity of the Arabic language present a big challenge for sentiment analysis especially if this analysis is performed on Arabic Facebook comments In this paper we present a methodology that we have elaborated for collecting and preprocessing Facebook comments written in Modern Standard Arabic (MSA) or in Moroccan Dialectal Arabic (MDA) for Sentiment Analysis (SA) using supervised classification methods In this methodology we have detailed the processing applied to the comments text as well as various schemes of features construction (words or groups of words) useful for supervised sentiments classification This methodology was tested on comments written in MSA or in MDA collected from Facebook for the sentiment analysis on a political phenomenon The experiments results obtained are promising and this encourages us to continue working on this topic',NLP
'In this paper we apply the round-tripping algorithm to Statistical Machine Translation (SMT) for making effective use of monolingual data to tackle the training data scarcity In this approach the outbound-trip (forward) and inbound-trip (backward) translation tasks make a closed loop and produce informative feedback to train the translation models Based on this produced feedback we iteratively update the forward and backward translation models The experimental results show that translation quality is improved for Persiaw <--> Spanish translation task',NLP
'We present LODeXporter a novel approach for exporting Natural Language Processing (NLP) results to a graph-based knowledge base following Linked Open Data (LOD) principles The rules for transforming NLP entities into Resource Description Framework (RDF) triples are described in a custom mapping language which is defined in RDF Schema (RDFS) itself providing a separation of concerns between NLP pipeline engineering and knowledge base engineering LODeXporter is available as an open source component for the GATE (General Architecture for Text Engineering) framework',NLP
'An information brokerage environment for effective information structuring indexing and retrieval in the health-care administration sector is presented The system is based on ontology modeling natural language processing extensible markup language semantics analysis and behavioral description Semantics-based information acquisition is achieved through the uniform modeling representation and handling of domain-specific knowledge both content-based and procedural The system has been validated using information located on several repositories in the web and its performance is reported in terms of precision and recall',NLP
'Despite the richness and vastness of the common-sense knowledge bases we argue that common-sense knowledge has to be integrated into the target applications (Text Classification Dialogue systems Information Extraction systems etc) more effectively In order to consider this common-sense knowledge in target applications we propose a deep learning model of common-sense knowledge in Portuguese language which can be easily coupled in Natural Language Understanding (NLU) systems in order to leverage their performance More specifically the model is composed by a neural network LSTM (Long Short Term Memory) that receives a text from the target application for example an user message in a dialog a response to a user tweet a news text; and selects and learns what is the best set of common-sense relations to return to the target application which should be considered in the target learning model or system We implemented the common-sense learning module in two target applications - a Stance Classification system and an End-to-End Dialogue system In both cases incorporating the deep learning model improved the results',NLP
'General language model BERT pre-trained on cross-domain text corpus BookCorpus and Wikipedia achieves excellent performance on a couple of natural language processing tasks through the way of fine-tuning in the downstream tasks But it still lacks of task-specific knowledge and domain-related knowledge for further improving the performance of BERT model and more detailed fine-tuning strategy analyses are necessary To address these problem a BERT-based text classification model BERT4TC is proposed via constructing auxiliary sentence to turn the classification task into a binary sentence-pair one aiming to address the limited training data problem and task-awareness problem The architecture and implementation details of BERT4TC are also presented as well as a post-training approach for addressing the domain challenge of BERT Finally extensive experiments are conducted on seven public widely-studied datasets for analyzing the fine-tuning strategies from the perspectives of learning rate sequence length and hidden state vector selection After that BERT4TC models with different auxiliary sentences and post-training objectives are compared and analyzed in depth The experiment results show that BERT4TC with suitable auxiliary sentence significantly outperforms both typical feature-based methods and fine-tuning methods and achieves new state-of-the-art performance on multi-class classification datasets For binary sentiment classification datasets our BERT4TC post-trained with suitable domain-related corpus also achieves better results compared with original BERT model',NLP
'Text classification is one of the major research areas for Natural Language Processing (NLP) Long Short Term Memory (LSTM) Convolutional Neural Networks (CNN) and their combination models have been applied in many NLP tasks This paper presents a joint CNN with no max-polling layer and Bidirectional LSTM to fulfill the requirements of each model The proposed model takes advantage of CNN to extract features and Bi-LSTM to capture long term contextual information from past and future contexts The proposed model is compared with CNN Bi-LSTM RNN and CNN-LSTM models with pre-trained word embedding on five article datasets in Myanmar language',NLP
'The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters The definition of illegal characters and the specific removal strategy depend on the task language domain etc which often lead to tiresome and repetitive scripting of rules In this paper we introduce a simple statistical method uniblock1 to overcome this problem For each sentence uniblock generates a fixed-size feature vector using Unicode block information of the characters A Gaussian mixture model is then estimated on some clean corpus using variational inference The learned model can then be used to score sentences and filter corpus We present experimental results on Sentiment Analysis Language-Modeling and Machine Translation and show the simplicity and effectiveness of our method',NLP
'When transcribing Broadcast News data in highly inflected languages the vocabulary growth leads to high out-of-vocabulary rates To address this problem we propose a daily and unsupervised adaptation approach which dynamically adapts the active vocabulary and LM to the topic of the current news segment during a multi-pass speech recognition process Based on texts daily available on the Web a story-based vocabulary is selected using a morpho-syntatic technique Using an Information Retrieval engine relevant documents are extracted from a large corpus to generate a story-based LM Experiments were carried out for a European Portuguese BN transcription system Preliminary results yield a relative reduction of 652% in OOV and 66% in WER',NLP
'In this work we train the first monolingual Lithuanian transformer model on a relatively large corpus of Lithuanian news articles and compare various output decoding algorithms for abstractive news summarization We achieve an average ROUGE-2 score 0163 generated summaries are coherent and look impressive at first glance However some of them contain misleading information that is not so easy to spot We describe all the technical details and share our trained model and accompanying code in an online open-source repository as well as some characteristic samples of the generated summaries',NLP
'In this paper we discuss recent advancements in the field of knowledge representation and question-answering using natural language interfaces The focus is to reveal projects weak capabilities in temporal processing namely time modeling and reasoning over temporal facts With these deficiencies in mind we propose new knowledge-based system called Dolphin suitable to parse sentences in Czech language identify and model temporal information and infer answers to questions regarding time information Fundamental theory behind Dolphin is Transparent Intensional Logic which is high-order intensional logic calculus Finally we discuss Dolphin evaluation and future work',NLP
'The hands-busy nature of many clinical examinations means that keyboard and mouse driven interface paradigms are unable to capture medical information at source The impact of compromised data integrity when using such systems and their inability to serve the needs of clinicians in an endoscopic context is examined A speech driven application has been developed to serve the clinical process of endoscopy and record data sir source The system exploits the power of a natural narrative to capture and generate consistent visual and textual clinical information',NLP
'On a wide range of natural language processing and information retrieval tasks transformer-based models particularly pre-trained language models like BERT have demonstrated tremendous effectiveness Due to the quadratic complexity of the self-attention mechanism however such models have difficulties processing long documents Recent works dealing with this issue include truncating long documents in which case one loses potential relevant information segmenting them into several passages which may lead to miss some information and high computational complexity when the number of passages is large or modifying the self-attention mechanism to make it sparser as in sparse-attention models at the risk again of missing some information We follow here a slightly different approach in which one first selects key blocks of a long document by local query-block pre-ranking and then few blocks are aggregated to form a short document that can be processed by a model such as BERT Experiments conducted on standard Information Retrieval datasets demonstrate the effectiveness of the proposed approach',NLP
'We present two comparable diachronic corpora of scientific English (RSC UD-Parsed 10) and German (DTAW UDParsed 10) from the Late Modern Period (17thc-19thc) annotated with Universal Dependencies We describe several steps of data pre-processing and evaluate the resulting parsing accuracy showing how our pre-processing steps significantly improve output quality As a sanity check for the representativity of our data we conduct a case study comparing previously gained insights on grammatical change in the scientific genre with our data Our results reflect the often reported trend of English scientific discourse towards heavy noun phrases and a simplification of the sentence structure (Halliday 1988; Halliday and Martin 1993; Biber and Gray 2011; Biber and Gray 2016) We also show that this trend applies to German scientific discourse as well The presented corpora are valuable resources suitable for the contrastive analysis of syntactic diachronic change in the scientific genre between 1650 and 1900 The presented pre-processing procedures and their evaluations are applicable to other languages and can be useful for a variety of Natural Language Processing tasks such as syntactic parsing',NLP
'Recently proposed pre-trained language models can be easily fine-tuned to a wide range of downstream tasks However a large-scale labelled task-specific dataset is required for fine-tuning creating a bottleneck in the development process of machine learning applications To foster a fast development by reducing manual labelling efforts we propose a Label-Efficient Training Scheme (LETS) The proposed LETS consists of three elements: (i) task-specific pre-training to exploit unlabelled task-specific corpus data (ii) label augmentation to maximise the utility of labelled data and (iii) active learning to label data strategically In this paper we apply LETS to a novel aspect-based sentiment analysis (ABSA) use-case for analysing the reviews of the health-related program supporting people to improve their sleep quality We validate the proposed LETS on a custom health-related program-reviews dataset and another ABSA benchmark dataset Experimental results show that the LETS can reduce manual labelling efforts 2-3 times compared to labelling with random sampling on both datasets The LETS also outperforms other state-of-the-art active learning methods Furthermore the experimental results show that LETS can contribute to better generalisability with both datasets compared to other methods thanks to the task-specific pre-training and the proposed label augmentation We expect this work could contribute to the natural language processing (NLP) domain by addressing the issue of the high cost of manually labelling data Also our work could contribute to the healthcare domain by introducing a new potential application of NLP techniques',NLP
'Fake news (FN) spreads faster than ever due to social networks ease of access increasing reach and lower cost Twitter and Facebook are the most used platforms allowing users to express news in short simple lines that can be fake using their smartphones Hence real-time prediction and fast response time are vital in spotting FN and opposing its negative impact However smartphones have limited computational capabilities besides unreliable network connections Relying on the amalgamation of the edge fog and cloud computing can relieve the previous bottleneck where computation offloads from edge devices to higher network layers on demand In this paper we proposed EdgeFNF an edge fake news finder approach toward a fully Edge-to-Cloud mobile architecture EdgeFNF collects data from social media platforms eg tweets and posts preprocess them on the mobile edge node and uploads the metadata into a cloud server where multiple data processing techniques for text such as Natural Language Processing (NLP) take place Henceforth detect fake news using NLTK and BERT algorithms We provide the methodology system architecture and merits for achieving real-time accurate detection of fake news',NLP
'Psychometric measures of ability attitudes perceptions and beliefs are crucial for understanding user behavior in various contexts including health security e-commerce and finance Traditionally psychometric dimensions have been measured and collected using survey-based methods Inferring such constructs from user-generated text could allow timely unobtrusive collection and analysis In this work we construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust anxiety numeracy and literacy in the health domain We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8502 respondents Our testbed also encompasses self-reported demographic information including race sex age income and education allowing for measuring bias and benchmarking fairness of text classification methods We report preliminary results on use of the text to predict/categorize users survey response labels and on the fairness of these models We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness',NLP
'Natural language processing models based on machine learning (ML-NLP models) have been developed to solve practical problems such as interpreting an Internet search query These models are not intended to reflect human language comprehension mechanisms and the word representations used by ML-NLP models and human brains might therefore be quite different However because ML-NLP models are trained with the same kinds of inputs that humans must process and they must solve many of the same computational problems as the human brain ML-NLP models and human brains may end up with similar word representations To distinguish between these hypotheses we used representational similarity analysis to compare the representational geometry of word representations in two ML-NLP models with the representational geometry of the human brain as indexed with event-related potentials (ERPs) Participants listened to stories while the electroencephalogram was recorded We extracted averaged ERPs for each of the 100 words that occurred most frequently in the stories and we calculated the similarity of the neural response for each pair of words We compared this 100 x 100 similarity matrix to the 100 x 100 similarity matrix for the word pairs according to two ML-NLP models We found significant representational similarity between the neural data and each ML-NLP model beginning within 250 ms of word onset These results indicate that ML-NLP systems that are designed to solve practical technology problems have a representational geometry that is correlated with that of the human brain presumably because both are influenced by the structural properties and statistics of language',NLP
'Biomedical natural language processing (BioNLP) is a useful technique that unlocks valuable information stored in textual data for practice and/or research Syntactic parsing is a critical component of BioNLP applications that rely on correctly determining the sentence and phrase structure of free text In addition to dealing with the vast amount of domain-specific terms a robust biomedical parser needs to model the semantic grammar to obtain viable syntactic structures With either a rule-based or corpus-based approach the grammar engineering process requires substantial time and knowledge from experts and does not always yield a semantically transferable grammar To reduce the human effort and to promote semantic transferability we propose an automated method for deriving a probabilistic grammar based on a training corpus consisting of concept strings and semantic classes from the Unified Medical Language System (UMLS) a comprehensive terminology resource widely used by the community The grammar is designed to specify noun phrases only due to the nominal nature of the majority of biomedical terminological concepts Evaluated on manually parsed clinical notes the derived grammar achieved a recall of 0644 precision of 0737 and average cross-bracketing of 061 which demonstrated better performance than a control grammar with the semantic information removed Error analysis revealed shortcomings that could be addressed to improve performance The results indicated the feasibility of an approach which automatically incorporates terminology semantics in the building of an operational grammar Although the current performance of the unsupervised solution does not adequately replace manual engineering we believe once the performance issues are addressed it could serve as an aide in a semi-supervised solution (C) 2011 Elsevier Inc All rights reserved',NLP
'An inconsistent and ambiguous Software Requirement Specification (SRS) document results in an erroneous/failed software project Hence it is a serious challenge to handle and process complex and ambiguous requirements Most of the literature work focuses on detection and resolution of ambiguity in software requirements Also there is no standardized way to write unambiguous and consistent requirements The goal of this research was to generate an ambiguity-less SRS document This paper presents a new approach to write ambiguity-less requirements Furthermore we design a framework for Natural Language (NL) to Controlled Natural Language (CNL) (such as Semantic Business Vocabulary and Rules (SBVR)) transition and develop a prototype The prototype also generates Resource Description Framework (RDF) representation The SBVR has a shared meaning concept that minimizes ambiguity and RDF representation is supported by query language such as SPARQL Protocol and RDF Query Language (SPARQL) The proposed approach can help software engineers to translate NL requirements into a format that is understandable by all stakeholders and also is machine processable The results of our prototype are encouraging exhibiting the efficient performance of our developed prototype in terms of usability and correctness',NLP
'In recent years a number of psycholinguistic experiments have pointed to the interaction between language and vision In particular the interaction between visual attention and linguistic reference In parallel with this several theories of discourse have attempted to provide an account of the relationship between types of referential expressions on the one hand and the degree of mental activation on the other Building on both of these traditions this paper describes an attention based approach to visually situated reference resolution The framework uses the relationship between referential form and preferred mode of interpretation as a basis for a weighted integration of linguistic and visual attention scores for each entity in the multimodal context The resulting integrated attention scores are then used to rank the candidate referents during the resolution process with the candidate scoring the highest selected as the referent One advantage of this approach is that the resolution process occurs within the full multimodal context in so far as the referent is selected from a full list of the objects in the multimodal context As a result situations where the intended target of the reference is erroneously excluded due to an individual assumption within the resolution process are avoided Moreover the system can recognise situations where attention cues from different modalities make a reference potentially ambiguous',NLP
'Advances in communication technologies have enabled peoples to deliver more Due to this phenomenon an increasing amount of data are easily disseminated and published on the internet which encouraged the practice of paraphrasing It allows the original sentence to be concealed by alternative expressions of the same meaning Its detection consists in identifying the degree of semantic similarity between them It is one of the complex tasks of automatic natural language processing and artificial intelligence Despite the fact that Arabic language is spoken by a large population around the world it is rich of grammars and semantics that made hard its sentences modeling and similarity computing In this paper an Arabic extrinsic paraphrase identification method is proposed It is based on a Siamese recurrent neural networks architecture seeing its performance in processing variable size of textual sequences Indeed pertinent features are firstly extracted using global word vector that used a global co-occurrence matrix based on a local context window Then bidirectional long short-term memory is introduced that incorporated efficiently long-term dependent relationships and captured meaningful contextual semantics between words For paraphrase identification cosine measure is used as a merge function It was useful for identifying semantic similarity between the obtained source and suspect vectors To address the lack of free and publicly Arabic paraphrased datasets word2vec algorithm and part-of-speech tagging are combined to generate suspect sentences For its validation its quality is compared to the SemEval benchmark Experiments demonstrated the effectiveness of our proposals methods',NLP
'Machine translation (MT) has already been widely applied in various fields such as businesses health communication and international relations However its performance is not satisfactory in specific domains Researchers have devised different solutions such as controlled languages (CLs) Nonetheless as the previous CLs have placed too much emphasis on words or sentences the author believes it is important to consider contextual factors Therefore the author created a controlled document authoring system that integrates document formalization CLs MT and terminology management Evaluation work was then conducted to assess whether the system was effective and satisfactory This research provides fascinating insights into CLs MT technical writing terminology management translation technology and natural language processing applications',NLP
'The objective of video description or dense video captioning task is to generate a description of the video content The task consists of identifying and describing distinct temporal segments called events Existing methods utilize relative context to obtain better sentences In this paper we propose a hierarchical captioning model which follows encoder-decoder scheme and consists of twoLSTMs for sentence generation The visual and language information are encoded as context using bi-directional alteration of single-stream temporal action proposal network and is utilized in the next stage to produce coherent and contextually aware sentences The proposed system is tested on ActivityNet captioning dataset and performed relatively better when compared with other existing approaches',NLP
'The hierarchical organization of human cortical circuits integrates information across different timescales via temporal receptive windows which increase in length from lower to higher levels of the cortical hierarchy (Hasson et al 2015) A recent neurobiological model of higher-order language processing (Bornkessel-Schlesewsky et al 2015) posits that temporal receptive windows in the dorsal auditory stream provide the basis for a hierarchically organized predictive coding architecture (Friston and Kiebel 2009) In this stream a nested set of internal models generates time-based (when) predictions for upcoming input at different linguistic levels (sounds words sentences discourse) Here we used naturalistic stories to test the hypothesis that multi-sentence discourse-level predictions are processed in the dorsal auditory stream yielding attenuated BOLD responses for highly predicted versus less strongly predicted language input The results were as hypothesized: discourse-related cues such as passive voice which effect a higher predictability of remention for a character at a later point within a story led to attenuated BOLD responses for auditory input of high versus low predictability within the dorsal auditory stream specifically in the inferior parietal lobule middle frontal gyrus and dorsal parts of the inferior frontal gyrus among other areas Additionally we found effects of content-related (what) predictions in ventral regions These findings provide novel evidence that hierarchical predictive coding extends to discourse-level processing in natural language Importantly they ground language processing on a hierarchically organized predictive network as a common underlying neurobiological basis shared with other brain functions',NLP
'Semantic role labelling (SRL) is a task in natural language processing which detects and classifies the semantic arguments associated with the predicates of a sentence It is an important step towards understanding the meaning of a natural language There exists SRL systems for well-studied languages like English Chinese or Japanese but there is not any such system for the Vietnamese language In this paper we present the first SRL system for Vietnamese with encouraging accuracy We first demonstrate that a simple application of SRL techniques developed for English could not give a good accuracy for Vietnamese We then introduce a new algorithm for extracting candidate syntactic constituents which is much more accurate than the common node-mapping algorithm usually used in the identification step Finally in the classification step in addition to the common linguistic features we propose novel and useful features for use in SRL Our SRL system achieves an Fl score of 7353% on the Vietnamese PropBank corpus This system including software and corpus is available as an open source project and we believe that it is a good baseline for the development of future Vietnamese SRL systems',NLP
'CSIEC (Computer Simulation in Educational Communication) system with newly developed multiple functions for English instruction still focuses on supplying a virtual chatting partner (chatbot) which can chat in English with the English learners anytime anywhere It generates communicative response according to the user input the dialogue context the users and its own personality knowledge common sense knowledge and inference knowledge All these kinds of knowledge are expressed in the form of NLML an annotation language for natural language text These NLMLs can either be automatically obtained through parsing the text or be easily authored with the help of GUI editors designed by LIS So the CSIEC system suggests a naive approach of logical reasoning and inference directly through syntactical and semantic analysis of textual knowledge This approach has advantages over the old ELIZA-like keywords matching mechanism The chatting log summarization of free Internet usage within six months demonstrates this advantage In this paper we present the system architecture and underlying technologies and the educational application results (C) 2009 Elsevier BV All rights reserved',NLP
'Many of existing Arabic stemming algorithms use a large set of rules In many cases they refer to a lookup table of patterns and roots This requires a large storage space and tithe to access the information A novel neural network based approach for stemming Arabic words is proposed in this paper This approach attempts to exploit numerical relations between characters by using Backpropagation Neural Network (BPNN) No such system in literature can be found that uses neural network to extract the stemming of Arabic words',NLP
'As robots are increasingly endowed with social and communicative capabilities they will interact with humans in more settings both collaborative and competitive We explore human-robot relationships in the context of a competitive Stackelberg Security Game We vary humanoid robot expressive language (in the form of encouraging or discouraging verbal commentary) and measure the impact on participants rationality strategy prioritization mood and perceptions of the robot We learn that a robot opponent that makes discouraging comments causes a human to play a game less rationally and to perceive the robot more negatively We also contribute a simple open source Natural Language Processing framework for generating expressive sentences which was used to generate the speech of our autonomous social robot',NLP
'A lot of work has been performed for many languages other than Arabic in sentence compression Unfortunately there is a lack of effort devoted to Arabic sentence compression One of the reasons behind the lack of work in Arabic sentence compression is the absence of Arabic sentence compression corpora In order to build and evaluate sentence compression systems parallel corpora consisting of source sentences and their corresponding compressions are needed In this paper we present TALAA-ASC the first Arabic sentence compression corpus We present the methodology we followed in order to construct the corpus We also give the different statistics and analyses that we have performed on this corpus',NLP
'Natural Language Generation systems usually require substantial knowledge about the structure of the target language in order to perform the final task in the generation process - the mapping from semantic representation to text known as surface realisation Designing knowledge bases of this kind typically represented as sets of grammar rules may however become a costly labour-intensive enterprise In this work we take a statistical approach to surface realisation in which no linguistic knowledge is hard-coded but rather trained automatically from large corpora Results of a small experiment in the generation of referring expressions show significant levels of similarity between our (computer-generated) text and those produced by humans besides the usual benefits commonly associated with statistical NLP such as low development costs domain- and language-independency',NLP
'Spoken language understanding (SLU) is one of the important problem in natural language processing and especially in dialog system Fifth Dialog State Tracking Challenge (DSTC5) introduced a SLU challenge task which is automatic tagging to speech utterances by two speaker roles with speech acts tag and semantic slots tag In this paper we focus on speech acts tagging We propose local coactivate multi-task learning model for capturing structured speech acts based on sentence features by recurrent convolutional neural networks An experiment result shows that our model outperformed all other submitted entries and were able to capture coactivated local features of category and attribute which are parts of speech act',NLP
'This paper introduces the generation of textual entailment within the project CSIEC (Computer Simulation in Educational Communication) an interactive web-based human-computer dialogue system with natural language for English instruction The generation of textual entailment (GTE) is critical to the further improvement of CSIEC project and other natural language generation program Up to now we have found few literatures on the general algorithm for GTE Simulating the process that a human being learns English as a foreign language we explore our naive approach to tackle the GTE problem and its algorithm within the framework of CSIEC ie rule annotation in NLML pattern recognition and entailment transformation The time and space complexity of our algorithm is tested with some entailment examples Ail interactive command line textual entailment editor is created to generalize an entailment rule from a case pair or text and entailment The test version or this innovative GTE approach can be accessed in the CSIEC website Further works include the rules annotation based on the English textbooks and a GUI interface for normal users to edit the entailment rules',NLP
'An advanced driver simulator methodology facilitates a well-connected interaction between the environment and drivers Multiple traffic information environment language processing aims to help drivers accommodate travel demand: safety prewarning destination navigation hotel/restaurant reservation and so on Task-oriented dialogue systems generally aim to assist human users in achieving these specific goals by a conversation in the form of natural language The development of current neural network based dialogue systems relies on relevant datasets such as KVRET These datasets are generally used for training and evaluating a dialogue agent (eg an in-vehicle assistant) Therefore a simulator for the human user side is necessarily required for assessing an agent system if no real person is involved We propose a new end-to-end simulator to operate as a human driver that is capable of understanding and responding to assistant utterances This proposed driver simulator enables one to interact with an in-vehicle assistant like a real person and the diversity of conversations can be simply controlled by changing the assigned driver profile Results of our experiment demonstrate that this proposed simulator achieves the best performance on all tasks compared with other models',NLP
'Objective: Currently the use of natural language processing (NLP) approaches in order to improve search and exploration of electronic health records (EHRs) within healthcare information systems is not a common practice One reason for this is the lack of suitable lexical resources Indeed in order to support such tasks various types of such resources need to be collected or acquired (ie morphological orthographic synonymous) Methods: We propose a novel method for the acquisition of synonymy resources This method is language-independent and relies on existence of structured terminologies It enables to decipher hidden synonymy relations between simple words and terms on the basis of their syntactic analysis and exploitation of their compositionality Results: Applied to series of synonym terms from the French subset of the UMLS the method shows 99% precision The overlap between thus inferred terms and the existing sparse resources of synonyms is very low In order to better integrate these resources in an EHR search system we analyzed a sample of clinical queries submitted by healthcare professionals Conclusions: Observation of clinical queries shows that they make a very little use of the query expansion function and whenever they do synonymy relations are rarely involved',NLP
'Clocks in synchronous data-flow languages are the natural way to define several time scales in reactive systems They play a fundamental role during the specification of the system and are largely used in the compilation process to generate efficient sequential code Based on the formulation of clocks as dependent types the paper presents a simpler clock calculus reminiscent to ML type systems with first order abstract types a la Laufer & Odersky Not only this system provides clock inference it shares efficient implementations of ML type systems and appears to be expressive enough for many real applications',NLP
'Arabic dependency parsers perform poorly compared to parsers of other languages There is little research on improving the performance of Arabic parsers However recent research has shown slight improvements in the performance of dependency parsers by utilizing the lexical level of a dependency treebank To our knowledge no previous study has studied the effect of utilizing the syntactic level In this study we empirically investigated the impact of varying the set of dependency relations on the performance of Arabic dependency parsers The results were compared to those of previous studies and showed that having an appropriate set of dependency relations could improve the performance of an Arabic dependency parser',NLP
'At present the most widely used technology of pinyin-Chinese character conversion combines statistics with linguistic rules Although it basically solves such problems as long distance restriction and language recursion phenomenon it relies on a great deal of computation because there are too many candidate paths This paper tries to simplify the candidate paths by using quotient space granularity computation theory first obtains the scope of the best path in the coarser granularity world and then uses more language rules to obtain the best path The experiments indicate that this method can reduce the computation speed up the conversion and enhance the rate of accuracy by nearly 2%',NLP
'Natural language-based applications benefit from the existence of language resources from their richness and the diversity of the encoded information The focus of this paper is on two such resources namely the Bulgarian and the Romanian wordnets which have been being developed since 2001 more or less in parallel but with the similar and clear understanding of their value and of the kind of linguistic knowledge they contain After briefly presenting the history of the two wordnets we emphasise the methodology and the results of two bilateral projects run by our institutes meant to increase the interconnectivity of our wordnets and as a consequence their effectiveness in language processing',NLP
'Most embedding models used in natural language processing require retraining of the entire model to obtain the embedding value of a new word In the current system as retraining is repeated the amount of data used for learning gradually increases It is thus very inefficient to retrain the entire model whenever some new words emerge Moreover since a language has a huge number of words and its characteristics change continuously over time it is not easy to embed all words To solve both problems we propose a new embedding model the Mirroring Vector Space (MVS) which enables us to obtain a new word embedding by using the previously built word embedding model without retraining it The MVS embedding model has a convolutional neural networks (CNN) structure and presents a novel strategy to obtain word embeddings It predicts the embedding value of a word by learning the vector space of an existing embedding model using the explanations of the word It also provides flexibility for external resources reusability for training times and portability in the point that our model can be used with any models We verify these three attributes and the novelty in our experiments',NLP
'Word Sense Disambiguation (WSD) is a well-known problem in the field of Natural Language Processing (NLP) related to automatically determining the most appropriate sense of words in context Several machine learning-based approaches have been proposed to tackle the ambiguity of language but the lack of labeled data to train supervised models made semi-supervised learning (SSL) appear as an attractive option Furthermore the use of word embeddings to enhance the results of NLP tasks was shown to be an efficient strategy Thus this paper aims at adapting semi-supervised algorithms for WSD using word embeddings from Word2Vec FastText and BERT models combined with part-of-speech tags as input We conduct a systematic evaluation of four graph-based SSL models analyzing the influence of their hyperparameters on the results as well as the distances to build the graphs the percentages of labeled data and the word embeddings architectural variations As a result we show that SSL algorithms which received 10% of labeled data are strong baselines on the subsets of nouns and adjectives Additionally these algorithms do not need further training to disambiguate new words hence being competitive to supervised systems',NLP
'In this paper we provide a general approach for the concepts and processes related to the generation of linguistic descriptions of time series As we will see this approach consists of two main tasks namely a knowledge extraction task which can be seen as a Knowledge Discovery in Databases (KDD) procedure and a linguistic expression process The presented approach incorporates as a core element a description model which is based on three pillars: a knowledge representation formalism an expression language and a quality framework In the paper we also analyze the main tools and techniques that can be used regarding the mentioned tasks and pillars of the generation of linguistic descriptions of time series Additionally we provide a deep review of the main contributions in the area which come mainly from the fields of Natural Language Generation (NLG) and Fuzzy Sets and Systems The existing and potential contributions of fuzzy sets and extensions are discussed in detail Together with the application of KDD techniques we encourage the cooperation of the Fuzzy Sets and the NLG communities in order to provide a significant step forward in the development of systems for providing linguistic descriptions of time series data (C) 2015 Elsevier BV All rights reserved',NLP
'An important question is how to determine if a person is exhibiting suicidal tendencies in behavior speech or writing This paper demonstrates a method of analyzing written material to determine whether or not a person is suicidal The method involves an analysis of word frequencies that are used in a fuzzy cognitive map The fuzzy cognitive map determines if there are suicidal tendencies The method could have substantial potential in suicide prevention as well as in other forms of sociological behavior studies that also might exhibit their own identifying patterns',NLP
'Named Entity Recognition(NER) one of the most fundamental problems in natural language processing seeks to identify the boundaries and types of entities with specific meanings in natural language text As an important international language Chinese has uniqueness in many aspects and Chinese NER (CNER) is receiving increasing attention In this paper we give a comprehensive survey of recent advances in CNER We first introduce some preliminary knowledge including the common datasets tag schemes evaluation metrics and difficulties of CNER Then we separately describe recent advances in traditional research and deep learning research of CNER in which the CNER with deep learning is our focus We summarize related works in a basic three-layer architecture including character representation context encoder and context encoder and tag decoder Meanwhile the attention mechanism and adversarial-transfer learning methods based on this architecture are introduced Finally we present the future research trends and challenges of CNER (c) 2021 The Authors Published by Elsevier BV This is an open access article under the CC BY license (http:// creativecommonsorg/licenses/by/40/)',NLP
'It has become a common pattern in our field: One group introduces a language task exemplified by a dataset which they argue is challenging enough to serve as a benchmark They also provide a baseline model for it which then soon is improved upon by other groups Often research efforts then move on and the pattern repeats itself What is typically left implicit is the argumentation for why this constitutes progress and progress towards what In this paper I try to step back for a moment from this pattern and work out possible argumentations and their parts',NLP
'This paper presents a study on the recognition of personality traits from text in Brazilian Portuguese obtained from Facebook Based on the well-known Big Five model of personality we collected a basic linguistic-computational resource - which can be seen as a corpus of texts and personality inventories - and then use this resource to build supervised models of personality recognition from Facebook status updates Results suggest that certain Big Five personality traits may be reliably learned from Facebook text and pave the way for the future development of non-intrusive personality recognition systems for the Portuguese language',NLP
'This paper summarises principles of manual acquisition of conceptual graphs which evolved within the framework of a natural language processing system and are now enriched and elaborated to facilitate the construction of a larger knowledge base Our conventions provide the mapping between language structures (at syntactic and semantic levels) and conceptual structures The task-dependent aspects of our approach are clearly presented We also discuss the problems of finding suitable domain descriptions to be used as acquisition sources Finally we evaluate the manual and automatic knowledge acquisition from the perspective of our current experience',NLP
'We explore unconstrained natural language feedback as a learning signal for artificial agents Humans use rich and varied language to teach yet most prior work on interactive learning from language assumes a particular form of input (eg commands) We propose a general framework which does not make this assumption instead using aspect-based sentiment analysis to decompose feedback into sentiment over the features of a Markov decision process We then infer the teachers reward function by regressing the sentiment on the features an analogue of inverse reinforcement learning To evaluate our approach we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human We implement three artificial learners: sentiment-based literal and pragmatic models and an inference network trained end-to-end to predict rewards We then re-run our initial experiment pairing human teachers with these artificial learners All three models successfully learn from interactive human feedback The inference network approaches the performance of the literal sentiment model while the pragmatic model nears human performance Our work provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning',NLP
'Document Image Analysis (DIA) is one of the research areas of Artificial Intelligence (AI) that converts document images into machine-readable codes In DIA systems Optical Character Recognition (OCR) plays a key role in digitizing document images The output of an OCR system is further used in many applications including Natural Language Processing (NLP) Sentiment Analysis Speech Recognition and Translation Services However standard datasets are an essential requirement for the development evaluation and comparison of different text recognition techniques Pashto is one of such low resource languages that lacks availability regarding standard dataset of handwritten text This paper therefore addresses the unavailability of standard dataset for the Pashto handwritten text by developing a dataset named Pashto Handwritten Text Imagebase (PHTI) The PHTI is created by collecting handwritten samples from diverse genre of the Pashto language including poetry religion short stories articles novels sports culture and news The dataset consists of 4000 scanned images written by 400 writers including 200 males and 200 females These 4000 images are further segmented into 36082 text-line images Each text-line image is annotated/ transcribed with UTF-8 codecs The dataset can be used for many deep learning-based applications including text recognition skew detection gender classification and age-groups classification',NLP
'Stop words removal is an important step in many natural language processing (NLP) tasks Till now there is no standardized exhaustive and dynamic stop word list created for documents written in Indian Gujarati language which is spoken by nearly 66 million people worldwide Most of the existing stop words removal approaches are file or dictionary based wherein a hard-coded static nonstandardized and individually created list of stop words is used The existing approaches are time consuming and complex owing to file or dictionary preparation by collecting possible stop words from a large vocabulary complex framework and a morphologically variant Gujarati document Even the other proposed approaches in the literature are also very restricted due to their dependence on word-length word-frequency and/or training data set For the first time in scientific community worldwide this paper proposes a dynamic approach independent of all factors namely usage of file or dictionary word-length word-frequency and training dataset An 11 rule-based approach is presented focusing on automatic and dynamic identification of a complete list of Gujarati stop words Extensive empirical evidence has been presented through deployment of proposed algorithm on nearly 600 Gujarati documents categorized into routine and domain-specific categories The respective results with 9810 and 9408% average accuracy show that the proposed approach is effective and promising enough for implementation in NLP tasks involving Gujarati written documents',NLP
'This paper presents the results of developing and evaluating an automatic approach that identifies causality boundaries from causality expressions This approach focuses on explicitly expressed causalities extracted from Root Cause Analysis (RCA) reports in engineering domains Causality expressions contain Cause and Effect pairs and multiple expressions can occur in a single sentence Causality boundaries are semantically annotated text fragments explicitly indicating which parts of a fragment denote Causes and corresponding Effects To identify these linguistic analysis using natural language processing (NLP) is required Current off-the-shelf NLP tools are mostly developed based on the language models of general-purpose texts e g newspapers The lack of portability of these tools to engineering domains has been identified as a barrier to achieving comparable analysis accuracy in new domains One of the reasons for this is the rare and unpredictable behaviours of certain words in closed domains Ill-formed sentences abbreviations and capitalization of common words also contribute to the difficulty The proposed approach addresses this problem by using a probability-based method that learns the probability distribution of the boundaries not only from the NLP analysis but also from the local contexts that exploit language conventions occurred in the RCA reports Using a collection of RCA reports obtained from an aerospace company a test showed that the proposed approach achieved 86% accuracy outperforming a baseline approach that relied only on the NLP analysis',NLP
'Natural Language (NL) is the root cause of ambiguity in the SRS document The quality of the software development process can be improved by mitigating the risk with the use of semantically controlled representation A possible solution to handle ambiguity can be the use of a mathematical formal logic representation in place of NL to capture software requirements However the use of formal logic is a complex task A wrongly written formal logic will be difficult to handle and it will create serious problems in later stages of software development Furthermore stakeholders are typically not able to understand mathematical logic Hence this solution does not look feasible Another possible way of addressing above discussed ambiguity problem is the use of controlled natural languages (CNL) It can work as a bridge between NL and formal representation Since Requirement Analysis is based on communication and the analysts experience it can be modeled up to a certain limit This limit gives birth to controlled language If the document is written in a controlled language it will be feasible for the development team to use a simpler and less costly linguistic tool The CNLs are syntactically unambiguous semantically consistent and controlled Several CNLs could be found in literature such as ACE PENG CPL Formalized-English and Semantics of Business Vocabulary and Rules (SBVR) etc We aim to use an SBVR based CNL to capture stakeholders requirements and prepare an SRS document using SBVR Such software requirements will not only be syntactically clear but also semantically consistent',NLP
'Transfer learning is one approach that could be used to better train deep neural networks It plays a key role in initializing a network in computer vision applications as opposed to implementing a network from scratch which could he time-consuming Natural Language Processing (NLP) shares a similar concept of transferring from large-scale data Recent studies demonstrated that pretrained language models can be used to achieve state-of-the-art results on a multitude of NLP tasks such as sentiment analysis machine translation and text summarization In this paper we demonstrate that a free RNN/CNN self attention model used for sentiment analysis can be improved with 253% by using contextualized word representation learned in a language modeling task',NLP
'Deep learning techniques have recently shown to be successful in many natural language processing tasks forming state-of-the-art systems They require however a large amount of annotated data which is often missing This paper explores the use of domain-adversarial learning as a regularizer to avoid overfitting when training domain invariant features for deep complex neural network in low-resource and zero-resource settings in new target domains or languages In case of new languages we show that monolingual word-vectors can be directly used for training without pre-alignment Their projection into a common space can be learnt ad-hoc at training time reaching the final performance of pretrained multilingual word-vectors',NLP
'Named Entity Recognition (NER) deals with identifying personal geographical organizational or other entity types in a raw text In this paper we propose the first NER model for the Albanian language Our model is based on the maximum entropy approach We manually annotate a corpus in the historical and political domains and train the models to generate classifiers that are able to recognize relevant entities in the text We achieve good performance for precision and recall on the selected domains despite the scarcity of Albanian corpora and the fact that this paper marks the first NER research for the Albanian language Experiments demonstrate that the models can be further improved if richer training corpus is provided',NLP
'Rates of Post-traumatic stress disorder (PTSD) have risen significantly due to the COVID-19 pandemic Telehealth has emerged as a means to monitor symptoms for such disorders This is partly due to isolation or inaccessibility of therapeutic intervention caused from the pandemic Additional screening tools may be needed to augment identification and diagnosis of PTSD through a virtual medium Sentiment analysis refers to the use of natural language processing (NLP) to extract emotional content from text information In our study we train a machine learning (ML) model on text data which is part of the Audio/Visual Emotion Challenge and Workshop (AVEC-19) corpus to identify individuals with PTSD using sentiment analysis from semi-structured interviews Our sample size included 188 individuals without PTSD and 87 with PTSD The interview was conducted by an artificial character (Ellie) over a video-conference call Our model was able to achieve a balanced accuracy of 804% on a held out dataset used from the AVEC-19 challenge Additionally we implemented various partitioning techniques to determine if our model was generalizable enough This shows that learned models can use sentiment analysis of speech to identify the presence of PTSD even through a virtual medium This can serve as an important accessible and inexpensive tool to detect mental health abnormalities during the COVID-19 pandemic',NLP
'Grammatical inference is a classical problem in computational learning theory and a topic of wider influence in natural language processing We treat grammars as a model of computation and propose a novel neural approach to induction of regular grammars from positive and negative examples Our model is fully explainable its intermediate results are directly interpretable as partial parses and it can be used to learn arbitrary regular grammars when provided with sufficient data We find that our method consistently attains high recall and precision scores across a range of tests of varying complexity',NLP
'Conversational dialog systems are well known to be an effective tool for learning Modern approaches to natural language processing and machine learning have enabled various enhancements to conversational systems but they mostly rely on text- or speech-only interactions which puts limits on how learners can express and explore their knowledge We introduce a novel method that addresses such limitations by adopting a visualization that is coordinated with a text-based conversational interface This allows learners to seamlessly perceive and express knowledge through language and visual representations',NLP
'Most work in natural language processing is geared towards written standardized language varieties In this paper we present a morphology generator that is able to handle continuous linguistic variation as it is encountered in the dialect landscape of German-speaking Switzerland The generator derives inflected dialect forms from Standard German input Besides generation of inflectional affixes this system also deals with the phonetic adaptation of cognate stems and with lexical substitution of non-cognate stems Most of its rules are parametrized by probability maps extracted from a dialectological atlas thereby providing a large dialectal coverage',NLP
'Semantic similarity and mapping between ontologies are a crucial subject which is just starting to be researched for ontologies written in Portuguese Our study begins with SiSe (Similaridade Semantica) measure an extension for the Taxonomic Overlap proposed by Maedche and Staab [1] which compares the similarity between terms of distinct ontologies through the analysis of the hierarchies where they are placed SiSe development and evaluation even bringing some interesting conclusions point to continuing efforts what is discussed here in the context of more recent proposals presented in the ontology mapping domain',NLP
'Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities interactions While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks their ability to handle the nuances of procedural texts is still untested In this paper we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text First we test standard lightweight approaches for prediction with pre-trained transformers and find that these approaches underperform even simple baselines We show that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity Second we assess the degree to which transformer networks capture the process dynamics investigating such factors as merged entities and oblique entity references On two different tasks ingredient detection in recipes and QA over scientific processes we achieve state-ofthe-art results but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state(1)',NLP
'The Internet contains a large amount of heterogeneous information the extraction and structuring of which is currently a relevant task This is especially relevant for tasks of social importance in particular the analysis of the experience of using pharmaceutical products In this paper we propose a two-step sequential algorithm for extracting named entities and the relationships between them Its creation was made possible by the availability of a marked-up corpus of Internet users reviews of medicines (Russian Drug Review Corpus) The basis of the algorithm is the language model XLM-RoBERTa-sag which is pre-trained on a large corpus of unlabeled texts of reviews The developed algorithm achieves the accuracy of identifying related entities: 716 and relations: 805 which is the first estimate of the accuracy of the solution of the considered problem on the Russian-language drug review texts',NLP
'Computer programming is a novel cognitive tool that has transformed modern society What cognitive and neural mechanisms support this skill? Here we used functional magnetic resonance imaging to investigate two candidate brain systems: the multiple demand (MD) system typically recruited during math logic problem solving and executive tasks and the language system typically recruited during linguistic processing We examined MD and language system responses to code written in Python a text-based programming language (Experiment 1) and in ScratchJr a graphical programming language (Experiment 2); for both we contrasted responses to code problems with responses to content-matched sentence problems We found that the MD system exhibited strong bilateral responses to code in both experiments whereas the language system responded strongly to sentence problems but weakly or not at all to code problems Thus the MD system supports the use of novel cognitive tools even when the input is structurally similar to natural language',NLP
'Parallel corpora are necessary for multilingual researches especially in information retrieval (IR) and natural language processing (NLP) However such corpora are hard to find specifically for low-resources languages like ethnic languages Parallel corpora of ethnic languages were usually collected manually On the other hand Wikipedia as a free online encyclopedia is supporting more and more languages each year including ethnic languages in Indonesia It has become one of the largest multilingual sites in World Wide Web that provides free distributed articles In this paper we explore a few sentence alignment methods which have been used before for another domain We want to check whether Wikipedia can be used as one of the resources for collecting parallel corpora of Indonesian and Javanese an ethnic language in Indonesia We used two approaches of sentence alignment by treating Wikipedia as both parallel corpora and comparable corpora In parallel corpora case we used sentence length based and word correspondence methods Meanwhile we used the characteristics of hypertext links from Wikipedia in comparable corpora case After the experiments we can see that Wikipedia is useful enough for our purpose because both approaches gave positive results',NLP
'Stemming has shown to be effective in many natural language processing (NLP) applications such as in document classification machine translation and information retrieval (IR) This paper compares the performance of nine stemmers for Arabic language on microblog IR These stemmers include: Information Science Research Institute (ISRI) Tashaphyne Khoja AL-stem Light10 Motaz Assem Farasa and ARLStem Each stemmer was studied independently using the EveTAR dataset on a specific information retrieval task to obtain relevant query tweets The performance of the nine stemmers was evaluated using BM25 precision at 30 and Mean Average Precision (MAP) The results show that root-based stemmers (ie ISRI and Khoja) outperformed others',NLP
'Machine translation has been a potential solution for addressing the language barrier In line with this we have developed English to Sinhala machine translation system that can be accessed through the Internet The translation system runs on a web sever and can be accessed by an ordinary web client The core of the translation system contains seven modules namely English Morphological analyzer English parser Translator Sinhala Morphological generator Sinhala parser Transliteration module and three Lexicon Databases This core system has already been tested and used on standalone machines The current project has extended the core system with the use of Prolog Server Pages to provide online access thereby opening the service to a wider audience',NLP
'Using natural language as a hint can supply an additional reward for playing sparse-reward games Achieving a goal should involve several different hints while the given hints are usually incomplete Those unmentioned latent hints still rely on the sparse reward signal and make the learning process difficult In this paper we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks Experiments show that SSI not only helps to learn faster (12x) but also has a higher success rate (11% relative improvement) of the final policy',NLP
'The aim of this work is to present some of the capabilities of a Semantically intellectual Enhanced Property Protection System The system has reached a prototype phase where experiments are possible It uses an extensive semantic net algorithms for Polish language that enable it to detect similarities in two compared documents on a level far beyond simple text matching SEIPro2S benefits both from using a local document repository and from Web based resources Main focus of this work is to give a reader overview of architecture and some actual results',NLP
'The grammatical framework for the mapping between linguistic form and meaning representation known as Universal Dependencies relies on a non-constituency syntactic analysis that is centered on the notion of grammatical relation (eg Subject Object etc) Given its core goal of providing a common set of analysis primitives suitable to every natural language and its practical objective of fostering their computational grammatical processing it keeps being an active domain of research in science and technology of language This paper presents a new collection of quality language resources for the computational processing of the Portuguese language under the Universal Dependencies framework (UD) This is an all-encompassing publicly available open collection of mutually consistent and inter-operable scientific resources that includes reliably annotated corpora top-performing processing tools and expert support services: a new UPOS-annotated corpus CINTIL-UPos with 675K tokens and a new UD treebank CINTIL-UDep Treebank with nearly 38K sentences; a UPOS tagger LX-UTagger and a UD parser LX-UDParser trained on these corpora available both as local stand-alone tools and as remote web-based services; and helpdesk support ensured by the Knowledge Center for the Science and Technology of Portuguese of the CLARIN research infrastructure',NLP
'To ensure quality and effective learning fluency and comprehension the proper identification of the difficulty levels of reading materials should be observed In this paper we describe the development of automatic machine learning-based readability assessment models for educational Filipino texts using the most diverse set of linguistic features for the language Results show that using a Random Forest model obtained a high performance of 627% in terms of accuracy and 661% when using the optimal combination of feature sets consisting of traditional and syllable pattern-based predictors',NLP
'This document describes the approach and techniques used in softvvare that has been developed to generate phonemes from written Thai This software has been used to generate the phonetic transcription of Thai words in a Thai-Dutch dictionary The most important part of this software is a lexical analyzer based on regular expressions for matching patterns in the Thai writing system Because most software tools (ha( use regular expressions are still based on the 7-bit ASCII set a mapping of Thai characters to ASCII-characters has been used',NLP
'Due to the phonetic morphological and lexical complexity of Sanskrit the automatic analysis of this language is a real challenge in the area of natural language processing The paper describes a series of tests that were performed to assess the accuracy of the tagging program SanskritTagger To our knowlegde it offers the first reliable benchmark data for evaluating the quality of taggers for Sanskrit using an unrestricted dictionary and texts from different domains Based on a detailed analysis of the test results the paper points out possible directions for future improvements of statistical tagging procedures for Sanskrit',NLP
'This paper presents a semantic parser that transforms an initial semantic hypothesis into the correct semantics by applying an ordered list of transformation rules These rules are learnt automatically from a training corpus with no prior linguistic knowledge and no alignment between words and semantic concepts The learning algorithm produces a compact set of rules which enables the parser to be very efficient while retaining high accuracy We show that this parser is competitive with respect to the state-of-the-art semantic parsers on the ATIS and TownInfo tasks',NLP
'Word segmentation and part-of-speech tagging are two preliminary but fundamental components of Chinese natural language processing With the upsurge of deep learning end-to-end models are built without handcrafted features In this work we model Chinese word segmentation and part-of-speech tagging jointly on the basis of state-of-the-art BiRNN-CRF architecture LSTM is adopted as the basic recurrent unit Apart from utilizing pre-trained character embeddings and trigram features we incorporate neural language model and conduct multi-task training Highway layers are applied to tackle the discordance issue of the naive co-training Experimental results on CTB5 CTB7 and PPD datasets show the effectiveness of the proposed method',NLP
'Background: We aimed to determine whether integrating concepts from the notes from the electronic health record (EHR) data using natural language processing (NLP) could improve the identification of gout flaresMethods: Using Medicare claims linked with EHR we selected gout patients who initiated the urate-lowering therapy (ULT) Patients 12-month baseline period and on treatment follow-up were segmented into 1-month units We retrieved EHR notes for months with gout diagnosis codes and processed notes for NLP concepts We selected a random sample of 500 patients and reviewed each of their notes for the presence of a physician-documented gout flare Months containing at least 1 note mentioning gout flares were considered months with events We used 60% of patients to train predictive models with LASSO We evaluated the models by the area under the curve (AUC) in the validation data and examined positive/negative predictive values (P/NPV)Results: We extracted and labeled 839 months of follow-up (280 with gout flares) The claims-only model selected 20 variables (AUC = 069) The NLP concept-only model selected 15 (AUC = 069) The combined model selected 32 claims variables and 13 NLP concepts (AUC = 073) The claims-only model had a PPV of 064 [050 077] and an NPV of 071 [065 076] whereas the combined model had a PPV of 076 [061 088] and an NPV of 071 [065 076]Conclusion: Adding NLP concept variables to claims variables resulted in a small improvement in the identification of gout flares Our data-driven claims-only model and our combined claims/NLP-concept model outperformed existing rule-based claims algorithms reliant on medication use diagnosis and procedure codes',NLP
'Background: The recent rapid development of digital technologies offers new possibilities for more efficient implementation of electronic health record (EHR) and personal health record (PHR) systems A growing volume of healthcare data has been the hallmark of this digital transformation The large healthcare datasets complexity and their dynamic nature pose various challenges related to processing analysis storage security privacy data exchange and usability Materials and Methods: We performed a systematic review of systematic reviews to assess technological progress in EHR and PHR systems We searched MEDLINE Cochrane Web of Science and Scopus for systematic literature reviews on technological advancements that support EHR and PHR systems published between January 1 2010 and October 06 2020 Results: The searches resulted in a total of 2448 hits Of these we finally selected 23 systematic reviews Most of the included papers dealt with information extraction tools and natural language processing technology (n = 10) followed by studies that assessed the use of blockchain technology in healthcare (n = 8) Other areas of digital technology research included EHR and PHR systems in austere settings (n = 1) de-identification methods (n = 1) visualization techniques (n = 1) communication tools within EHR and PHR systems (n = 1) and methodologies for defining Clinical Information Models that promoted EHRs and PHRs interoperability (n = 1) Conclusions: Technological advancements can improve the efficiency in the implementation of EHR and PHR systems in numerous ways Natural language processing techniques either rule-based machine-learning or deep learning-based can extract information from clinical narratives and other unstructured data locked in EHRs and PHRs allowing secondary research (ie phenotyping) Moreover EHRs and PHRs are expected to be the primary beneficiaries of the blockchain technology implementation on Health Information Systems Governance regulations lack of trust poor scalability security privacy low performance and high cost remain the most critical challenges for implementing these technologies',NLP
'Despite the rapid development of artificial intelligence technology in legal services around the world little research work is being performed in the area of legal document classification in Korean language In this paper we propose and compare three different legal document classification approaches based on two deep neural network models ie Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) and two word embedding schemes Based on nearly 60000 precedent case data we obtained the highest classification accuracy (up to 86 percent) with the RNN model with Word2Vec embedding',NLP
'Ontological lexicons are considered a rich source of knowledge for the development of various natural language processing tools and applications; however they are expensive to build maintain and extend In this paper we present the Badea system for the semi-automated extraction of lexical relations specifically antonyms using a pattern-based approach to support the task of ontological lexicon enrichment The approach is based on an ontology of seed pairs of antonyms in the Arabic language; we identify patterns in which the pairs occur and then use the patterns identified to find new antonym pairs in an Arabic textual corpora Experiments are conducted on Badea using texts from three Arabic textual corpuses: KSUCCA KACSTAC and CAC The system is evaluated and the patterns reliability and system performance is measured The results from our experiments on the three Arabic corpora show that the pattern-based approach can be useful in the ontological enrichment task as the evaluation of the system resulted in the ontology being updated with over 300 new antonym pairs thereby enriching the lexicon and increasing its size by over 400% Moreover the results show important findings on the reliability of patterns in extracting antonyms for Arabic The Badea system will facilitate the enrichment of ontological lexicons that can be very useful in any Arabic natural language processing system that requires semantic relation extraction',NLP
'Fake news detection is a difficult problem due to the nuances of language Understanding the reasoning behind certain fake items implies inferring a lot of details about the various actors involved We believe that the solution to this problem should be a hybrid one combining machine learning semantics and natural language processing We introduce a new semantic fake news detection method built around relational features like sentiment entities or facts extracted directly from text Our experiments show that by adding semantic features the accuracy of fake news classification improves significantly',NLP
'Pandemic diseases such as plague have produced a vast amount of literature providing information about the spatiotemporal extent transmission or countermeasures However the manual extraction of such information from running text is a tedious process and much of this information remains locked into a narrative format Natural Language processing (NLP) is a promising tool for the automated extraction of epidemiological data and can facilitate the establishment of datasets In this paper we explore the utility of NLP to assist in the creation of a plague outbreak dataset We produced a gold standard list of toponyms by manual annotation of a German plague treatise published by Sticker in 1908 We investigated the performance of five pre-trained NLP libraries (Google Stanford CoreNLP spaCy germaNER and Geoparser) for the automated extraction of location data compared to the gold standard Of all tested algorithms spaCy performed best (sensitivity 092 F1 score 083) followed closely by Stanford CoreNLP (sensitivity 081 F1 score 087) Google NLP had a slightly lower per-formance (F1 score 072 sensitivity 078) Geoparser and germaNER had a poor sensitivity (041 and 061) We then evaluated how well automated geocoding services such as Google geocoding Geonames and Geoparser located these outbreaks correctly All geocoding services performed poorly - particularly for historical regions - and returned the correct GIS information only in 604% 527% and 338% of all cases Finally we compared our newly digitized plague dataset to a re-digitized version of the plague treatise by Biraben and provide an update of the spatio-temporal extent of the second pandemic plague outbreaks We conclude that NLP tools have their limitations but they are potentially useful to accelerate the collection of data and the generation of a global plague outbreak database',NLP
'Context The Trauma Quality Improvement Program Best Practice Guidelines recommend palliative care (PC) concurrent with restorative treatment for patients with life-threatening injuries Measuring PC delivery is challenging: administrative data are nonspecific and manual review is time intensive Objectives To identify PC delivery to patients with life-threatening trauma and compare the performance of natural language processing (NLP) a form of computer-assisted data abstraction to administrative coding and gold standard manual review Methods Patients 18 years and older admitted with life-threatening trauma were identified from two Level I trauma centers (July 2016-June 2017) Four PC process measures were examined during the trauma admission: code status clarification goals-of-care discussion PC consult and hospice assessment The performance of NLP and administrative coding were compared with manual review Multivariable regression was used to determine patient and admission factors associated with PC delivery Results There were 76791 notes associated with 2093 admissions NLP identified PC delivery in 33% of admissions compared with 8% using administrative coding Using NLP code status clarification was most commonly documented (27%) followed by goals-of-care discussion (18%) PC consult (4%) and hospice assessment (4%) Compared with manual review NLP performed more than 50 times faster and had a sensitivity of 93% a specificity of 96% and an accuracy of 95% Administrative coding had a sensitivity of 21% a specificity of 92% and an accuracy of 68% Factors associated with PC delivery included older age increased comorbidities and longer intensive care unit stay Conclusion NLP performs with similar accuracy with manual review but with improved efficiency NLP has the potential to accurately identify PC delivery and benchmark performance of best practice guidelines (C) 2019 Published by Elsevier Inc on behalf of American Academy of Hospice and Palliative Medicine',NLP
'With the advancement of scientific and engineering research a huge number of academic literature are accumulated Manually reviewing the existing literature is the main way to explore embedded knowledge and the process is quite time-consuming and labor intensive As the quantity of literature is increasing exponentially it would be more difficult to cover all aspects of the literature using the traditional manual review approach To overcome this drawback bibliometric analysis is used to analyze the current situation and trend of a specific research field In the bibliometric analysis only a few key phrases (eg authors publishers journals and citations) are usually used as the inputs for analysis Information other than those phrases is not extracted for analysis while that neglected information (eg abstract) might provide more detailed knowledge in the article To tackle with this problem this study proposed an automatic literature knowledge graph and reasoning network modeling framework based on ontology and Natural Language Processing (NLP) to facilitate the efficient knowledge exploration from literature abstract In this framework a representation ontology is proposed to characterize the literature abstract data into four knowledge elements (background objectives solutions and findings) and NLP technology is used to extract the ontology instances from the abstract automatically Based on the representation ontology a four-space integrated knowledge graph is built using NLP technology Then reasoning network is generated according to the reasoning mechanism defined in the proposed ontology model To validate the proposed framework a case study is conducted to analyze the literature in the field of construction management The case study proves that the proposed ontology model can be used to represent the knowledge embedded in the literatures abstracts and the ontology elements can be automatically extracted by NLP models The proposed framework can be an enhancement for the bibliometric analysis to explore more knowledge from the literature',NLP
'Background and objectives: Researchers have developed effective methods to index free-text clinical notes into structured database in which negation detection is a critical but challenging step In Chinese clinical records negation detection is particularly challenging because it may depend on upstream Chinese information processing components such as word segmentation [1] Traditionally negation detection was carried out mostly using rule-based methods whose comprehensiveness and portability were usually limited Our objectives in this paper are to: 1) Construct a large Chinese clinical notes corpus with negation annotated; 2) develop a negation detection tool for Chinese clinical notes; 3) evaluate the performance of character and word embedding features in Chinese clinical natural language processing Methods: In this paper we construct a Chinese clinical corpus consisting of admission and discharge summaries and propose sequence labeling based systems for negation and scope detection Our systems rely on features from bag of characters bag of words character embedding and word embedding For scopes we introduce an additional feature to handle nested scopes with multiple negations Results: The two annotators reached an agreement of 079 measured by Kappa in manual annotation In cue detection our systems are able to achieve a performance as high as 990% measured by F score which significantly outperform its rule-based counterpart (79% F) The best system uses word embedding as features which yields precision of 990% and recall of 991% In scope detection our system is able to achieve a performance of 946% measured by F score Conclusions: Our study provides a state-of-the-art negation-detecting tool for Chinese clinical free-text notes; Experimental results demonstrate that word embedding is effective in identifying negations and that nested scopes can be identified effectively by our method (C) 2016 Elsevier Ireland Ltd All rights reserved',NLP
'Drug-induced liver injury (DILI) is an adverse hepatic drug reaction that can potentially lead to life-threatening liver failure Previously published work in the scientific literature on DILI has provided valuable insights for the understanding of hepatotoxicity as well as drug development However the manual search of scientific literature in PubMed is laborious and time-consuming Natural language processing (NLP) techniques along with artificial intelligence/machine learning approaches may allow for automatic processing in identifying DILI-related literature but useful methods are yet to be demonstrated To address this issue we have developed an integrated NLP/machine learning classification model to identify DILI-related literature using only paper titles and abstracts For prediction modeling we used 14203 publications provided by the Critical Assessment of Massive Data Analysis (CAMDA) challenge employing word vectorization techniques in NLP in conjunction with machine learning methods Classification modeling was performed using 2/3 of the data for training and the remainder for test in internal validation The best performance was achieved using a linear support vector machine (SVM) model on the combined vectors derived from term frequency-inverse document frequency (TF-IDF) and Word2Vec resulting in an accuracy of 950% and an F1-score of 950% The final SVM model constructed from all 14203 publications was tested on independent datasets resulting in accuracies of 925% 963% and 983% and F1-scores of 935% 861% and 756% for three test sets (T1-T3) Furthermore the SVM model was tested on four external validation sets (V1-V4) resulting in accuracies of 920% 962% 983% and 931% and F1-scores of 924% 829% 750% and 933%',NLP
'Background: Delirium is underdiagnosed in clinical practice and is not routinely coded for billing Manual chart review can be used to identify the occurrence of delirium; however it is labor-intensive and impractical for large-scale studies Natural language processing (NLP) has the capability to process raw text in electronic health records (EHRs) and determine the meaning of the information We developed and validated NLP algorithms to automatically identify the occurrence of delirium from EHRs Methods: This study used a randomly selected cohort from the population-based Mayo Clinic Biobank (N = 300 age >= 65) We adopted the standardized evidence-based framework confusion assessment method (CAM) to develop and evaluate NLP algorithms to identify the occurrence of delirium using clinical notes in EHRs Two NLP algorithms were developed based on CAM criteria: one based on the original CAM (NLP-CAM; delirium vs no delirium) and another based on our modified CAM (NLP-mCAM; definite possible and no delirium) The sensitivity specificity and accuracy were used for concordance in delirium status between NLP algorithms and manual chart review as the gold standard The prevalence of delirium cases was examined using International Classification of Diseases 9th Revision (ICD-9) NLP-CAM and NLP-mCAM Results: NLP-CAM demonstrated a sensitivity specificity and accuracy of 0919 1000 and 0967 respectively NLP-mCAM demonstrated sensitivity specificity and accuracy of 0827 0913 and 0827 respectively The prevalence analysis of delirium showed that the NLP-CAM algorithm identified 12 651 (94%) delirium patients the NLP-mCAM algorithm identified 20 611 (153%) definite delirium cases and 10 762 (80%) possible cases Conclusions: NLP algorithms based on the standardized evidence-based CAM framework demonstrated high performance in delineating delirium status in an expeditious and cost-effective manner',NLP
'One of the main challenges to be addressed in text summarization concerns the detection of redundant information This paper presents a detailed analysis of three methods for achieving such goal The proposed methods rely on different levels of language analysis: lexical syntactic and semantic Moreover they are also analyzed for detecting relevance in texts The results show that semantic-based methods are able to detect up to 90% of redundancy compared to only the 19% of lexical-based ones This is also reflected in the quality of the generated summaries obtaining better summaries when employing syntactic- or semantic-based approaches to remove redundancy (C) 2012 Elsevier BV All rights reserved',NLP
'The paper describes the application of artificial neural networks for corpus analysis which consists of intelligent mechanisms of analysis and recognition of word clusters and their meaning The task of analyzing a corpus of academic articles was resolved with probabilistic neural networks A review of selected issues is carried out with regards to computational approaches to language modeling as well as statistical patterns of language The paper features recognition algorithms of word clusters of similar meanings but different lexico-grammatical patterns from the established corpus using four-layer neural networks The paper also presents experimental results of word cluster recognition in the context of phrase meaning analysis',NLP
'This project proposes using BERT (Bidirectional Encoder Representations from Transformers) as a tool to assist educators with automated short answer grading (ASAG) as opposed to replacing human judgement in high-stakes scenarios Many educators are hesitant to give authority to an automated system especially in assessment tasks such as grading constructed response items However evaluating free-response text can be time and labor costly for one rater let alone multiple raters In addition some degree of inconsistency exists within and between raters for assessing a given task Recent advances in Natural Language Processing have resulted in subsequent improvements for technologies that rely on artificial intelligence and human language New state-of-theart models such as BERT an open source pre-trained language model have decreased the amount of training data needed for specific tasks and in turn have reduced the amount of human annotation necessary for producing a high-quality classification model After training BERT on expert ratings of constructed responses we use subsequent automated grading to calculate Cohens Kappa as a measure of inter-rater reliability between the automated system and the human rater For practical application when the inter-rater reliability metric is unsatisfactory we suggest that the human rater(s) use the automated model to call attention to ratings where a second opinion might be needed to confirm the raters correctness and consistency of judgement',NLP
'Business process management (BPM) has been proven to provide several benefits for organizations (eg efficiency agility governance) However the effort required for adopting a process-centered approach can be a challenge in different aspects including financial concerns organizational changes and time consumption To achieve this goal many companies use different approaches such as document analysis to be able to discover and understand their business processes In light of this in this paper we propose a user-interactive visual approach to support the process comprehension by identifying and annotating core BPMN 20 elements in process descriptions Specifically our approach is able to detect sequences of words that indicate the presence of a process element create a consistent data structure and expose it as a consumable web service To evaluate our approach we conducted a survey experiment showing promising results in every category evaluated for which 88% of the users indicated positive results concerning the usefulness of the approach to assist the process modeling phase Additionally a process modeling case study shows a designed process model with a precision of 77% of process elements in comparison to its original process model',NLP
'Trends in novel manufacturing systems lead to an increased level of data availability and smart usage of these data Nowadays many approaches are available to use the data but because of an increased flexibility of the systems the interaction between machines and humans has become a challenge Humans have to browse through a huge amount of data need knowledge about the machine and underlying algorithms to interpret the results; they cannot use their known terms for communication we call it the conceptual gap The user should be enabled to communicate with the machine on a more abstract level and in a more natural way Therefore a natural language layer is introduced to provide users with a familiar interaction interface Underlying layers contain knowledge about the domain the machines and how data can be accessed and processed This enables users questions such as Are there any anomalies in the system? to be answered Answers are provided in natural language and evaluated with a test set of 204 questions',NLP
'Rationale Therapeutic administration of psychedelics has shown significant potential in historical accounts and recent clinical trials in the treatment of depression and other mood disorders A recent randomized double-blind phase-IIb study demonstrated the safety and efficacy of COMP360 COMPASS Pathways proprietary synthetic formulation of psilocybin in participants with treatment-resistant depression Objective While the phase-IIb results are promising the treatment works for a portion of the population and early prediction of outcome is a key objective as it would allow early identification of those likely to require alternative treatment Methods Transcripts were made from audio recordings of the psychological support session between participant and therapist 1 day post COMP360 administration A zero-shot machine learning classifier based on the BART large language model was used to compute two-dimensional sentiment (valence and arousal) for the participant and therapist from the transcript These scores combined with the Emotional Breakthrough Index (EBI) and treatment arm were used to predict treatment outcome as measured by MADRS scores (Code and data are available at https://githubcom/compasspathways/Sentiment2D) Results Two multinomial logistic regression models were fit to predict responder status at week 3 and through week 12 Cross-validation of these models resulted in 85% and 88% accuracy and AUC values of 88% and 85% Conclusions A machine learning algorithm using NLP and EBI accurately predicts long-term patient response allowing rapid prognostication of personalized response to psilocybin treatment and insight into therapeutic model optimization Further research is required to understand if language data from earlier stages in the therapeutic process hold similar predictive power',NLP
'The grand challenge for the 2020 International Conference on Multimedia Big Data (BigMM20) introduced a multi-aspect analysis of the MeToo movement using Twitter to analyze five different linguistic aspects: relevance stance hate speech sarcasm and dialogue acts The challenge focused on accurately assigning multiple categorical labels to tweets from the #MeTooMA dataset We received submissions from 10 different teams with 25 participants The results provide progress and insight into the value of language signals in helping to analyze tweets related to the MeToo movement',NLP
'Morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications especially when languages with complex morphology are concerned We present a system which disambiguates the output of a morphological analyzer for Hebrew It consists of several simple classifiers and a module that combines them under the constraints imposed by the analyzer We explore several approaches to classifier combination as well as a back-off mechanism that relies on a large unannotated corpus Our best result around 83 percent accuracy compares favorably with the state of the art on this task',NLP
'A double-array is a compact and fast data structure for a trie but it degrades the speed of insertion for a large set of keys In this paper two kinds of methods for improving insertion are presented The basic functions for retrieval insertion and deletion are implemented in the C language Comparing with the original double-array for large sets of keys the improved double-array is about six to 320 times faster than that for insertion Copyright (C) 2001 John Wiley & Sons Ltd',NLP
'1A prototype knowledge-based system for recipe transformation and management in batch chemical processes is presented in this paper The system named as recipe management system (RMS) consists of a knowledge base a data base and an user interface The basic architecture of the RMS is described and two important issues knowledge representation and natural language processing are discussed The aim of the system is to provide the process engineer a tool to simplify the recipe transformation process and to help the control engineer to control the recipe management activity Object-oriented programming technique and key word analysis technique are employed in the RMS',NLP
'Objective: To comparatively evaluate a range of Natural Language Processing (NLP) approaches for Information Extraction (IE) of low-prevalence concepts in clinical notes on the example of decline of insulin therapy recommendation by patients Materials and methods: We evaluated the accuracy of detection of documentation of decline of insulin therapy by patients using sentence-level naive Bayes logistic regression and support vector machine (SVM)-based classification (with and without SMOTE oversampling) token-level sequence labelling using conditional random fields (CRFs) uni- and bi-directional recurrent neural network (RNN) models with GRU and LSTM cells and rule-based detection using Canary platform All models were trained using the same manually annotated 50046-document training set and evaluated on the same 1501-document held-out set Hyperparameter optimization was performed using 10-fold cross-validation Results: At the sentence level prevalence of documentation of decline of insulin therapy by patients was 002% in both training and held-out sets Naive Bayes and logistic regression models did not achieve F-1 score >= 05 on the training set and were not further evaluated Among the other models evaluation against the held-out test set showed that SVM identified decline of insulin therapy by patients with F-1 score of 061 CRF with F-1 of 051 RNN with F-1 of 067 and Canary rule-based model with F-1 of 097 Conclusions: Identification of low-prevalence concepts can present challenges in medical language processing Rule-based systems that include the designers background knowledge of language may be able to achieve higher accuracy under these circumstances',NLP
'Realizing autonomic management control loops is pivotal for achieving self-driving networks Some studies have recently evidence the feasibility of using Automated Planning (AP) to carry out these loops However in practice the use of AP is complicated since network administrators who are non-experts in Artificial Intelligence need to define network management policies as AP-goals and combine them with the network status and network management tasks to obtain AP-problems AP planners use these problems to build up autonomic solutions formed by primitive tasks that modify the initial network state to achieve management goals Although recent approaches have investigated transforming network management policies expressed in specific languages into low-level configuration rules transforming these policies expressed in natural language into AP-goals and subsequently build up AP-based autonomic management loops remains unexplored This paper introduces a novel approach called NORA to automatically generate AP-problems by translating Goal Policies expressed in natural language into AP-goals and combining them with both the network status and the network management tasks NORA uses Natural Language Processing as the translation technique and templates as the combination technique to avoid network administrators to learn policy languages or AP-notations We used a dataset containing Goal Policies to evaluate the NORAs prototype The results show that NORA achieves high precision and spends a short-time on generating AP-problems which evinces NORA aids to overcome barriers to using AP in autonomic network management scenarios',NLP
'Machine Translation (MT) is a Natural Language Processing (NLP) application which has taken off and reported considerable progress in recent years Most recent applications of MT employ neural networks imitating the principles of human understanding and creation of meaning at conceptual and cognitive levels (Nerlich and Clarke 2000: 141) They are based on techniques which try to simulate the mechanisms of learning in biological organisms carried out through the neurons (Aggarwal 2018: 1; Theordoris 2020: 903) However human intelligence and the cognitive models which humans use through language should be subject to more examination in order to have the capacity to unveil more basic cognitive features For this reason a more holistic understanding of certain aspects of human intelligence based on cognitive models is still required in order to take machine learning a step further (Goertzel et al 2012: 124) Image schemas and image schematic complexes are among the cognitive issues which would benefit from further studies as its basic structure is fundamental in natural language processing and conceptualisation (Hedblom et al 2019) They are common in all languages and all cultures but their use is not always universal This variation influences the quality of MT as in some cases this variance is not taken into consideration while feeding the neural networks of MT This preliminary study has the objective of studying the novel idea of image schemas and image schematic complexes and proposing an applied methodology to use them in MT',NLP
'We review the problems of the acquisition of temporal knowledge for the automated construction of knowledge base in dynamic integrated expert systems the development of which is based on the task-oriented methodology and AT-TECHNOLOGY workbench Analyze modern approaches of temporal knowledge acquisition from different sources of knowledge And present features of the extended knowledge representation language and combined knowledge acquisition method as well as promising directions of its development',NLP
'The semantic interpretation of natural language utterances is usually based a large number of transformation rules which map syntactic structures (parse trees) some kind of meaning representation However those interpretation rules exhibit insufficient degree of abstraction so that the scalability and portability of such language processing systems is hard to maintain In this paper we introduce approach that is able to cope with a wide variety of semantic interpretation patterns medical free texts by applying a small inventory of abstract semantic interpretation schemata These schemata address generalized graph configurations within syntactic dependancy parse trees which abstract away from specific syntactic constructions',NLP
'Requirements specification includes technical concerns of an information system and is used throughout its life cycle It allows for sharing the vision of the system among stakeholders and facilitates its development and operation processes Natural languages are the most common form of requirements representation however they also exhibit characteristics that often introduce quality problems such as inconsistency incompleteness and ambiguousness This paper adopts the notions of linguistic pattern and linguistic style and discusses their relevance to produce better technical documentation It focuses on the textual specification of data entities which are elements commonly referred to throughout different types of requirements like use cases user stories or functional requirements This paper discusses how to textually represent the following elements: data entity attribute data type data entity constraint attribute constraint and even cluster of data entities This paper shows concrete examples and supports the discussion with three linguistic styles represented by a rigorous requirements specification language and two informal controlled natural languages one with a more compact and another with a more verbose expressive and complete representation We analyzed how other languages cope with the representation of these data entity elements and complemented that analysis and comparison based on the PENS classification scheme We conducted a pilot evaluation session with nineteen professional subjects who participated and provided encouraging feedback with positive scores in all the analyzed dimensions From this feedback we preliminarily conclude that the adoption of these linguistic patterns would help to produce better requirements specifications written more systematically and consistently',NLP
'In recent years participatory budgeting (PB) in Scotland has grown from a handful of community-led processes to a movement supported by local and national government This is epitomized by an agreement between the Scottish Government and the Convention of Scottish Local Authorities (COSLA) that at least 1% of local authority budgets will be subject to PB This ongoing research paper explores the challenges that emerge from this scaling up or mainstreaming across the 32 local authorities that make up Scotland The main objective is to evaluate local authority use of the digital platform Consul which applies Natural Language Processing (NLP) to address these challenges This project adopts a qualitative longitudinal design with interviews observations of PB processes and analysis of the digital platform data Thematic analysis is employed to capture the major issues and themes which emerge Longitudinal analysis then explores how these evolve over time The potential for 32 live study sites provides a unique opportunity to explore discrete political and social contexts which materialize and allow for a deeper dive into the challenges and issues that may exist something a wider cross-sectional study would miss Initial results show that issues and challenges which come from scaling up may be tackled using NLP technology which in a previous controlled use case-based evaluation has shown to improve the effectiveness of citizen participation',NLP
'Developers frequently use APIs to implement certain functionalities such as parsing Excel Files reading and writing text files line by line etc Developers can greatly benefit from automatic API usage sequence generation based on natural language queries for building applications in a faster and cleaner manner Existing approaches utilize information retrieval models to search for matching API sequences given a query or use RNN-based encoder-decoder to generate API sequences As it stands the first approach treats queries and API names as bags of words It lacks deep comprehension of the semantics of the queries The latter approach adapts a neural language model to encode a user query into a fixed-length context vector and generate API sequences from the context vector We want to understand the effectiveness of recent Pre-trained Transformer based Models (PTMs) for the API learning task These PTMs are trained on large natural language corpora in an unsupervised manner to retain contextual knowledge about the language and have found success in solving similar Natural Language Processing (NLP) problems However the applicability of PTMs has not yet been explored for the API sequence generation task We use a dataset that contains 7 million annotations collected from GitHub to evaluate the PTMs empirically This dataset was also used to assess previous approaches Based on our results PTMs generate more accurate API sequences and outperform other related methods by similar to 11% We have also identified two different tokenization approaches that can contribute to a significant boost in PTMs performance for the API sequence generation task',NLP
'Objective: Assessment of medical trainee learning through pre-defined competencies is now commonplace in schools of medicine We describe a novel electronic advisor system using natural language processing (NLP) to identify two geriatric medicine competencies from medical student clinical notes in the electronic medical record: advance directives (AD) and altered mental status (AMS) Materials and methods: Clinical notes from third year medical students were processed using a general-purpose NLP system to identify biomedical concepts and their section context The system analyzed these notes for relevance to AD or AMS and generated custom email alerts to students with embedded supplemental learning material customized to their notes Recall and precision of the two advisors were evaluated by physician review Students were given pre and post multiple choice question tests broadly covering geriatrics Results: Of 102 students approached 66 students consented and enrolled The system sent 393 email alerts to 54 students (82%) including 270 for AD and 123 for AMS Precision was 100% for AD and 93% for AMS Recall was 69% for AD and 100% for AMS Students mentioned ADs for 43 patients with all mentions occurring after first having received an AD reminder Students accessed educational links 34 times from the 393 email alerts There was no difference in pre (mean 62%) and post (mean 60%) test scores Conclusions: The system effectively identified two educational opportunities using NLP applied to clinical notes and demonstrated a small change in student behavior Use of electronic advisors such as these may provide a scalable model to assess specific competency elements and deliver educational opportunities (C) 2015 Elsevier Inc All rights reserved',NLP
'In recent years Robotic Process Automation (RPA) has been widely adopted across the industry as an important enabler for business process automation and digital transformation Recent advancements suggest that next generation RPA will require advanced humanrobot collaboration capabilities for providing a more natural conversational interface and supporting more complex automation orchestration needs Our work focuses on the nascent field of conversational RPA bots that are able to dynamically orchestrate automation tasks through natural language In this context recommending possible utterances and next steps to the user is an important capability to enhance humanbot collaboration We take an exploratory approach to the problem of next-best-skill recommendation in human-robot collaboration We highlight key characteristics of this problem examine existing approaches and call out specific challenges in implementing a solution We suggest that this problem calls for an integrated strategy for recommendation and illustrate a possible implementation architecture that can integrate multiple recommendation strategies',NLP
'At Meta we work to incorporate privacy-by-design into all of our products and keep user information secure We have created an ML model that detects code changes (diffs) that have privacy-sensitive implications At our scale of tens of thousands of engineers creating hundreds of thousands of diffs each month we use automated tools for detecting such diffs Inspired by recent studies on detecting defects [2 3 5] and security vulnerabilities [4 6 7] we use techniques from natural language processing to build a deep learning system for detecting privacy-sensitive code',NLP
'The availability of material in Indian languages in general and Hindi in particular on the internet is common now The material includes the literary work Unfortunately very little work has been done to analyze such text Our aim is to automate the detection of Chaupai Chhand in Hindi poems This is the maiden attempt to autodetect Chaupai Chhand in Hindi poems This would help analyze really big poems like RamCharitManas which has more than 85% Chaupais among all pads',NLP
'We address the problems of the automated temporal knowledge acquisition for development of knowledge bases in dynamic intelligent systems based on the problem-oriented methodology and AT-TECHNOLOGY workbench We review some features of the temporal version of combined knowledge acquisition method including the extended knowledge representation language for temporal knowledge representation We present modified Random Forest algorithm for temporal information extraction from databases and show its application in AT-TECHNOLOGY workbench',NLP
'BackgroundIntimate partner violence (IPV) victims and perpetrators often report suicidal ideation yet there is no comprehensive national dataset that allows for an assessment of the connection between IPV and suicide The National Violent Death Reporting System (NVDRS) captures IPV circumstances for homicide-suicides (<2% of suicides) but not single suicides (suicide unconnected to other violent deaths; >98% of suicides) ObjectiveTo facilitate a more comprehensive understanding of the co-occurrence of IPV and suicide we developed and validated a tool that detects mentions of IPV circumstances (yes/no) for single suicides in NVDRS death narratives MethodsWe used 10 000 hand-labelled single suicide cases from NVDRS (2010-2018) to train (n=8500) and validate (n=1500) a classification model using supervised machine learning We used natural language processing to extract relevant information from the death narratives within a concept normalisation framework We tested numerous models and present performance metrics for the best approach ResultsOur final model had robust sensitivity (070) specificity (098) precision (072) and kappa values (069) False positives mostly described other family violence False negatives used vague and heterogeneous language to describe IPV and often included abusive suicide threats ImplicationsIt is possible to detect IPV circumstances among singles suicides in NVDRS although vague language in death narratives limited our tools sensitivity More attention to the role of IPV in suicide is merited both during the initial death investigation processes and subsequent NVDRS reporting This tool can support future research to inform targeted prevention',NLP
'Objective: The COVID-19 (coronavirus disease 2019) pandemic response at the Medical University of South Carolina included virtual care visits for patients with suspected severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection The telehealth system used for these visits only exports a text note to integrate with the electronic health record but structured and coded information about COVID-19 (eg exposure risk factors symptoms) was needed to support clinical care and early research as well as predictive analytics for data-driven patient advising and pooled testing Materials and Methods: To capture COVID-19 information from multiple sources a new data mart and a new natural language processing (NLP) application prototype were developed The NLP application combined reused components with dictionaries and rules crafted by domain experts It was deployed as a Web service for hourly processing of new data from patients assessed or treated for COVID-19 The extracted information was then used to develop algorithms predicting SARS-CoV-2 diagnostic test results based on symptoms and exposure information Results: The dedicated data mart and NLP application were developed and deployed in a mere 10-day sprint in March 2020 The NLP application was evaluated with good accuracy (858% recall and 815% precision) The SARS-CoV-2 testing predictive analytics algorithms were configured to provide patients with data-driven COVID-19 testing advices with a sensitivity of 81% to 92% and to enable pooled testing with a negative predictive value of 90% to 91% reducing the required tests to about 63% Conclusions: SARS-CoV-2 testing predictive analytics and NLP successfully enabled data-driven patient advising and pooled testing',NLP
'Critical thinking is a well-established concept but difficult to define This article presents a prototype for the automatic evaluation of critical thinking through Natural Language Processing techniques in order to quantify four macro-indicators of critical thinking The system works on answers given to open questions and recognize the basic language skills relevance importance and novelty on the given text In the future we will try to include all the macro-indicators of critical thinking and provide an experimental evaluation of the system',NLP
'We propose a Semantic Web Service Discovery framework for finding semantically annotated Web services by using natural language processing techniques The framework searches through a set of annotated Web services for matches with a user query which consists of keywords so that knowledge about semantic languages is not required For matching keywords with Semantic Web service descriptions given in Web Service Modeling Ontology (WSMO) techniques like part-of-speech tagging lemmatization and word sense disambiguation are used Three different matching algorithms are defined and evaluated for their ability to do exact matching and approximate matching between the user query and Web Service descriptions',NLP
'The most important knowledge in the area of biology currently consists of raw text documents Bibliographic databases of biomedical articles can be searched but an efficient procedure should evaluate the relevance of documents to biology In genetics this challenge is even trickier because of the lack of consistency in genes naming tradition We aim to define a good approach for collecting relevant abstracts for biology and for studied species and genes Our approach relies on defining best queries detecting and filtering best sources',NLP
'In this study we investigate the process of generating single-sentence representations for the purpose of Dialogue Act (DA) classification including several aspects of text pre-processing and input representation which are often overlooked or underreported within the literature for example the number of words to keep in the vocabulary or input sequences We assess each of these with respect to two DA-labelled corpora using a range of supervised models which represent those most frequently applied to the task Additionally we compare context-free word embedding models with that of transfer learning via pre-trained language models including several based on the transformer architecture such as Bidirectional Encoder Representations from Transformers (BERT) and XLNET which have thus far not been widely explored for the DA classification task Our findings indicate that these text pre-processing considerations do have a statistically significant effect on classification accuracy Notably we found that viable input sequence lengths and vocabulary sizes can be much smaller than is typically used in DA classification experiments yielding no significant improvements beyond certain thresholds We also show that in some cases the contextual sentence representations generated by language models do not reliably outperform supervised methods Though BERT and its derivative models do represent a significant improvement over supervised approaches and much of the previous work on DA classification',NLP
'Purpose: Cohort building is a powerful foundation for improving clinical care performing biomedical research recruiting for clinical trials and many other applications We set out to build a cohort of all monogenic patients with a definitive causal gene diagnosis in a 3-million patient hospital system Methods: We define a subset (4461) of OMIM diseases that have at least 1 known monogenic causal gene We then introduce MonoMiner a natural language processing framework to identify molecularly confirmed monogenic patients from free-text clinical notes Results: We show that ICD-10-CM codes cover only a fraction of monogenic diseases and that even where available ICD-10-CM code-based patient retrieval offers 014 precision Searching by causal gene symbol offers great recall but has an even worse 007 precision MonoMiner achieves 6 to 11 times higher precision (080) with 087 precision on disease diagnosis alone tagging 4259 patients with 560 monogenic diseases and 534 causal genes at 048 recall Conclusion: MonoMiner enables the discovery of a large high-precision cohort of patients with monogenic diseases with an established molecular diagnosis empowering numerous downstream uses Because it relies solely on clinical notes MonoMiner is highly portable and its approach is adaptable to other domains and languages (C) 2022 American College of Medical Genetics and Genomics Published by Elsevier Inc All rights reserved',NLP
'Speculations represent uncertainty toward certain facts In clinical texts identifying speculations is a critical step of natural language processing (NLP) While it is a nontrivial task in many languages detecting speculations in Chinese clinical notes can be particularly challenging because word segmentation may be necessary as an upstream operation The objective of this paper is to construct a state-of-the-art speculation detection system for Chinese clinical notes and to investigate whether embedding features and word segmentations are worth exploiting toward this overall task We propose a sequence labeling based system for speculation detection which relies on features from bag of characters bag of words character embedding and word embedding We experiment on a novel dataset of 36828 clinical notes with 5103 gold-standard speculation annotations on 2000 notes and compare the systems in which word embeddings are calculated based on word segmentations given by general and by domain specific segmenters respectively Our systems are able to reach performance as high as 922% measured by F score We demonstrate that word segmentation is critical to produce high quality word embedding to facilitate downstream information extraction applications and suggest that a domain dependent word segmenter can be vital to such a clinical NLP task in Chinese language (C) 2016 Elsevier Inc All rights reserved',NLP
'Artificial Intelligence is a way of making a computer that works in a similar manner that the smart human think Machine language which is a type of artificial intelligence that provides computers to learn without being explicitly programmed or ruled A Multi-Purposed Question Answer Generator which is based on AI in which machine automatically generates the question from contests and also give their answers So it is very useful for teachers for giving an assignment and their solution to students It is also useful for giving one mark question as well its answer to students after every chapter So it eliminates the tedious job of teachers and gives an easy solution',NLP
'Dependency parsing is a fundamental problem in natural language processing We introduce a novel dependency-parsing framework called head-pointing-based dependency parsing In this framework we cast the Korean dependency parsing problem as a statistical head-pointing and arc-labeling problem To address this problem a novel neural network called the multitask pointer network is devised for a neural sequential head-pointing and type-labeling architecture Our approach does not require any handcrafted features or language-specific rules to parse dependency Furthermore it achieves state-of-the-art performance for Korean dependency parsing',NLP
'Word2vec is a neural network language model which can convert words and phrases into a high-quality distributed vector (called word embedding) with semantic word relationships so it offers a unique perspective to the text classification and other natural language processing (NLP) tasks In this paper we propose to combine improved tf-idf algorithm and word embedding as a way to represent documents and conduct text classification experiments on the Sogou Chinese classification corpus Our results show that the combination of word embedding and improved tf-idf algorithm can outperform either individually',NLP
'We propose an event extraction method from traffic image sequences This method extracts moving objects and their trajectories from image sequences recorded by a stationary camera These trajectories are mapped to 3D virtual space and physical parameters such as velocity and direction are estimated After that traffic events are extracted from these trajectories and physical parameters based on case-frame analysis in the field of natural language processing Our method facilitates to describe events easily and detect general traffic events and abnormal situations The experimental results of actual intersection traffic image sequence have shown the effectiveness of the method',NLP
'Semantic knowledge is often used in the framework of Natural Language Processing (NLP) applications However for some languages different from English such knowledge is not always easily available In fact for example French thesaurus are not numerous and are not enough developed In this context we present two modifications made on the French version of the EuroWordnet Thesaurus in order to improve it Firstly we present the French EuroWordNet thesaurus and its limits Then we explain two improvements we have made We add non-existing relationships by using the bilinguism capability of the EuroWordnet thesaurus and definitions by using an external multilingual resource (Wikipedia [1])',NLP
'The development of Chinas manufacturing industry has received global attention However research on the distribution pattern changes and driving forces of the manufacturing industry has been limited by the accessibility of data This study proposes a method for classifying based on natural language processing A case study was conducted employing this method hotspot detection and driving force analysis wherein the driving forces industrial development during the 13th Five-Year plan period in Jiangsu province were determined The main conclusions of the empirical case study are as follows 1) Through the acquisition of Amaps point-of-interest (POI a special point location that commonly used in modern automotive navigation systems) data an industry type classification algorithm based on the natural language processing of POI names is proposed with Jiangsu Province serving as an example The empirical test shows that the accuracy was 95% and the kappa coefficient was 0872 2) The seven types of manufacturing industries including the pulp and paper (PP) industry metallurgical chemical (MC) industry pharmaceutical manufacturing (PM) industry machinery and electronics (ME) industry wood furniture (WF) industry textile clothing (TC) industry and agricultural and food product processing (AF) industry are drawn through a 1 kmx 1km projection grid The evolution map of the spatial pattern and the density field hotspots are also drawn 3) After analyzing the driving forces of the changes in the number of manufacturing industries mentioned above we found that manufacturing base distance from town population GDP per capita distance from the railway station were the significant driving factors of changes in the manufacturing industries mentioned above The results of this research can help guide the development of manufacturing industries maximize the advantages of regional factors and conditions and provide insight into how the spatial layout of the manufacturing industry could be optimized',NLP
'Objective: Alcohol misuse is present in over a quarter of trauma patients Information in the clinical notes of the electronic health record of trauma patients may be used for phenotyping tasks with natural language processing (NLP) and supervised machine learning The objective of this study is to train and validate an NLP classifier for identifying patients with alcohol misuse Materials and Methods: An observational cohort of 1422 adult patients admitted to a trauma center between April 2013 and November 2016 Linguistic processing of clinical notes was performed using the clinical Text Analysis and Knowledge Extraction System The primary analysis was the binary classification of alcohol misuse The Alcohol Use Disorders Identification Test served as the reference standard Results: The data corpus comprised 91 045 electronic health record notes and 16 091 features In the final machine learning classifier 16 features were selected from the first 24 hours of notes for identifying alcohol misuse The classifiers performance in the validation cohort had an area under the receiver-operating characteristic curve of 078 (95% confidence interval [CI] 072 to 085) Sensitivity and specificity were at 560% (95% CI 441% to 680%) and 889% (95% CI 844% to 928%) The Hosmer-Lemeshow goodness-of-fit test demonstrates the classifier fits the data well (P = 17) A simpler rule-based keyword approach had a decrease in sensitivity when compared with the NLP classifier from 560% to 182% Conclusions: The NLP classifier has adequate predictive validity for identifying alcohol misuse in trauma centers External validation is needed before its application to augment screening',NLP
'Currently in the context in which the activities from all domains are in a closed interdependence with the instructional activity and in which the courses cant be anymore carried out exclusively in the traditional ways the use of the computer has become an essential and mandatory requirement for all levels of education and for all parties involved This stage of the educational process evolution has been named the age of the Computer Assisted Instruction Since this age started many companies researchers specialists and teachers have involved themselves in the design development and implementation of new systems software tools methods and methodologies capable to answer to the highest standards applicable in education of all grades and also capable to offer a very friendly environment for instruction satisfying in the same time the effectiveness requirements One of the goals followed and also achieved was the use of the Computer Assisted Instruction within the Intelligent Tutoring Systems (ITS) This paper is presenting an ITS developed by the author and designed using Natural Language Processing technologies a system which is capable to assist the students who are looking to achieve and understand elements and aspects related to the Computers Programming and C language discipline Also in the article are described and exemplified the functionalities of the system proposed and are presented the steps that a student has to follow from the very beginning (enrolling stage) up to the end of the instruction (visualization of performances achieved) Moreover in this paper are briefly reviewed the facilities offered by the system to the instructor in terms of the students evolution visualization assistance and follow up during the instrul k',NLP
'While commonsense knowledge acquisition and reasoning has traditionally been a core research topic in the knowledge representation and reasoning community recent years have seen a surge of interest in the natural language processing community in developing pre-trained models and testing their ability to address a variety of newly designed commonsense knowledge reasoning and generation tasks This paper presents a survey of these tasks discusses the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks and reflects on future research directions',NLP
'We present a novel method to analyse speaker alignment in second language practice dialogue Our method represents utterances as Dialogue Acts and applies Epistemic Network Analysis to their use ENA makes convergence between speakers visible and enables us to confirm hypotheses that both initial similarity and final convergence increase with student ability; and that Dialogue Act use changes with ability and over the course of an interaction Our results can inform personalised automatic tutoring tools as well as formative assessment and feedback',NLP
'This article proposes an Intelligent Retrieval System (IRS) for helping Japanese writers directly create English words and expressions from ideas originally formed in their first language as do experts of English IRS enables users to retrieve practical English document styles expressions and technical terms from the knowledge base generated from current and practical bilingual corpora with technologies of natural language processing The prototype system of IRS is more supportive than conventional tools; the number of corresponding pairs of words retrieved from the knowledge base of IRS is about 18 times larger than pairs of English and Japanese words retrieved from electronic dictionaries IRS users commented that context and co-occurrence information helped them create English sentences and avoid creating redundant expressions without anxiety',NLP
'Purpose To enhance automated methods for accurately identifying opioid-related overdoses and classifying types of overdose using electronic health record (EHR) databases Methods We developed a natural language processing (NLP) software application to code clinical text documentation of overdose including identification of intention for self-harm substances involved substance abuse and error in medication usage Using datasets balanced with cases of suspected overdose and records of individuals at elevated risk for overdose we developed and validated the application using Kaiser Permanente Northwest data then tested portability of the application using Kaiser Permanente Washington data Datasets were chart-reviewed to provide a gold standard for comparison and evaluation of the automated method Results The method performed well in identifying overdose (sensitivity = 080 specificity = 093) intentional overdose (sensitivity = 081 specificity = 098) and involvement of opioids (excluding heroin sensitivity = 072 specificity = 096) and heroin (sensitivity = 084 specificity = 10) The method performed poorly at identifying adverse drug reactions and overdose due to patient error and fairly at identifying substance abuse in opioid-related unintentional overdose (sensitivity = 067 specificity = 096) Evaluation using validation datasets yielded significant reductions in specificity and negative predictive values only for many classifications mentioned above However these measures remained above 080 thus performance observed during development was largely maintained during validation Similar results were obtained when evaluating portability although there was a significant reduction in sensitivity for unintentional overdose that was attributed to missing text clinical notes in the database Conclusions Methods that process text clinical notes show promise for improving accuracy and fidelity at identifying and classifying overdoses according to type using EHR data',NLP
'Background: Natural language processing (NLP) has become an increasingly significant role in advancing medicine Rich research achievements of NLP methods and applications for medical information processing are available It is of great significance to conduct a deep analysis to understand the recent development of NLP-empowered medical research field However limited study examining the research status of this field could be found Therefore this study aims to quantitatively assess the academic output of NLP in medical research field Methods: We conducted a bibliometric analysis on NLP-empowered medical research publications retrieved from PubMed in the period 2007-2016 The analysis focused on three aspects Firstly the literature distribution characteristics were obtained with a statistics analysis method Secondly a network analysis method was used to reveal scientific collaboration relations Finally thematic discovery and evolution was reflected using an affinity propagation clustering method Results: There were 1405 NLP-empowered medical research publications published during the 10 years with an average annual growth rate of 1839% 10 most productive publication sources together contributed more than 50% of the total publications The USA had the highest number of publications A moderately significant correlation between countrys publications and GDP per capita was revealed Denny Joshua C was the most productive author Mayo Clinic was the most productive affiliation The annual co-affiliation and co-country rates reached 6404% and 1579% in 2016 respectively 10 main great thematic areas were identified including Computational biology Terminology mining Information extraction Text classification Social medium as data source Information retrieval etc Conclusions: A bibliometric analysis of NLP-empowered medical research publications for uncovering the recent research status is presented The results can assist relevant researchers especially newcomers in understanding the research development systematically seeking scientific cooperation partners optimizing research topic choices and monitoring new scientific or technological activities',NLP
'Objective: An accurate comprehensive and up-to-date problem list can help clinicians provide patient-centered care Unfortunately problem lists created and maintained in electronic health records by providers tend to be inaccurate duplicative and out of date With advances in machine learning and natural language processing it is possible to automatically generate a problem list from the data in the EHR and keep it current In this paper we describe an automated problem list generation method and report on insights from a pilot study of physicians assessment of the generated problem lists compared to existing providers-curated problem lists in an institutions EHR system Materials and methods: The natural language processing and machine learning-based Watson(1) method models clinical thinking in identifying a patients problem list using clinical notes and structured data This pilot study assessed the Watson method and included 15 randomly selected de-identified patient records from a large healthcare system that were each planned to be reviewed by at least two internal medicine physicians The physicians created their own problem lists and then evaluated the overall usefulness of their own problem lists (P) Watson generated problem lists (W) and the existing EHR problem lists (E) on a 10-point scale The primary outcome was pairwise comparisons of P W and E Results: Six out of the 10 invited physicians completed 27 assessments of P W and E and in process evaluated 732 Watson generated problems and 444 problems in the EHR system As expected physicians rated their own lists P highest However W was rated higher than E Among 89% of assessments Watson identified at least one important problem that physicians missed Conclusion: Cognitive computing systems like this Watson system hold the potential for accurate problem-listcentered summarization of patient records potentially leading to increased efficiency better clinical decision support and improved quality of patient care',NLP
'In the last few years the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs) Given the importance of knowledge available in tabular data recent research efforts extend LMs by developing neural representations for structured data In this article we present a survey that analyzes these efforts We first abstract the different systems according to a traditional machine learning pipeline in terms of training data input representation model training and supported downstream tasks For each aspect we characterize and compare the proposed solutions Finally we discuss future work directions',NLP
'Definite Clause Grammars (DCGs) are a convenient way to describe lists in Prolog They are a common mean to specify grammars for natural language processing and to parse formal languages Despite its long history tools dedicated to the development and debugging of DCGs are rare In this paper we present an interactive web-based tool to visualise the execution of a DCG To collect the required information from Swi-Prolog we discuss several techniques including meta-interpreters trace interceptors and term expansions',NLP
'Analyzing the semantic representations of 10000 Chinese sentences and describing a new sentence analysis method that evaluates semantic preference knowledge we create a model of semantic representation analysis based on the correspondence between lexical meanings and conceptual structures and relations that underlie those lexical meanings We also propose a semantical argument-head relation that combines basic conceptual structure and Head-Driven Principle With this framework which is different from Fillmores case theory (1968) and HPSG among other we can successfully disambiguate some troublesome sentences and minimize the redundancy in language knowledge description for natural language processing',NLP
'Background Autism spectrum disorder (ASD) is a complex neurodevelopmental condition characterized by restricted repetitive behavior and impaired social communication and interactions However significant challenges remain in diagnosing and subtyping ASD due in part to the lack of a validated standardized vocabulary to characterize clinical phenotypic presentation of ASD Although the human phenotype ontology (HPO) plays an important role in delineating nuanced phenotypes for rare genetic diseases it is inadequate to capture characteristic of behavioral and psychiatric phenotypes for individuals with ASD There is a clear need therefore for a well-established phenotype terminology set that can assist in characterization of ASD phenotypes from patients clinical narratives Methods To address this challenge we used natural language processing (NLP) techniques to identify and curate ASD phenotypic terms from high-quality unstructured clinical notes in the electronic health record (EHR) on 8499 individuals with ASD 8177 individuals with non-ASD psychiatric disorders and 8482 individuals without a documented psychiatric disorder We further performed dimensional reduction clustering analysis to subgroup individuals with ASD using nonnegative matrix factorization method Results Through a note-processing pipeline that includes several steps of state-of-the-art NLP approaches we identified 3336 ASD terms linking to 1943 unique medical concepts which represents among the largest ASD terminology set to date The extracted ASD terms were further organized in a formal ontology structure similar to the HPO Clustering analysis showed that these terms could be used in a diagnostic pipeline to differentiate individuals with ASD from individuals with other psychiatric disorders Conclusion Our ASD phenotype ontology can assist clinicians and researchers in characterizing individuals with ASD facilitating automated diagnosis and subtyping individuals with ASD to facilitate personalized therapeutic decision-making',NLP
'Despite the known financial economical and humanitarian impacts of hurricanes and the floods that follow datasets consisting of flood and flood risk reduction projects are either small in scope lack in details or held privately by commercial holders However with the amount of online data growing exponentially we have seen a rise of information extraction techniques on unstructured text to drive insights On one hand social media in particular has seen a tremendous increase in popularity On the other hand despite this popularity social media has proven to be unreliable and difficult to extract full information from In contrast online newspapers are often vetted by a journalist and consist of more fine details As a result in this paper we leverage Natural Language Processing (NLP) to create a hybrid Named-Entity Recognition (NER) model that employs a domain-specific machine learning model linguistic features and rule-based matching to extract information from newspapers To the knowledge of the authors this model is the first of its kind to extract detailed flooding information and risk reduction projects over the entire contiguous United States The approach used in this paper expands upon previous similar works by widening the geographical location and applying techniques to extract information over large documents with minimal accuracy loss from the previous methods Specifically our model is able to extract information such as street closures project costs and metrics Our validation indicates an F1 score of 7213% for the NER model entity extraction a binary classification location filter with a score of 73% and an overall performance only 84% lower than a human validator against a goldstandard Through this process we find the location of 27444 streets 181076 flood risk reduction projects and 435353 storm locations throughout the United States in the past two decades',NLP
'This paper presents an implementation of a feedback mechanism for a workflow management framework A chatbot that uses natural language processing (NLP) is central to the proposed feedback mechanism NLP is used to transform text-based plain language input both human-written and machine-generated into a form that the framework can use to generate a workflow for execution in an environment of interest The example environment described here is containerized network management in which the workflow management framework using feedback can detect anomalies and mitigate potential incidents',NLP
'Objectives: This study sought to explore the use of novel natural language processing (NLP) methods for classifying unstructured qualitative textual data from interviews of patients with cancer to identify patient-reported symptoms and impacts on quality of life Methods: We tested the ability of 4 NLP models to accurately classify text from interview transcripts as symptom quality of life impact and other Interview data sets from patients with hepatocellular carcinoma (HCC) (n = 25) biliary tract cancer (BTC) (n = 23) and gastric cancer (n = 24) were used Models were cross-validated with transcript subsets designated for training validation and testing Multiclass classification performance of the 4 models was evaluated at paragraph and sentence level using the HCC testing data set and analyzed by the one-versus-rest technique quantified by the receiver operating characteristic area under the curve (ROC AUC) score Results: NLP models accurately classified multiclass text from patient interviews The Bidirectional Encoder Representations from Transformers model generally outperformed all other models at paragraph and sentence level The highest predictive performance of the Bidirectional Encoder Representations from Transformers model was observed using the HCC data set to train and BTC data set to test (mean ROC AUC 0940 [SD 0028]) with similarly high predictive performance using balanced and imbalanced training data sets from BTC and gastric cancer populations Conclusions: NLP models were accurate in predicting multiclass classification of text from interviews of patients with cancer with most surpassing 09 ROC AUC at paragraph level NLP may be a useful tool for scaling up processing of patient interviews in clinical studies and thus could serve to facilitate patient input into drug development and improving patient care',NLP
'This paper deals with the development of advanced tools and technologies for creating relevant information and suitable metadata out of textual documentation produced by Italian archaeological research A set of Natural Language Processing tools were developed to recognize and annotate various archaeological entities in Italian language textual reports The CIDOC CRM is the ontology chosen for encoding resulting output allowing for a maximum degree of standardisation of the produced metadata to guarantee interoperability with archaeological information already existing in other semantically enabled digital archives The work took place as part of the development for the TEXTCROWD platform for the European Open Science Cloud for Research Pilot Project',NLP
'Tenders are powerful means of investment of public funds and represent a strategic development resource Despite the efforts made so far by governments at national and international levels to digitalise documents related to the Public Administration sector most of the information is still available in an unstructured format only With the aim of bridging this gap we present OIE4PA our latest study on extracting and classifying relations from tenders of the Public Administration Our work focuses on the Italian language where the availability of linguistic resources to perform Natural Language Processing tasks is considerably limited Nevertheless OIE4PAadopts a multilingual approach so it can be applied to several languages by providing appropriate training data Rather than purely training a classifier on a portion of the extracted relations the backbone idea of our learning strategy is to put a supervised method based on self-training to the proof and to assess whether or not it improves the performance of the classifier For evaluation purposes we built a dataset composed of 2000 triples which have been manually annotated by two human experts The in-vitro evaluation shows that OIE4PA achieves a MacroF1 equal to 089 and a 91% accuracy In addition OIE4PA was used as the pillar of a prototype search engine which has been evaluated through an in-vivo experiment with positive feedback from 32 final users obtaining a SUS score equal to 8398',NLP
'Robot-assisted minimally invasive surgery is the gold standard for the surgical treatment of many pathological conditions since it guarantees to the patient shorter hospital stay and quicker recovery Several manuals and academic papers describe how to perform these interventions and thus contain important domain-specific knowledge This information if automatically extracted and processed can be used to extract or summarize surgical practices or develop decision making systems that can help the surgeon or nurses to optimize the patients management before during and after the surgery by providing theoretical-based suggestions However general English natural language understanding algorithms have lower efficacy and coverage issues when applied to domain others than those they are typically trained on and a domain specific textual annotated corpus is missing To overcome this problem we annotated the first robotic-surgery procedural corpus with PropBank-style semantic labels Starting from the original PropBank framebank we enriched it by adding new lemmas frames and semantic arguments required to cover missing information in general English but needed in procedural surgical language releasing the Robotic-Surgery Procedural Framebank (RSPF) We then collected from robotic-surgery textbooks as-is sentences for a total of 32448 tokens and we annotated them with RSPF labels We so obtained and publicly released the first annotated corpus of the robotic-surgical domain that can be used to foster further research on language understanding and procedural entities and relations extraction from clinical and surgical scientific literature',NLP
'The automatic correction of grammar and spelling errors is important for students second language learners and some Natural Language Processing (NLP) tasks such as part of speech and text summarization Recently Neural Machine Translation (NMT) has been an out-performing and well-established model in the task of Grammar Error Correction (GEC) Arabic GEC is still growing because of some challenges such as scarcity of training sets and the complexity of Arabic language To overcome these issues we introduced an unsupervised method to generate large-scale synthetic training data based on confusion function to increase the amount of training set Furthermore we introduced a supervised NMT model for AGEC called SCUT AGEC SCUT AGEC is a convolutional sequence-to-sequence model consisting of nine encoder-decoder layers with attention mechanism We applied fine-tuning to improve the performance and get more efficient results Convolutional Neural Networks (CNN) gives our model ability to joint feature extraction and classification in one task and we proved that it is an efficient way to capture features of the local context Moreover it is easy to obtain long-term dependencies because of convolutional layers staking Our proposed model becomes the first supervised AGEC system based on the convolutional sequence-to-sequence learning to outperforms the current state-of-the-art neural AGEC models (C) 2021 THE AUTHORS Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence Cairo University',NLP
'Study Design Retrospective cohort Objective Billing and coding-related administrative tasks are a major source of healthcare expenditure in the United States We aim to show that a second-iteration Natural Language Processing (NLP) machine learning algorithm XLNet can automate the generation of CPT codes from operative notes in ACDF PCDF and CDA procedures Methods We collected 922 operative notes from patients who underwent ACDF PCDF or CDA from 2015 to 2020 and included CPT codes generated by the billing code department We trained XLNet a generalized autoregressive pretraining method on this dataset and tested its performance by calculating AUROC and AUPRC Results The performance of the model approached human accuracy Trial 1 (ACDF) achieved an AUROC of 82 (range: 48-93) an AUPRC of 81 (range: 45-97) and class-by-class accuracy of 77% (range: 34%-91%); trial 2 (PCDF) achieved an AUROC of 83 (44-94) an AUPRC of 70 (45-96) and class-by-class accuracy of 71% (42%-93%); trial 3 (ACDF and CDA) achieved an AUROC of 95 (68-99) an AUPRC of 91 (56-98) and class-by-class accuracy of 87% (63%-99%); trial 4 (ACDF PCDF CDA) achieved an AUROC of 95 (76-99) an AUPRC of 84 (49-99) and class-by-class accuracy of 88% (70%-99%) Conclusions We show that the XLNet model can be successfully applied to orthopedic surgeons operative notes to generate CPT billing codes As NLP models as a whole continue to improve billing can be greatly augmented with artificial intelligence assisted generation of CPT billing codes which will help minimize error and promote standardization in the process',NLP
'In this paper we propose a verification methodology for System-On-Chip (SoC) design using Unified Modeling Language (UML) We introduce UML as a formal model to analyze and formalize the specification The specification and implementation validation can be performed systematically by introducing UML We applied our method to a Mobile Media Processors SoC We improved the quality of tau the specification written in informal natural language through UML modeling techniques The test scenarios and coverage metrics for implementation are derived from the UML model systematically The result shows that our proposal is effective for eliminating errors from both specification and implementation',NLP
'The Architecture Analysis and Design Language (AADL) has been widely accepted to support the development process of Distributed Real-time and Embedded (DRE) systems and ease the tension of analyzing the systems non-functional properties The AADL standard prescribes the dispatching and scheduling semantics for the thread components in the system using natural language The lack of formal semantics limits the possibility to perform formal veri fi cation of AADL speci fi cations The main contribution of this paper is a mapping from a substantial asynchronous subset of AADL into the TASM language allowing us to perform resource consumption and schedulability analysis of AADL models A small case study is presented as a validation of the usefulness of this work',NLP
'In this paper we present KIND an Italian dataset for Named-entity recognition It contains more than one million tokens with annotation covering three classes: person location and organization The dataset (around 600K tokens) mostly contains manual gold annotations in three different domains (news literature and political discourses) and a semi-automatically annotated part The multi-domain feature is the main strength of the present work offering a resource which covers different styles and language uses as well as the largest Italian NER dataset with manual gold annotations It represents an important resource for the training of NER systems in Italian Texts and annotations are freely downloadable from the Github repository',NLP
'For an autonomous robotic system the ability to share the same workspace and interact with humans is the basis for cooperative behavior In this work we investigate human spatial language as the communicative channel between the robot and the human facilitating their joint work on a tabletop We specifically combine the theory of Dynamic Neural Fields that represent perceptual and cognitive states with motor control and linguistic input in a robotic demonstration We show that such a neural dynamic framework can integrate across symbolic perceptual and motor processes to generate task-specific spatial communication in real time',NLP
'This paper presents an algorithm based on heuristic rules in order to solve Spanish definite description references This algorithm is applied to an information extraction system for Spanish language These heuristic rules are extracted from the study of an unrestricted corpus This algorithm solves identity co-reference produced by a definite description whose relation with its antecedents can be solved with syntactic or semantic information This module achieves a precision of 953% in classification task (anaphoric or non-anaphoric) and a average precision of 78% in',NLP
'Design documents are intended to outline the goals of a system or project which are utilized in the creation of specific software requirements At the NASA Jet Propulsion Laboratory California Institute of Technology Functional Design Description (FDD) documents describe the scope of the project and reflect the design and implementation of the system The specifications in the document are not explicitly written as requirements though these guidelines must be reflected in the official software requirements In this work we present a fully automatic approach to extracting software requirements from design documents as well as comparing the extracted requirements to those that exist in the official software requirement database We do this through (1) sentence extraction from the design document (2) the incorporation of coreferent text and (3) aligning the extracted text to the official software requirements Via natural language processing and information retrieval techniques our system results in an automated process that ensures that the specifications in the design document result in official software requirements We find that extraction of imperatives results in a recall rate of 073 and the TF-IDF cosine similarity metric is shown to be a useful and successful way to compare requirements Though there has been recent work investigating the usefulness of natural language processing techniques in requirement engineering this has not been made use of in the aerospace industry Aerospace requirement engineering is a field particularly ripe for this type of innovation because these techniques can both automate some of needlessly manual work and contribute to aerospace safety practices by identifying issues that a human may miss We present the first fully automated approach that extracts requirements from a design document and compares them to a database and use these findings as encouragement for future work that makes use of natural language processing techniques in aerospace requirement engineering',NLP
'Background Due to reimbursement tied in part to patients perception of their care hospitals continue to stress obtaining patient feedback and understanding it to plan interventions to improve patients experience We demonstrate the use of natural language processing (NLP) to extract meaningful information from patient feedback obtained through Press Ganey surveys Methods The first step was to standardize textual data programmatically using NLP libraries This included correcting spelling mistakes converting text to lowercase and removing words that most likely did not carry useful information Next we converted numeric data pertaining to each category based on sentiment and care aspect into charts We selected care aspect categories where there were more negative comments for more in-depth study Using NLP we made tables of most frequently appearing words adjectives and bigrams Comments with frequent words/combinations underwent further study manually to understand factors contributing to negative patient feedback We then used the positive and negative comments as the training dataset for a neural network to perform sentiment analysis on sentences obtained by splitting mixed reviews Results We found that most of the comments were about doctors and nurses confirming the important role patients ascribed to these two in patient care Room discharge and tests and treatments were the three categories that had more negative than positive comments We then tabulated commonly appearing words adjectives and two-word combinations We found that climate control housekeeping and noise levels in the room time delays in discharge paperwork conflicting information about discharge plan frequent blood draws and needle sticks were major contributors to negative patient feedback None of this information was available from numeric data alone Conclusion NLP is an effective tool to gain insight from raw textual patient feedback to extract meaningful information making it a powerful tool in processing large amounts of patient feedback efficiently',NLP
'External knowledge is often useful for natural language understanding tasks We introduce a contextual text representation model called Conceptual-Contextual (CC) embeddings which incorporates structured knowledge into text representations Unlike entity embedding methods our approach encodes a knowledge graph into a context model CC embeddings can be easily reused for a wide range of tasks in a similar fashion to pre-trained language models Our model effectively encodes the huge UMLS database by leveraging semantic generalizability Experiments on electronic health records (EHRs) and medical text processing benchmarks showed our model gives a major boost to the performance of supervised medical NLP tasks',NLP
'Today deep learning is one of the most reliable and technically equipped approaches for developing more accurate speech recognition model and natural language processing (NLP) In this paper we propose Context-Dependent Deep Neural-network HMMs (CD-DNN-HMM) for large vocabulary Hindi speech using Kaldi automatic speech recognition toolkit Experiments on AMUAV database demonstrate that CD-DNN-HMMs outperform the conventional CD-GMM-HMMs model and provide the improvement in word error rate of 31% over conventional triphone model',NLP
'The written exam is a universal tool for evaluating student performance in the field of education The written exam provides a mechanism by which instructors and organizations ensure the consistency of the assessment process Human effort required for the assessment is very high and it depends on several factors such as knowledge of the teacher application level understanding of the teacher criteria of the marking and time allotted However traditional evaluation processes consume very costly efforts and take huge time for the completion of the complete evaluation verification and publishing of the result process This research introduces the design and implementation of Handwritten Answer Evaluation (HAES) system for student exam papers The HAES is an automatic response assessment system that enables the identification of text in answer sheets and can evaluate the grade of each answer based on the previous knowledge of the model In this study Optical Character Recognition (OCR) tool is used to extract the text from human written scanned answer script and machine learning/natural language processing (NLP) techniques are used for grading the answer sheets The scores are based on cosine set similarity measures where each sentence in the evaluated answer paper carries their corresponding mark The developed model can be used to evaluate the marks of the unscored descriptive answers',NLP
'Automated Compliance Checking (ACC) of building/construction projects is one of the important applications in Architecture Engineering and Construction (AEC) industry because it provides the checking processes and results of whether a building design complies with relevant laws policies and regulations Currently Automated Compliance Checking still involves lots of manual operations and massive time and cost consumption Additionally some sub-tasks of ACC have been researched while few studies can automatically implement the whole ACC process To solve related issues we proposed a semantic approach to implement the whole ACC process in an automated way Natural Language Processing (NLP) is used to extract rule terms and logic relationships among these terms from text regulatory documents Rule terms are mapped to keywords (concepts or properties) in BIM data through term matching and semantic similarity analysis After that according to the mapped keywords in BIM and logic relationships among keywords a corresponding SPARQL query is automatically generated The query results can be non-compliance or compliance with rules based on the generated SPARQL query and requirements of stakeholders The cases study proves that the proposed approach can provide a flexible and effective rule checking for BIM data In addition based on the proposed approach we also further develop a semantic framework to implement automated rule compliance checking in construction industry',NLP
'Data information and knowledge processing systems in the domain of healthcare are currently plagued by heterogeneity at various levels Current solutions have focused on developing a standard-based manual intervention mechanism which requires a large number of human resources and necessitates the realignment of existing systems State-of-the-art methodologies in the field of natural language processing and machine learning can help to partially automate this process reducing the resource requirements and providing a relatively good multi-class-based classification algorithm We present a novel methodology for bridging the gap between various healthcare data management solutions by leveraging the strength of transformer-based machine learning models to create mappings between the data elements Additionally the annotated data collected against five medical schemas and labeled by four annotators is made available for helping future researchers Our results indicate that for biased dependent multi-class text classification transformer-based models provide better results than linguistic and other classical models In particular the Robustly Optimized BERT Pretraining Approach (RoBERTa) provides the best schema matching performance by achieving a Cohens kappa score of 047 and Matthews Correlation Coefficient (MCC) score of 048 with human-annotated data',NLP
'Importance Case reports that externalize expert diagnostic reasoning are utilized for clinical reasoning instruction but are difficult to search based on symptoms final diagnosis or differential diagnosis construction Computational approaches that uncover how experienced diagnosticians analyze the medical information in a case as they formulate a differential diagnosis can guide educational uses of case reportsObjective To develop a reasoning-encoded  case database for advanced clinical reasoning instruction by applying natural language processing (NLP) a sub-field of artificial intelligence to a large case report libraryDesignWe collected 2525 cases from the New England Journal of Medicine (NEJM) Clinical Pathological Conference (CPC) from 1965 to 2020 and used NLP to analyze the medical terminology in each case to derive unbiased (not prespecified) categories of analysis used by the clinical discussant We then analyzed and mapped the degree of category overlap between casesResults Our NLP algorithms identified clinically relevant categories that reflected the relationships between medical terms (which included symptoms signs test results pathophysiology and diagnoses) NLP extracted 43291 symptoms across 2525 cases and physician-annotated 6532 diagnoses (both primary and related diagnoses) Our unsupervised learning computational approach identified 12 categories of medical terms that characterized the differential diagnosis discussions within individual cases We used these categories to derive a measure of differential diagnosis similarity between cases and developed a website () to allow visualization and exploration of 55 years of NEJM CPC case seriesConclusions Applying NLP to curated instances of diagnostic reasoning can provide insight into how expert clinicians correlate and coordinate disease categories and processes when creating a differential diagnosis Our reasoning-encoded CPC case database can be used by clinician-educators to design a case-based curriculum and by physicians to direct their lifelong learning efforts',NLP
'The lack of standardized structure names in radiotherapy (RT) data limits interoperability data sharing and the ability to perform big data analysis To standardize radiotherapy structure names we developed an integrated natural language processing (NLP) and machine learning (ML) based system that can map the physician-given structure names to American Association of Physicists in Medicine (AAPM) Task Group 263 (TG-263) standard names The dataset consist of 794 prostate and 754 lung cancer patients across the 40 different radiation therapy centers managed by the Veterans Health Administration (VA) Additionally data from the Radiation Oncology department at Virginia Commonwealth University (VCU) was collected to serve as a test set Domain experts identified as anatomically significant nine prostate and ten lung organs-at-risk (OAR) structures and manually labeled them according to the TG-263 standards and remaining structures were labeled as Non_OAR We experimented with six different classification algorithms and three feature vector methods and the final model was built with fastText algorithm Multiple validation techniques are used to assess the robustness of the proposed methodology The macro-averaged F(1)score was used as the main evaluation metric The model achieved an F(1)score of 097 on prostate structures and 099 for lung structures from the VA dataset The model also performed well on the test (VCU) dataset achieving an F(1)score of 093 for prostate structures and 095 on lung structures In this work we demonstrate that NLP and ML based approaches can used to standardize the physician-given RT structure names with high fidelity This standardization can help with big data analytics in the radiation therapy domain using population-derived datasets including standardization of the treatment planning process clinical decision support systems treatment quality improvement programs and hypothesis-driven clinical research',NLP
'Natural Language Processing (NLP) techniques have been successfully used to automatically extract information from unstructured text through a detailed analysis of their content often to satisfy particular information needs in this paper an automatic concept map construction technique Fuzzy Association Concept Mapping (FACM) is proposed for the conversion of abstracted short texts into concept maps The approach consists of a linguistic module and a recommendation module The linguistic module is a text mining method that does not require the use to have any prior knowledge about using NLP techniques It incorporates rule-based reasoning (RBR) and case based reasoning (CBR) for anaphoric resolution It aims at extracting the propositions in text so as to construct a concept map automatically The recommendation module is arrived at by adopting fuzzy set theories It is an interactive process which provides Suggestions of propositions for further human refinement of the automatically generated concept maps The suggested propositions are relationships among the concepts which are not explicitly found in the paragraphs This technique helps to stimulate individual reflection and generate new knowledge Evaluation was carried out by using the Science Citation Index (SCI) abstract database and CNET News as test data which are well known databases and the quality of the text is assured Experimental results show that the automatically generated concept maps conform to the outputs generated manually by domain experts since the degree of difference between them is proportionally small The method provides users with the ability to convert scientific and short texts into a structured format which can be easily processed by computer Moreover it provides knowledge workers with extra time to rethink their written text and to view their knowledge from another angle (c) 2008 Elsevier Ltd All rights reserved',NLP
'Background: Participant recruitment is a barrier to successful clinical research One strategy to improve recruitment is to conduct eligibility prescreening a resource-intensive process where clinical research staff manually reviews electronic health records data to identify potentially eligible patients Criteria2Query (C2Q) was developed to address this problem by capitalizing on natural language processing to generate queries to identify eligible participants from clinical databases semi-autonomouslyObjective: We examined the clinical research staffs perceived usability of C2Q for clinical research eligibility prescreeningMethods: Twenty clinical research staff evaluated the usability of C2Q using a cognitive walkthrough with a think-aloud protocol and a Post-Study System Usability Questionnaire On-screen activity and audio were recorded and transcribed After every-five evaluators completed an evaluation usability problems were rated by informatics experts and prioritized for system refinement There were four iterations of system refinement based on the evaluation feedback Guided by the Organizational Framework for Intuitive Human-computer Interaction we performed a directed deductive content analysis of the verbatim transcriptions Results: Evaluators aged from 24 to 46 years old (338; SD: 732) demonstrated high computer literacy (636; SD:017); female (75 %) White (35 %) and clinical research coordinators (45 %) C2Q demonstrated high usability during the final cycle (226 out of 7 [lower scores are better] SD: 074) The number of unique usability issues decreased after each refinement Fourteen subthemes emerged from three themes: seeking user goals performing well-learned tasks and determining what to do nextConclusions: The cognitive walkthrough with a think-aloud protocol informed iterative system refinement and demonstrated the usability of C2Q by clinical research staff Key recommendations for system development and implementation include improving system intuitiveness and overall user experience through comprehensive consideration of user needs and requirements for task completion',NLP
'IMPORTANCE: Case reports that externalize expert diagnostic reasoning are utilized for clinical reasoning instruction but are difficult to search based on symptoms final diagnosis or differential diagnosis construction Computational approaches that uncover how experienced diagnosticians analyze the medical information in a case as they formulate a differential diagnosis can guide educational uses of case reports OBJECTIVE: To develop a reasoning-encoded case database for advanced clinical reasoning instruction by applying natural language processing (NLP) a sub-field of artificial intelligence to a large case report library DESIGN: We collected 2525 cases from the New England Journal of Medicine (NEJM) Clinical Pathological Conference (CPC) from 1965 to 2020 and used NLP to analyze the medical terminology in each case to derive unbiased (not prespecified) categories of analysis used by the clinical discussant We then analyzed and mapped the degree of category overlap between cases RESULTS: Our NLP algorithms identified clinically relevant categories that reflected the relationships between medical terms (which included symptoms signs test results pathophysiology and diagnoses) NLP extracted 43291 symptoms across 2525 cases and physician-annotated 6532 diagnoses (both primary and related diagnoses) Our unsupervised learning computational approach identified 12 categories of medical terms that characterized the differential diagnosis discussions within individual cases We used these categories to derive a measure of differential diagnosis similarity between cases and developed a website (universeofcpccom ) to allow visualization and exploration of 55 years of NEJM CPC case series CONCLUSIONS: Applying NLP to curated instances of diagnostic reasoning can provide insight into how expert clinicians correlate and coordinate disease categories and processes when creating a differential diagnosis Our reasoning-encoded CPC case database can be used by clinician-educators to design a case-based curriculum and by physicians to direct their lifelong learning efforts',NLP
'BackgroundIdentifying patients at a high risk of psychosis relapse is crucial for early interventions A relevant psychiatric clinical context is often recorded in clinical notes; however the utilization of unstructured data remains limited This study aimed to develop psychosis-relapse prediction models using various types of clinical notes and structured data MethodsClinical data were extracted from the electronic health records of the Ajou University Medical Center in South Korea The study population included patients with psychotic disorders and outcome was psychosis relapse within 1 year Using only structured data we developed an initial prediction model then three natural language processing (NLP)-enriched models using three types of clinical notes (psychological tests admission notes and initial nursing assessment) and one complete model Latent Dirichlet Allocation was used to cluster the clinical context into similar topics All models applied the least absolute shrinkage and selection operator logistic regression algorithm We also performed an external validation using another hospital database ResultsA total of 330 patients were included and 62 (188%) experienced psychosis relapse Six predictors were used in the initial model and 10 additional topics from Latent Dirichlet Allocation processing were added in the enriched models The model derived from all notes showed the highest value of the area under the receiver operating characteristic (AUROC = 0946) in the internal validation followed by models based on the psychological test notes admission notes initial nursing assessments and structured data only (0902 0855 0798 and 0784 respectively) The external validation was performed using only the initial nursing assessment note and the AUROC was 0616 ConclusionsWe developed prediction models for psychosis relapse using the NLP-enrichment method Models using clinical notes were more effective than models using only structured data suggesting the importance of unstructured data in psychosis prediction',NLP
'Purpose The objective was to develop a natural language processing (NLP) algorithm to identify vaccine-related anaphylaxis from plain-text clinical notes and to implement the algorithm at five health care systems in the Vaccine Safety Datalink Methods The NLP algorithm was developed using an internal NLP tool and training dataset of 311 potential anaphylaxis cases from Kaiser Permanente Southern California (KPSC) We applied the algorithm to the notes of another 731 potential cases (423 from KPSC; 308 from other sites) with relevant codes (ICD-9-CM diagnosis codes for anaphylaxis vaccine adverse reactions and allergic reactions; Healthcare Common Procedure Coding System codes for epinephrine administration) NLP results were compared against a reference standard of chart reviewed and adjudicated cases The algorithm was then separately applied to the notes of 6 427 359 KPSC vaccination visits (9 402 194 vaccine doses) without relevant codes Results At KPSC NLP identified 12 of 16 true vaccine-related cases and achieved a sensitivity of 750% specificity of 985% positive predictive value (PPV) of 667% and negative predictive value of 990% when applied to notes of patients with relevant diagnosis codes NLP did not identify the five true cases at other sites When NLP was applied to the notes of KPSC patients without relevant codes it captured eight additional true cases confirmed by chart review and adjudication Conclusions The current study demonstrated the potential to apply rule-based NLP algorithms to clinical notes to identify anaphylaxis cases Increasing the size of training data including clinical notes from all participating study sites in the training data and preprocessing the clinical notes to handle special characters could improve the performance of the NLP algorithms We recommend adding an NLP process followed by manual chart review in future vaccine safety studies to improve sensitivity and efficiency',NLP
'The growth of social media has enabled the spread of tendentiously fake news content in a disorganized and fast manner Despite the extensive research on fake news detection methods algorithms and applications [1] most of the studies focused on Natural Language Processing (NLP) techniques for English content analysis Assessing online news in Portuguese language and ranking them based on their credibility is still an open issue Addressing this gap the present work proposes a platform for European Portuguese online news credibility classification and ranking based on content and publication metadata A dataset was built from the collection of online news from 36 sources comprising content-related data (title lead body) and supporting metadata (source category year author) A hierarchical architecture was envisioned and implemented for supervised binary classification probabilities extraction aggregated into multi-classes resorting to three Machine Learning models: i) focused on textual analysis resorting to several NLP techniques; ii) delving into the news publication metadata; and iii) leveraging the balance between the two prior models probability predictions This architecture minimizes data entropy maximizing information retrieved providing two credibility indexes and a fake news detector with 92% test accuracy and 95% deployed proof-of-concept accuracy These models were integrated into a web service enabling users to evaluate and rank online news resorting to the implemented architecture This solution has several application areas fostering societys literacy and increased critical thinking especially amongst younger generations',NLP
'In this article we present the result of the recent research in the recognition of events in Polish Event recognition plays a major role in many natural language processing applications such as question answering or automatic summarization We adapted Time ML specification (the well known guideline for English) to Polish language We annotated 540 documents in Polish Corpus of Wroclaw University of Technology (KPWr) using our specification Here we describe the results achieved by Liner2 (a machine learning toolkit) adapted to the recognition of events in Polish texts',NLP
'Entity Recognition detects all the entities present in a document to improve the performance of some high level Natural Language Processing (NLP) tasks like Question Answering Auto Summarization Machine Translation Information Extraction The task is subdivided into two parts: Parts of Speech Tagging (POS) and Entity Recognition Each sentence is annotated with part-of-speech tags and then the proper nouns are again classified with our own entity tag set This paper introduces Entity Recognition in Assamese Text using Conditional Random Fields (CRF) Results are measured with F-measure metric for each different entity class',NLP
'Recently society has expressed increased demands for translation of Japanese statutes into foreign languages The various motivations behind these demands include social and economic globalization and the need for technical assistance to legal reform In this paper we describe the problem of translating Japanese statues and show how to solve it by utilizing technologies developed for natural language processing In particular we show how to support both the compilation of a standard bilingual dictionary and the unification of translation equivalents of legal technical terms in compliance with the dictionary by using word alignment',NLP
'Physical misreading (as opposed to interpretational misreading) is an unnoticed substitution in silent reading Especially for legally important documents or instruction manuals this can lead to serious consequences We present a prototype of an automatic highlighter targeting words which can most easily be misread in a given text using a dynamic orthographic neighbour concept We propose measures of fit of a misread token based on Natural Language Processing and detect a list of short most easily misread tokens in the English language We design a highlighting scheme for avoidance of misreading',NLP
'Translation studies rely more and more on corpus data to examine specificities of translated texts that can be translated from different original languages and compared to original texts In parallel more and more multilingual corpora are becoming available for various natural language processing tasks This paper questions the use of these multilingual corpora in translation studies and shows the methodological steps needed in order to obtain more reliably comparable sub-corpora that consist of original and directly translated text only Various experiments are presented that show the advantage of directional sub-corpora',NLP
'The pressure on systems engineering is ever-increasing to support the development and implementation of systems that meet a complex environments demands As a growing discipline systems engineering requires insight into past research to identify opportunities for future growth Analyzing the bibliometric data on published research provides valuable information on a scientific disciplines past progress and future prospects Therefore this paper extracts the research topics published in INCOSEs journal Systems Engineering and the annual international symposium proceedings to analyze their composition and allocation to papers The implemented process applies natural language processing and topic modeling to extract the main topics from these papers titles and abstracts Analyzing these research topics composition and mapping them to processed articles helps to understand their relative importance The analysiss output confirms the importance of modeling in systems engineering as it is the most popular topic The additional focus of research papers on the systems engineering process practice and methodologies also indicates that the field is still growing and evolving Some important topics to systems engineering which were not found as prominent topics are humans roles in systems verification and validation and other specialty fields This new knowledge about the structure of research into systems engineering can identify future research project opportunities to continue growing the field',NLP
'The use of offensive terms in user-generated content on different social media platforms is one of the major concerns for these platforms The offensive terms have a negative impact on individuals which may lead towards the degradation of societal and civilized manners The immense amount of content generated at a higher speed makes it humanly impossible to categorise and detect offensive terms Besides it is an open challenge for natural language processing (NLP) to detect such terminologies automatically Substantial efforts are made for high-resource languages such as English However it becomes more challenging when dealing with resource-poor languages such as Urdu Because of the lack of standard datasets and pre-processing tools for automatic offensive terms detection This paper introduces a combinatorial preprocessing approach in developing a classification model for cross-platform (Twitter and YouTube) use The approach uses datasets from two different platforms (Twitter and YouTube) the training and testing the model which is trained to apply decision tree random forest and naive Bayes algorithms The proposed combinatorial pre-processing approach is applied to check how machine learning models behave with different combinations of standard pre-processing techniques for low-resource language in the cross-platform setting The experimental results represent the effectiveness of the machine learning model over different subsets of traditional pre-processing approaches in building a classification model for automatic offensive terms detection for a low resource language ie Urdu in the cross-platform scenario In the experiments when dataset D1 is used for training and D2 is applied for testing the pre-processing approach named Stopword removal produced better results with an accuracy of 8327% Whilst in this case when dataset D2 is used for training and D1 is applied for testing stopword removal and punctuation removal were observed as a better preprocessing approach with an accuracy of 7454% The combinatorial approach proposed in this paper outperformed the benchmark for the considered datasets using classical as well as ensemble machine learning with an accuracy of 829% and 972% for dataset D1 and D2 respectively',NLP
'Images are core components of multi-modal learning in natural language processing (NLP) and results have varied substantially as to whether images improve NLP tasks or not One confounding effect has been that previous NLP research has generally focused on sophisticated tasks (in varying settings) generally applied to English only We focus on text classification in the context of assigning named entity classes to a given Wikipedia page where images generally complement the text and the Wikipedia page can be in one of a number of different languages Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training but when combined with BERT models pre-trained on largescale external data images contribute nothing',NLP
'As all natural languages are subject to change over time and as the Web becomes more prevalent it also constitutes a major source for identifying language evolution Although these changes affect all linguistic branches ranging from phonetics lexicon and grammar to semantics and pragmatics we will focus only on discovering new potential words that entered the Romanian lexicon or alternative forms (lexicalizations) that are frequently used In this paper we describe the architecture of a system which models the rate of Romanian vocabulary growth based on different statistics gathered by a focused web crawler In order to validate the proposed system the paper also presents the main new words identified by the system in online texts written in Romanian',NLP
'In natural language processing applications such as those related to question answering systems and more specifically to semantic role labelling an important task to perform during the text normalization phase is lemmatization which consists in determining those two words which have the same root despite their surface differences Due to the lack of a practical lemmatizing tool suitable for the Italian language (which is a highly inflectional one) in this paper we aim to present LIT a Rule based Italian lemmatizer consisting of a full rule-base lemmatization of all dictionary-words and a discovery algorithm which attempts to predict the grammar of neologisms This is followed by a practical application of LIT on Europarl v7 a well-known open corpus',NLP
'Background: Natural language processing (NLP) applications are increasingly important in biomedical data analysis knowledge engineering and decision support Concept recognition is an important component task for NLP pipelines and can be either general-purpose or domain-specific We describe a novel flexible and general-purpose concept recognition component for NLP pipelines and compare its speed and accuracy against five commonly used alternatives on both a biological and clinical corpus NOBLE Coder implements a general algorithm for matching terms to concepts from an arbitrary vocabulary set The systems matching options can be configured individually or in combination to yield specific system behavior for a variety of NLP tasks The software is open source freely available and easily integrated into UIMA or GATE We benchmarked speed and accuracy of the system against the CRAFT and ShARe corpora as reference standards and compared it to MMTx MGrep Concept Mapper cTAKES Dictionary Lookup Annotator and cTAKES Fast Dictionary Lookup Annotator Results: We describe key advantages of the NOBLE Coder system and associated tools including its greedy algorithm configurable matching strategies and multiple terminology input formats These features provide unique functionality when compared with existing alternatives including state-of-the-art systems On two benchmarking tasks NOBLEs performance exceeded commonly used alternatives performing almost as well as the most advanced systems Error analysis revealed differences in error profiles among systems Conclusion: NOBLE Coder is comparable to other widely used concept recognition systems in terms of accuracy and speed Advantages of NOBLE Coder include its interactive terminology builder tool ease of configuration and adaptability to various domains and tasks NOBLE provides a term-to-concept matching system suitable for general concept recognition in biomedical NLP pipelines',NLP
'The spread of sensors and intelligent devices of the Internet of Things and their integration in daily environments are changing the way we interact with some of the most common objects in everyday life Therefore there is an evident need to provide nonexpert users with the ability to customize in a simple but effective way the behaviour of these devices based on their preferences and habits This paper presents RuleBot a conversational agent that uses machine learning and natural language processing techniques to allow end users to create automations according to a flexible implementation of the trigger-action paradigm and thereby customize the behaviour of devices and sensors using natural language In particular the paper describes the design and implementation of RuleBot and reports on a user test and lessons learnt',NLP
'The prevalence of misinformation both online and offline has prompted a great demand of fact verification Table-based fact verification aims to check whether a textual claim is supported or refuted based on relational tables However most of the existing approaches are in a closed-domain setting which may not be realistic in practice To address this problem in this paper we introduce OPENTFV a user-friendly system that supports open domain table-based fact verification Given a claim input by an end-user OPENTFV retrieves the relevant tables and provides a verification result for each table with an intuitive interpretation in natural language We have implemented OPENTFV and demonstrated OPENTFV in two representative scenarios COVID-19 claims fact verification based on academic tables and general fact verification on Wiki-tables',NLP
'For years recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks However the main drawback of RvNNs is that they require structured input which makes data preparation and model implementation hard In this paper we propose Gumbel Tree-LSTM a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision We evaluate the proposed model on natural language inference and sentiment analysis and show that our model outperforms or is at least comparable to previous models We also find that our model converges significantly faster than other models',NLP
'It is crucial to design products bearing security in mind from the initial development stage Consequently many threat analysis support tools have been developed However it is difficult to determine the inherent threats in various designed documents written in natural language which is used in the initial development stage It is not uncommon to find attacks that closely resemble past attacks In addition many designs are limited in the number of data they can handle We propose a method of extracting existing vulnerabilities similar to those used in the attack by collating a large vulnerability database with existing attack cases using Latent Dirichlet Allocation one of the topic model methods We apply the proposed method to several cases and verify its effectiveness',NLP
'Recognizing Textual Entailment (RTE) is a Natural Language Processing task to determine whether a sentence (text) semantically entails another sentence (hypothesis) In this paper we extracted and learned 35 features from a pair of text and hypothesis in Indonesian The ablation study was conducted to analyze features contribution to RTE model The experiments shown that using Support Vector Machine (SVM) and Logistic Regression the token-based features contribute positively to improve the model performance The best model in our experiment is SVM that scored Fl-Score of 7965% Despite sacrificing 5-points accuracy to the state-of-the-art BERT model SVM classifier is 31 hours more efficient in terms of training time (C) 2021 The Authors Published by Elsevier BV',NLP
'Using adversarial examples as test cases is an effective way to detect the security and reliability problems of Deep Neural Networks (DNNs) Genetic Algorithm (GA) performs well in generating Natural Language Processing (NLP) adversarial examples due to its excellent ability to search for optimal solutions However the existing GAs need substantial time to generate NLP adversarial examples Moreover the semantics of examples generated by GAs is quite different from real texts which makes it impossible to generate efficient and reliable adversarial examples To address these two problems this paper proposes a Heuristic-word-selection Genetic Algorithm (HGA) to generate NLP adversarial examples Initial experimental results indicate that HGA not only improves the efficiency of GA to generate adversarial examples but also enhances the preservation of text semantics while maintaining the existing GAs advantages',NLP
'When software engineers set out to build a system they usually have the informal idea that there is a relation between the linguistic world and the conceptual world In this paper we present a formalisation of this empirical relation defining an intermediate mapping of the components of the linguistic and conceptual worlds to their mathematical representations This process outputs a justified correspondence between natural language used as a means of communication between users and software engineers and conceptual models employed by software engineers as a first step towards building a system In other words our aim is to show how the equivalence of the linguistic and conceptual representations of a requirement can be established in a formal and justified manner (C) 2000 Elsevier Science BV All rights reserved',NLP
'In scientific literature and industry semantic and context-aware Natural Language Processing-based solutions have been gaining importance in recent years The possibilities and performance shown by these models when dealing with complex Human Language Understanding tasks are unquestionable from conversational agents to the fight against disinformation in social networks In addition considerable attention is also being paid to developing multilingual models to tackle the language bottleneck An increase in size has accompanied the growing need to provide more complex models implementing all these features without being conservative in the number of dimensions required This paper aims to provide a comprehensive account of the impact of a wide variety of dimensional reduction techniques on the performance of different state-of-the-art multilingual siamese transformers including unsupervised dimensional reduction techniques such as linear and nonlinear feature extraction feature selection and manifold techniques In order to evaluate the effects of these techniques we considered the multilingual extended version of Semantic Textual Similarity Benchmark (mSTSb) and two different baseline approaches one using the embeddings from the pre-trained version of five models and another using their fine-tuned STS version The results evidence that it is possible to achieve an average reduction of 9158% +/- 259% in the number of dimensions of embeddings from pre-trained models requiring a fitting time 9668% +/- 068% faster than the fine-tuning process Besides we achieve 5465% +/- 3220% dimensionality reduction in embeddings from fine-tuned models The results of this study will significantly contribute to the understanding of how different tuning approaches affect performance on semantic-aware tasks and how dimensional reduction techniques deal with the high-dimensional embeddings computed for the STS task and their potential for other highly demanding NLP tasks',NLP
'In recent years the Natural Language Processing scene has witnessed the steady growth of interest in connectionist modeling The main appeal of such an approach is that one does not have to determine the grammar rules in advance: the learning abilities displayed by such systems take care of input regularities Better and faster learning can be obtained through the implementation of a symbolic-connectionist hybrid system Such system combines the advantages of symbolic approaches by introducing symbolic rules as network connection weights with the advantages of connectionism In a hybrid system called HTRP words within a sentence are represented by means of semantic features The features for the verbs are arranged along certain semantic dimensions and are mutually exclusive within each dimension One may infer that this happens because of the semantic features encoded in the network inputs',NLP
'With the capability of modeling lighter MLP-based models like the pNLP-Mixer and the HyperMixer demonstrate the potential for diverse tasks in NLP However these linguistic models are not optimized for the regularity of textual hierarchical abstraction Here this paper proposes the hidden bias attention (HBA) a novel attention mechanism that is lighter than self-attention and focuses on extracting hidden (topic) semanteme Additionally this paper introduces a series of lightweight deep learning architectures HBA-Mixer based on HBA and MHBA-Mixers based on multi-head HBA which both outperforms pNLP-Mixer and HyperMixer in accuracy with fewer parameters on 3 tasks including text classification natural language inference and sentiment analysis Compared with large pre-trained models MHBA-Mixers achieve over 90% of their accuracy with one-thousandth of the parameters',NLP
'Chatbot is a computer application that interacts with users using natural language in a similar way to imitate a human travel agent A successful implementation of a chatbot system can analyze user preferences and predict collective intelligence In most cases it can provide better user-centric recommendations Hence the chatbot is becoming an integral part of the future consumer services This paper is an implementation of an intelligent chatbot system in travel domain on Echo platform which would gather user preferences and model collective user knowledge base and recommend using the Restricted Boltzmann Machine (RBM) with Collaborative Filtering With this chatbot based on DNN we can improve human to machine interaction in the travel domain',NLP
'Automated part-of-speech (POS) tagging has been a very active research area for many years and is the foundation of natural language processing systems Natural Language Toolkit (NLTK) library in the Python environment provides the necessary tools for tagging but doesnt actually tell us what methods work the best Therefore this work analyzes the performance of part-of-speech taggers namely the NLTK Default tagger Regex tagger and N-gram taggers (Unigram Bigram and Trigram) on a particular corpus The corpora we have used for the analysis are; Brown Penn Treebank and CoNLL2000 We have applied all taggers to these three corpora resultantly we have shown that whereas Unigram tagger does the best tagging in all corpora the combination of taggers does better if it is correctly ordered',NLP
'Design intent is a kind of design knowledge which is contributive in 3D model re-use 3D model retrieval etc Capturing presenting and storing design intent still remains a challenge In this paper we propose an information model to explicitly presenting design intents which are embedded in the collaborative designing process and in the ultimate complex artifact from multiple dimensions To support the information model a meaningful design operation sequence discovering method is facilitated along with a structured organization of design operations To provide readable and understandable design intents presentation a natural language generation method is proposed as well Finally the proposed methods are tested in a prototype system with case study',NLP
'With recent regulatory advances modern enterprises have to not only comply with regulations but have to be prepared to provide explanation of proof of (non-)compliance On top of compliance checking this necessitates modeling concepts from regulations and enterprise operations so that stakeholder-specific and close to natural language explanations could be generated We take a step in this direction by using Semantics of Business Vocabulary and Rules to model and map vocabularies of regulations and operations of enterprise Using these vocabularies and leveraging proof generation abilities of an existing compliance engine we show how such explanations can be created Basic natural language explanations that we generate can be easily enriched by adding requisite domain knowledge to the vocabularies',NLP
'Knowledge graph (KG) has been fully considered in natural language generation (NLG) tasks A KG can help models generate controllable text and achieve better performance However most existing related approaches still lack explainability and scalability in large-scale knowledge reasoning In this work we propose a novel CogNLG framework for KG-to-text generation tasks Our CogNLG is implemented based on the dual-process theory in cognitive science It consists of two systems: one system acts as the analytic system for knowledge extraction and another is the perceptual system for text generation by using existing knowledge During text generation CogNLG provides a visible and explainable reasoning path Our framework shows excellent performance on all datasets and achieves a BLEU score of 367 which increases by 67 compared to the best competitor',NLP
'Visual Question Answering (VQA) is the multitask research field of computer vision and natural language processing and is one of the most intelligent applications among machine learning applications at present It firstly analyzes and copes with the problem sentences to extract the core key words as well as then seeking out the answers from the figure In our research it extracts characteristic values from problem sentences and images by adopting the BI-LSTM and VGG_19 algorithms Then after integrating the values into new feature vectors the paper correlates them into the attention through the attention mechanism and finally predicts the answers finally Also the VQA10 data set is adopted to train the model After conducting the training the accuracy of the test by using the test set reached up to 548%',NLP
'We present a general solution towards building task-oriented dialogue systems for online shopping aiming to assist online customers in completing various purchase-related tasks such as searching products and answering questions in a natural language conversation manner As a pioneering work we show what & how existing natural language processing techniques data resources and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage To demonstrate its effectiveness we integrate our system into a mobile online shopping application To the best of our knowledge this is the first time that an dialogue system in Chinese is practically used in online shopping scenario with millions of real consumers Interesting and insightful observations are shown in the experimental part based on the analysis of human-bot conversation log Several current challenges are also pointed out as our future directions',NLP
'Parsing and Tagging are very important tasks in Natural Language Processing Parsing amounts to searching the correct combination of grammatical rules among those compatible with a given sentence Tagging amounts to labeling each word in a sentence with its lexical category and because many words belong to more than one lexical class it turns out to be a disambiguation task Because parsing and tagging are related tasks its simultaneous resolution can improve the results of both of them This work aims developing a multiobjective genetic program to perform simultaneously statistical parsing and tagging It combines the statistical data about grammar rules and about tag sequences to guide the search of the best structure Results show that any of the implemented multiobjective optimization models improve on the results obtained in the resolution of each problem separately',NLP
'Natural Language is a mean to express and discuss about concepts objects events ie it carries semantic contents One of the ultimate aims of Natural Language Processing techniques is to identify the meaning of the text providing effective ways to make a proper linkage between textual references and their referents that is real world objects This work addresses the problem of giving a sense to proper names in a text that is automatically associating words representing Named Entities with their referents The proposed methodology for Named Entity Disambiguation is based on Semantic Relatedness Scores obtained with a graph based model over Wikipedia We show that without building a Bag of Words representation of the text but only considering named entities within the text the proposed paradigm achieves results competitive with the state of the art on two different datasets',NLP
'Automatic sentence segmentation of spoken language is an important precursor to downstream natural language processing Previous studies combine lexical and prosodic features but can impose significant computational challenges because of the large size of feature sets Little is understood about which features most benefit performance particularly for speech data from different speaking styles We compare sentence segmentation for speech from broadcast news versus natural multi-party meetings using identical lexical and prosodic feature sets across genres Results based on boosting and forward selection for this task show that (1) features sets can be reduced with little or no loss in performance and (2) the contribution of different feature types differs significantly by genre We conclude that more efficient approaches to sentence segmentation and similar tasks can be achieved especially if genre differences are taken into account',NLP
'Existing website fingerprinting techniques are not effective with video streaming traffic when the encrypted traffic contains multiple streams This paper presents a deep learning-based source identification method for identifying multiple video sources within a single encrypted tunnel The core contribution is a novel feature inspired by natural language processing (NLP) that allows existing NLP techniques to identify the source The feature extraction method is described A large dataset containing video streaming and web traffic is created to verify its effectiveness Results are obtained by applying several NLP methods to show that the proposed method performs well on both binary and multilabel traffic classification problems The work proves that the method can overcome the challenges given by mixed-traffic tunnels',NLP
'Natural Language Generation (NLG) is one of the most important part in Natural Language Processing (NLP) Recently generating text automatically with deep learning method has been improved a lot While there are lots of defects in text generation such as the quality is not satisfied and the text of title is not clear The paper used the recurrent convolution attention model with LSTM (Long Short-Term Memory) cells for text generation by giving a title The result proved that it can generate sentence according with the title and make the text express more fluently Moreover it uses less time to train by contrast with the SeqGAN (Sequence Generative Adversarial Networks) At the same time the result is better than other attention mechanism with LSTM models Therefore it has more significance for NLP research',NLP
'Named Entity Recognition(NER) is a very important part of many Natural Language Processing(NLP) tasks but the accuracy rate of NER has not reached our expectation especially in Chinese Therefore we studied NER task in Chinese social media Compared to the previous papers on this dataset we propose a new network structure that greatly improve the recognition performance At the same time we noticed that the recall rate of their experimental results is much lower than the precision rate so we also explored a method to mitigate this situation by changing the loss function Finally the experimental results of ours with the new loss function obtained not only higher recall rates but also significant speedup in training phase compared to the state-of-the-art methods',NLP
'Software stakeholders who do not have a technical profile (ie users clients) but do want to take part in the development and/or quality assurance process of software have an unmet need for communication on what is being tested during the development life-cycle The transformation of test properties and models into semi-natural language representations is one way of responding to such need Our research has demonstrated that these transformations are challenging but feasible and they have been implemented into a prototype tool called readSpec The readSpec tool transforms universally-quantified test properties and stateful test models - the two kinds of test artifacts used in property-based testing - into plain text interpretations The tool has been successfully evaluated on the PBT artifacts',NLP
'Text Mining is one of the computational intelligence research areas The main goal of text mining tool is to discover knowledge which is embedded in unstructured text The first step of text mining is to extract fact from the texts However to build a robust text mining tool is very complex The first step requires the tool to process a natural language The major challenging issue in any natural languages is the ambiguity problem The problem may occur at lexical and phrase levels This paper addresses ambiguity problem which occur in the preposition phrase and presents a new technique for resolving the problem The technique has been developed by applying possibility theory fuzzy set and context knowledge The technique has been implemented and tested using a set of test cases and promising results are obtained',NLP
'Over the past 15 years a range of methods have been developed that are able to learn human-like estimates of the semantic relatedness between terms from the way in which these terms are distributed in a Corpus Of unannotated natural language text These methods have also been evaluated in a number of applications in the cognitive science computational linguistics and the information retrieval literatures In this paper we review the available methodologies for derivation of semantic relatedness front free text as well as their evaluation in a variety of biomedical and Other applications Recent methodological developments and their applicability to several existing applications are also discussed (C) 2009 Elsevier Inc All rights reserved',NLP
'In a broadcasting station it is important to establish an environment that facilitates the reuse of TV programs and legacy materials Therefore it is necessary to build efficient video archives that allow for the accumulation of a large number of TV programs A video skimming system is very useful because it helps us quickly grasp the contents of accumulated TV programs in video archives In this paper we propose a video skimming method based on information about a TV program which have natural language annotations that describe the contents of TV programs Using these annotations we can generate skimmed TV programs by extracting representative video scenes We performed a series of skimming experiments for TV programs and obtained good results',NLP
'This paper introduces a novel application of information extraction techniques to extract data from he maintenance records to populate a database The goals of the research are to preprocess the text-based data or further use in data mining efforts and to develop a system to provide a rough analysis of generic maintenance records to facilitate in the development of training corpora for use in machine-learning for more refined information extraction system design The Natural Language Toolkit was used to implement partial parsing of text by way of hierarchical chunking of the text The system was targeted towards inspection descriptions and succeeded in extracting the inspection code description of the part/action and date/time information with 80 7% recall and 89 9% precision',NLP
'Multiword Expressions (MWEs) are a frequently occurring phenomenon found in all natural languages that is of great importance to linguistic theory natural language processing applications and machine translation systems Neural Machine Translation (NMT) architectures do not handle these expressions well and previous studies have rarely addressed MWEs in this framework In this work we show that annotation and data augmentation using external linguistic resources can improve both translation of MWEs that occur in the source and the generation of MWEs on the target and increase performance by up to 509 BLEU points on MWE test sets We also devise a MWE score to specifically assess the quality of MWE translation which agrees with human evaluation We make available the MWEscore implementation - along with MWE-annotated training sets and corpus-based lists of MWEs - for reproduction and extension',NLP
'In this paper we present our method for automatically extracting narrative information of characters and their narrative roles from natural language stories In our corpus of 15 unannotated folk tales our Voz system identifies 87% of the characters in the stories and correctly assigns 68% of the character roles To better understand the sources of error in our system we present an analytical methodology to study how the error is introduced by different modules and how it propagates through the pipeline This methodology allows us to identify the bottleneck with the largest impact on the final error which might be different from the module with the largest individual error in isolation Our methodology can be applied to a wide variety of similar information extraction pipelines',NLP
'The spread of information on Facebook and Twitter is much more efficient than on traditional social media platforms For word-of-mouth (WOM) marketing social media have become a rich information source for companies or scholars to design models to examine this repository and mine useful insights for marketing strategies However social media language is relatively short and contains special words and symbols Most natural language processing (NLP) methods focus on processing formal sentences and are not well-suited to such short messages In this study we propose a novel sentiment analysis framework based on deep learning models to extract sentiment from social media We collect data from which we compile a dataset After processing these special terms we seek to establish a semantic dataset for further research The extracted information will be useful for many future applications The experimental data have been obtained by crawling several social media platforms',NLP
'Sentiment analysis classification models trained using neural networks require large amounts of data but collecting these datasets requires significant time and resources Although artificial data has been used successfully in computer vision there are few effective and generalizable methods for creating artificial augmented text data In this paper a text based data augmentation method is proposed called back-and-forth translation that can be used to artificially increase the size of any natural language dataset By creating augmented text data and adding it to the original dataset it is demonstrated by empirical experiments that back-and-forth translation data augmentation can reduce the error rate in binary sentiment classification models by up to 34% These results are shown to be statistically significant',NLP
'In this paper we focus on the architecture of the convolutional neural network (CNN) for sentence classification For understanding natural language context in the sentence is important information for grasping the word sense However traditional CNNs feed-forward architecture is insufficient to reflect this factor To solve this limitation we propose a contextual CNN (C-CNN) for better text understanding by adding recurrent connection to the convolutional layer This architecture helps C-CNN units to be modulated over time with their neighboring units thus the model integrates word meanings with surrounding information within the same layer We evaluate our model on sentence-level sentiment prediction tasks and question categorization task The C-CNN achieves state-of-the-art performances on fine-grained sentiment prediction and question categorization',NLP
'Natural language dialogue is an important component of interaction between ordinary users and complex computer applications Short Text Semantic Similarity algorithms have been developed to improve the efficiency of producing sophisticated dialogue systems Such algorithms are currently unable to discriminate between different dialogue acts (assertions questions instructions etc) requiring the addition of efficient dialogue act classifiers to enhance them The Slim Function Word Classifier (SFWC) has proved promising particularly in its computational simplicity This study optimizes the SFWC by clustering function word features using grammatical principles Experiments show a significant improvement in classification accuracy for a selection of sentence forms which were challenging for the unoptimized SFWC Results are expected to be applicable to many intelligent text processing applications ranging from question answering to meeting summarization',NLP
'Research on electoral events in conjunction with social media provides opportunities to describe an interesting phenomenon that can be analyzed using sentiment analysis techniques The goal of the study is to analyze the support of political parties during electoral periods from Twitter comments including 250 000 tweets regarding the Spanish general elections of 2015 and 2016 respectively Text mining and natural language processing techniques enable information analysis and the methodology emphasizes good practices for large-scale data collection retrieved from Twitter through a quantitative analysis of text collection written in the Spanish language After information extraction obtained in three Spanish regions defined by geolocation as well as feature selection based on keywords of the main four political parties we conducted an in-depth examination of Twitter users support during the course of the election By weighting the tendency of tweets we were able to obtain a proposed indicator of support: the positiveness ratio (PR) The results suggest that PR is a feasible barometer to demonstrate the measurable patterns of support tendency regarding political parties and users behavioral activity to track their affinity on Twitter The findings indicate consistent support behavior by users toward traditional parties and optimistic users behavior regarding emerging political parties',NLP
'Semantic image annotation is a field of paramount importance in which deep learning excels However some application domains like security or medicine may need an explanation of this annotation Explainable Artificial Intelligence is an answer to this need In this work an explanation is a sentence in natural language that is dedicated to human users to provide them clues about the process that leads to the decision: the labels assignment to image parts We focus on semantic image annotation with fuzzy logic that has proven to be a useful framework that captures both image segmentation imprecision and the vagueness of human spatial knowledge and vocabulary In this paper we present an algorithm for textual explanation generation of the semantic annotation of image regions',NLP
'Humans are remarkably efficient at processing natural text We quantified efficiency for discriminating a sample of meaningful text from a sample of random text by disrupting the meaningful sample and measuring how much disruption human readers can tolerate before the two samples become indistinguishable We performed these measurements for a wide range of conditions involving samples of different lengths and containing letters words or Chinese characters We then compared human performance to the best possible performance achieved by a Bayesian estimator under the conditions in which we tested our participants and in so doing we determined their absolute efficiency Values were mostly in the range 5-40% In agreement with reported efficiencies for many visual tasks Although not intended as a veridical model of human processing we found that the Bayesian model captured some (but not all) aspects of how humans classified text in our tasks and conditions (C) 2010 Elsevier Ltd All rights reserved',NLP
'A common tool for improving the performance quality of natural language processing systems is the use of contextual information for disambiguation Here I describe the use of a finite state machine (FSM) to disambiguate speech acts in a machine translation system The FSM has two layers that model respectively the global and local structures found in naturally-occurring conversations The FSM has been modeled on a corpus of task-oriented dialogues in a travel planning situation In the dialogues one of the interactants is a travel agent or hotel clerk and the other a client requesting information or services A discourse processor based on the FSM was implemented in order to process contextual information in a machine translation system Evaluation results show that the discourse processor is able to disambiguate and improve the quality of the dialogue translation Other applications include human-computer interaction and computer-assisted language learning',NLP
'Generating a natural language description from an image is an emerging interdisciplinary problem at the intersection of computer vision natural language processing and artificial intelligence ( AI) This task often referred to as image or visual captioning forms the technical foundation of many important applications such as semantic visual search visual intelligence in chatting robots photo and video sharing in social media and aid for visually impaired people to perceive surrounding visual content Thanks to the recent advances in deep learning the AI research community has witnessed tremendous progress in visual captioning in recent years In this article we will first summarize this exciting emerging visual captioning area We will then analyze the key development and the major progress the community has made their impact in both research and industry deployment and what lies ahead in future breakthroughs',NLP
'With the Web popularization the digital inclusion comes motivating the research and development of engines of more sophisticated search each time concentrating in the concepts of the Web Semantics that incorporates semantic elements in the formatting of their concepts and propitiates a search with bigger intelligence Such intelligence characterizes for the quality or precision of the knowledge retrieved or returned in reply to one determined request involving a structure of how the knowledge is represented accessed retrieved manipulated and enriched The intention of this process is to enable to the executive professionals to interact naturally with computers using the natural language where the computers understand the submitted questions it and answer using the same terms of the question in understandable way guiding them in complex decisions allowing that they can occupy of nobler tasks the businesses of their company',NLP
'Introduction: Research related to the automatic detection of Alzheimers disease (AD) is important given the high prevalence of AD and the high cost of traditional diagnostic methods Since AD significantly affects the content and acoustics of spontaneous speech natural language processing and machine learning provide promising techniques for reliably detecting AD There has been a recent proliferation of classification models for AD but these vary in the datasets used model types and training and testing paradigms In this study we compare and contrast the performance of two common approaches for automatic AD detection from speech on the same well-matched dataset to determine the advantages of using domain knowledge vs pre-trained transfer models Methods: Audio recordings and corresponding manually-transcribed speech transcripts of a picture description task administered to 156 demographically matched older adults 78 with Alzheimers Disease (AD) and 78 cognitively intact (healthy) were classified using machine learning and natural language processing as AD or non-AD The audio was acoustically-enhanced and post-processed to improve quality of the speech recording as well control for variation caused by recording conditions Two approaches were used for classification of these speech samples: (1) using domain knowledge: extracting an extensive set of clinically relevant linguistic and acoustic features derived from speech and transcripts based on prior literature and (2) using transfer-learning and leveraging large pre-trained machine learning models: using transcript-representations that are automatically derived from state-of-the-art pre-trained language models by fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models Results: We compared the utility of speech transcript representations obtained from recent natural language processing models (ie BERT) to more clinically-interpretable language feature-based methods Both the feature-based approaches and fine-tuned BERT models significantly outperformed the baseline linguistic model using a small set of linguistic features demonstrating the importance of extensive linguistic information for detecting cognitive impairments relating to AD We observed that fine-tuned BERT models numerically outperformed feature-based approaches on the AD detection task but the difference was not statistically significant Our main contribution is the observation that when tested on the same demographically balanced dataset and tested on independent unseen data both domain knowledge and pretrained linguistic models have good predictive performance for detecting AD based on speech It is notable that linguistic information alone is capable of achieving comparable and even numerically better performance than models including both acoustic and linguistic features here We also try to shed light on the inner workings of the more black-box natural language processing model by performing an interpretability analysis and find that attention weights reveal interesting patterns such as higher attribution to more important information content units in the picture description task as well as pauses and filler words Conclusion: This approach supports the value of well-performing machine learning and linguistically-focussed processing techniques to detect AD from speech and highlights the need to compare model performance on carefully balanced datasets using consistent same training parameters and independent test datasets in order to determine the best performing predictive model',NLP
'Reuse is a major goal of modern software engineering because it is considered the key to improving the quality of software and productivity Using formal specifications to represent software components facilitates the determination of reusable software because they more precisely characterize the functionality of the software and the well-defined syntax makes processing amenable to automation In the present work a hybrid model based on natural language and formal specifications using K-nn technique has been proposed Benefits of both formal methods and natural language have been exploited in the retrieval of reusable software components Existing components are weighted according to their degree of similarity on basis of certain attributes to the required component A k-nn based methodology is used for retrieval of similar components from the library',NLP
'In this paper we describe the process of converting Tunisian Dialect text that is written in Latin script (also called Arabizi) into Arabic script following the CODA orthography convention for Dialectal Arabic Our input consists of messages and comments taken from SMS social networks and broadcast videos The language used in social media and SMS messaging is characterized by the use of informal and non-standard vocabulary such as repeated letters for emphasis typos non-standard abbreviations and nonlinguistic content such as emoticons There is a high degree of variation is spelling in Arabic dialects due to the lack of orthographic widely supported standards in both Arabic and Latin scripts In the context of natural language processing transliterating from Arabizi to Arabic script is a necessary step since most recently available tools for processing Arabic Dialects expect Arabic script input',NLP
'Robotic systems for chemical synthesis are growing in popularity but can be difficult to run and maintain because of the lack of a standard operating system or capacity for direct access to the literature through natural language processing Here we show an extendable chemical execution architecture that can be populated by automatically reading the literature leading to a universal autonomous workflow The robotic synthesis code can be corrected in natural language without any programming knowledge and because of the standard is hardware independent This chemical code can then be combined with a graph describing the hardware modules and compiled into platform-specific low-level robotic instructions for execution We showcase automated syntheses of 12 compounds from the literature including the analgesic lidocaine the Dess-Martin periodinane oxidation reagent and the fluorinating agent AlkylFluor',NLP
'Machine Comprehension (MC) is one of the core problems in natural language processing requiring both understanding of the natural language and knowledge about the world Rapid progress has been made since the release of several benchmark datasets and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation In this paper we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset',NLP
'Anaphora resolution is a central topic in dialogue and discourse that deals with finding the referent of a pronoun It plays a critical role in conversational Intelligent Tutoring Systems (ITSs) as it can increase the accuracy of assessing students knowledge level ie mental model based on their natural language inputs Although anaphora resolution is one of the most studied problems in Natural Language Processing there are very few studies that focus on anaphora resolution in dialogue based ITSs To this end we present Deep Anaphora Resolution Engine++ (DARE++) that adapts and extends existing machine learning solutions to resolve pronouns in ITS dialogues Experiments showed that DARE++ achieves a F-measure of 8893% proving the potential of the proposed method for resolving pronouns in student-tutor dialogues',NLP
'Developments in Biology Economy Information Technology Social studies etc have introduced and acknowledged the understanding that the whole world consists of Information Processing Systems (IPS) We understand rather well computations in man-made devices especially in computers and how these computations change the (logical) state of the world (pre- and post-conditions) But we do not have general model of computations which occur in natural IPS living organisms and its subsystems businesses languages social systems etc and how these computations change the environment where they proceed Here is proposed a unified view of computations occurring in different Information Processing Systems and clarified notions of Data and Information; the view is based on Entropy While IPS (eg living systems) are capable reducing their inner entropy during their limited existence (none of them are immortal) the final result of their existence is increase of Entropy; Life is (possibly) the greatest factor increasing entropy on Earth',NLP
'One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories In this paper we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem To better help our network maintain this context while also generating long and diverse sentences we incorporate natural language image descriptions along with the images themselves to generate each story sentence We evaluate our system on the Visual Storytelling (VIST) dataset [7] and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling',NLP
'Systematic benchmark evaluation plays an important role in the process of improving technologies for Question Answering (QA) systems While currently there are a number of existing evaluation methods for natural language (NL) QA systems most of them consider only the final answers limiting their utility within a black box style evaluation Herein we propose a subdivided evaluation approach to enable finer-grained evaluation of QA systems and present an evaluation tool which targets the NL question (NLQ) interpretation step an initial step of a QA pipeline The results of experiments using two public benchmark datasets suggest that we can get a deeper insight about the performance of a QA system using the proposed approach which should provide a better guidance for improving the systems than using black box style approaches',NLP
'Categorical compositional distributional semantics provide a method to derive the meaning of a sentence from the meaning of its individual words: the grammatical reduction of a sentence automatically induces a linear map for composing the word vectors obtained from distributional semantics In this paper we extend this passage from word-to-sentence to sentence-to-discourse composition To achieve this we introduce a notion of basic anaphoric discourses as a mid-level representation between natural language discourse formalised in terms of basic discourse representation structures (DRS); and knowledge base queries over the Semantic Web as described by basic graph patterns in the Resource Description Framework (RDF) This provides a high-level specification for compositional algorithms for question answering and anaphora resolution and allows us to give a picture of natural language understanding as a process involving both statistical and logical resources',NLP
'While recent statistical methods for disambiguation in natural language processing have been very successful earlier arguments still hold for the position that fully successful disambiguation requires reasoning to new conclusions from old facts We explore ways of complementing statistical approaches with the use of domain theories: collections of facts and axioms that characterize the typical structure of some task domain In particular we hypothesize that disambiguation decisions can supply tacit information about such theories and that the theories can in part be automatically induced from such data We describe a pilot experiment in which a partial domain theory for the domain of air-travel information was induced from a corpus of disambiguated example sentences the resulting theory then being used successfully in disambiguating other sentences from the same domain',NLP
'We identify two main factors that can cause numerous difficulties when developing a generic entity linking system: i) the amount of data currently available on the Web that do not stop to increase and where a large part comes in the form of natural language texts; ii) the velocity at which data is published that may impose to process streams of text in near real-time Social media platforms such as Twitter Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world Encyclopedia and newspaper articles contain general knowledge of our world and they can be used to explain concepts and known entities Videos can be associated with subtitles and images may have captions Depending on where a text comes from it can have different properties such as a specific language style of writing or topic In this research we present a preliminary framework based on a novel hybrid architecture for an entity linking system that combines methods from the Natural Language Processing (NLP) information retrieval and semantic fields In particular we propose a modular approach in order to be as independent as possible of the text to be processed Our evaluation suggests that this framework can outperform the state-of-the-art systems or show encouraging results on three datasets: OKE2015 #Micropost 2014 and #Micropost 2015 We identify the current limitations and we provide promising future research directions',NLP
'NL2SQL (NLP Language To SQL) is a cutting-edge research direction of natural language processing which converts natural query statements input by users into executable SQL statements CCKS2022 proposes a multi-database multi-table NL2SQL task for the financial domain For this task this paper proposes a SQL generation method based on semantic parsing The method adopts the multi-stage iterative generation mode of question-database name-table namecolumn name-SQL statement uses semantic parsing and semantic similarity learning methods in acquiring table names and generates and selects SQL statements based on the same query Multi-input statement for integrated filtering At the end of the competition the label replacement method was adopted that is the tables and columns in the SQL statement were replaced with tags to reduce the difficulty of generation',NLP
'Automated computational analysis of the vast chemical space is critical for numerous fields of research such as drug discovery and material science Representation learning techniques have recently been employed with the primary objective of generating compact and informative numerical expressions of complex data for efficient usage in subsequent prediction tasks One approach to efficiently learn molecular representations is processing string-based notations of chemicals via natural language processing algorithms Majority of the methods proposed so far utilize SMILES notations for this purpose which is the most extensively used string-based encoding for molecules However SMILES is associated with numerous problems related to validity and robustness which may prevent the model from effectively uncovering the knowledge hidden in the data In this study we propose SELFormer a transformer architecture-based chemical language model (CLM) that utilizes a 100% valid compact and expressive notation SELFIES as input in order to learn flexible and high-quality molecular representations SELFormer is pre-trained on two million drug-like compounds and fine-tuned for diverse molecular property prediction tasks Our performance evaluation has revealed that SELFormer outperforms all competing methods including graph learning-based approaches and SMILES-based CLMs on predicting aqueous solubility of molecules and adverse drug reactions while producing comparable results for the remaining tasks We also visualized molecular representations learned by SELFormer via dimensionality reduction which indicated that even the pre-trained model can discriminate molecules with differing structural properties We shared SELFormer as a programmatic tool together with its datasets and pre-trained models at https:// githubcom/HUBioDataLab/SELFormer Overall our research demonstrates the benefit of using the SELFIES notations in the context of chemical language modeling and opens up new possibilities for the design and discovery of novel drug candidates with desired features',NLP
'Consider two competitive machine learning models one of which was considered state-of-the art and the other a competitive baseline Suppose that by just permuting the examples of the training set say by reversing the original order by shuffling or by mini-batching you could report substantially better/worst performance for the system of your choice by multiple percentage points In this paper we illustrate this scenario for a trending NLP task: Natural Language Inference (NLI) We show that for the two central NLI corpora today the learning process of neural systems is far too sensitive to permutations of the data In doing so we reopen the question of how to judge a good neural architecture for NLI given the available dataset and perhaps further the soundness of the NLI task itself in its current state',NLP
'The dissemination of best medical practices should contribute to a higher quality of care Because natural language specifications can be ambiguous their miss interpretation can lead to all kinds of errors Here we propose a declarative approach for precisely defining medical recommendations We also propose an approach based on semantic process mining to verify that an arbitrary Computer Interpretable Guideline (CIG) complies with the medical recommendations Taking into account that some medical recommendations are critical our work can be seen as a contribution to the design of safer CIGs Moreover we introduce some novel strategies to take full advantage of the information provided by the semantic conformance checker in order to: 1) suggest scenarios than could lead to violations of the medical constraints in the CIG and 2) estimate how flexible is the CIG with respect to the medical recommendations used as starting point',NLP
'Named entity recognition systems have the untapped potential to extract information from legal documents which can improve information retrieval and decision-making processes In this paper a dataset for named entity recognition in Brazilian legal documents is presented Unlike other Portuguese language datasets this dataset is composed entirely of legal documents In addition to tags for persons locations time entities and organizations the dataset contains specific tags for law and legal cases entities To establish a set of baseline results we first performed experiments on another Portuguese dataset: Paramopama This evaluation demonstrate that LSTM-CRF gives results that are significantly better than those previously reported We then retrained LSTM-CRF on our dataset and obtained F-1 scores of 9704% and 8882% for Legislation and Legal case entities respectively These results show the viability of the proposed dataset for legal applications',NLP
'Contract conditions are crucial as they outline an agreement between different parties The semantic terms in contract conditions need to be precisely designated Where these conditions contain vague meanings the interpretation of the conditions will vary especially since the parties of the contract will be differently motivated to pursue their different expectations from it The vague terms in contract conditions may thus cause a dispute and conflict among the parties that can jeopardize the eventual success of a construction project The conventional practice of identifying vagueness in construction contract conditions is done manually which is prone to error time-consuming and requires expert involvement This study develops a methodology to automate the identification of vague terms in construction contract conditions with the sequential application of natural language processing (NLP) and machine learning (ML) techniques Morphological and lexical analysis procedures are used to evaluate the corpus data obtained from a widely used typical construction contract published by International Federation of Consulting Engineers (FIDIC) Classifications of contract conditions in the corpus data are searched using several supervised ML techniques to determine the best performing classifier The results show that the developed methodology reduces time spent on contract review is reliable with a high level of accuracy in predicting the presence of vagueness and removes dependence on expert participation in the contract review processes',NLP
'The increasing use of social media has led to the emergence of a new challenge in the form of abusive content There are many forms of abusive content such as hate speech cyberbullying offensive language and abusive language This article will present a review of abusive content automatic detection approaches Specifically we are focusing on the recent contributions that were using natural language processing (NLP) technologies to detect the abusive content in social media Accordingly we adopt PRISMA flow chart for selecting the related papers and filtering process with some of inclusion and exclusion criteria Therefore we select 25 papers for meta-analysis and another 87 papers were cited in this article during the span of 2017-2021 In addition we searched for the available datasets that are related to abusive content categories in three repositories and we highlighted some points related to the obtained results Moreover after a comprehensive review this article propose a new taxonomy of abusive content automatic detection by covering five different aspects and tasks The proposed taxonomy gives insights and a holistic view of the automatic detection process Finally this article discusses and highlights the challenges and opportunities for the abusive content automatic detection problem',NLP
'In recent years a large proportion of cybercrime attacks are text-data based whereby the cyberattack propagation method is non-technical Though copious information about these attacks mainly the meta-data can be extracted such information is usually unavailable for digital forensic (DF) investigation because these cyber-attacks are often unreported One of the reasons for victim under-reporting of cybercrime is the lack of an anonymous system to report cybercrime as well as the lack of digital forensic defined criteria for cybercrime reporting The problem identified by this paper therefore is the lack of defined digital forensic criteria for reporting cybercrime attacks anonymously A defined digital forensic (DF) criteria for cybercrime reporting is proposed to address this problem The method employed is to firstly develop a generic DF cybercrime reporting data collection system that uses a DF cybercrime incident criteria definition that is also privacy enhanced Secondly the creation of a natural language data preparation semantic builder that is integrated to store mapped semantics data used to create defined DF criteria for a cybercrime language database Thirdly the integrated database (DB) defined DF cybercrime semantic DB can be analysed to formulate a digital forensic readiness architecture for cybercrime language detection The generated natural language (NL) semantic data of potential cybercrime language could be developed as a plugin and APIs pluggable to any DF investigations tools and applications The benefits of the proposed defined DF cybercrime reporting criteria include the following; (i) It is a medium for cybercrime victims to report a crime (ii) The proposed system generates useful data for the implementation of digital forensic readiness architecture and planning that is re-usable and scalable to accommodate other forms of intrusion detection techniques and processes (iii) To design and develop cyber- attack reporting tools and techniques using the information provided by the victims especially in a text-based cyber-attack (iv) To provide research data for cybercrime data analytics using the gathered reported data (v) The proposed criteria ensure that the victims of cybercrime could report cyber-attacks anonymously while maintaining privacy The findings of this paper pave the way to develop a readily available easily accessible and defined digital forensic criteria for cybercrime reporting',NLP
'In this paper we study multi-class classification of tweets where we introduce highly efficient dimensionality reduction techniques suitable for online processing of high dimensional feature vectors generated from freely-worded text As for the real life case study we work on tweets in the Turkish language however our methods are generic and can be used for other languages as clearly explained in the paper Since we work on a real life application and the tweets are freely worded we introduce text correction normalization and root finding algorithms Although text processing and classification are highly important due to many applications such as emotion recognition advertisement selection etc online classification and regression algorithms over text are limited due to need for high dimensional vectors to represent natural text inputs We overcome such limitations by showing that randomized projections and piecewise linear models can be efficiently leveraged to significantly reduce the computational cost for feature vector extraction from the tweets Hence we can perform multi-class tweet classification and regression in real time We demonstrate our results over tweets collected from a real life case study where the tweets are freely-worded eg with emoticons shortened words special characters etc and are unstructured We implement several well-known machine learning algorithms as well as novel regression methods and demonstrate that we can significantly reduce the computational complexity with insignificant change in the classification and regression performance',NLP
'Question answering over knowledge graphs targets to leverage facts in knowledge graphs to answer natural language questions The presence of large number of facts particularly in huge and well-known knowledge graphs such as DBpedia makes it difficult to access the knowledge graph for each given question This paper describes a generic solution based on Personal Page Rank for extracting a small subset from the knowledge graph as a knowledge subgraph which is likely to capture the answer of the question Given a natural language question relevant facts are determined by a bi-directed propagation process based on Personal Page Rank Experiments are conducted over FreeBase DBPedia and WikiMovie to demonstrate the effectiveness of the approach in terms of recall and size of the extracted knowledge subgraphs',NLP
'Text is an important element in document classification in many natural language applications Natural language processing (NLP) is todays computational advancement that provides many significant modern uses of text documents such as efficient information retrieval In this paper we describe the theoretical framework of predicting ICD-9 codes through tagging of clinical notes using our improved framework in deep learning called EnHANs This proposed model improvement covers combination of word and topic embedding as well as adding character-level representation of a document in a hierarchical attention neural networks This paper also present the use of sigmoid activation function in the last layer of the enhanced neural network in order to arrive with a multi-label multi-class prediction of clinical notes with ICD-9 codes',NLP
'In this paper an overview of human-machine interactive communication for controlling lifting devices is presented covering also the integration with vision and sensorial systems Following a general concept and motivation towards intelligent human-machine communication through artificial neural networks selected methods are proposed which provide further directions both of recent as well as of future research on human-machine interaction The aim of the experimental research is to design a prototype of an innovative interaction system equipped with a speech interface in a natural language augmented reality and interactive manipulators with force feedback The presented research offers the possibility of motivating and inspiring further development of the intelligent speech interaction system and methods that have been elaborated in this paper',NLP
'The paper presents a method and an implemented system for the assessment of the participants competences in a collaborative environment based on an instant messenger conversation (chat) For each utterance in the chat a score is computed that takes into account several features specific to text mining (like the presence and the density of keywords via synonymy) natural language pragmatics and to social networks The total rating of the competence of a participant is computed considering the scores of utterances and inter-utterance factors Within the frame of the developed system special attention was given to multiple ways of visualizing the analysis results An annotation editor was also implemented and used in order to construct a golden standard which was further employed for the evaluation of the developed assessment tools',NLP
'Simulation-based training is increasingly being used within the military to practice and develop the skills of successful soldiers For the skills associated with successful military leadership our inability to model human behavior to the necessary degree of fidelity in constructive simulations requires that new interactive designs be developed The ICT Leaders project supports leadership development through the use of branching storylines realized within a virtual reality environment Trainees assume a role in a fictional scenario where the decisions that they make in this environment ultimately affect the success of a mission All trainee decisions are made in the context of natural language conversations with virtual characters The ICT Leaders project advances a new form of interactive training by incorporating a suite of Artificial Intelligence technologies including control architectures agents of mixed autonomy and natural language processing algorithms',NLP
'Style transfer is a natural language processing generation task it consists of substituting one given writing style for another one In this work we seek to perform informal-to-formal style transfers in the English language This process is shown in our web interface where the user input a informal message by text or voice This projects target audience are students and professionals in the need to improve the quality of their work by formalizing their texts A style transfer is considered successful when the original semantic meaning of the message is preserved after the independent style has been replaced This task is hindered by the scarcity of training and evaluation datasets alongside the lack of metrics To accomplish this task we opted to utilize OpenAIs GPT-2 Transformer-based pre-trained model To adapt the GPT-2 to our research we fine-tuned the model with a parallel corpus containing informal text entries paired with the equivalent formal ones We evaluate the fine-tuned model results with two specific metrics formality and meaning preservation To further fine-tune the model we integrate a human-based feedback system where the user selects the best formal sentence out of the ones generated by the model The resulting evaluations of our solution exhibit similar to improved scores in formality and meaning preservation to state-of-the-art approaches',NLP
'Humankind has the ability of learning new things automatically due to the capacities with which we were born We simply need to have experiences read study live For these processes we are capable of acquiring new abilities or modifying those we already have Another ability we possess is the faculty of thinking imagine create our own ideas and dream Nevertheless what occurs when we extrapolate this to machines? Machines can learn We can teach them In the last years considerable advances have been done and we have seen cars that can recognise pedestrians or other cars systems that distinguish animals and even how some artificial intelligences have been able to dream paint and compose music by themselves Despite this the doubt is the following: Can machines think? Or in other words could a machine which is talking to a person and is situated in another room make them believe they are talking with another human? This is a doubt that has been present since Alan Mathison Turing contemplated it and it has not been resolved yet In this article we will show the beginnings of what is known as Artificial Intelligence and some branches of it such as Machine Learning Computer Vision Fuzzy Logic and Natural Language Processing We will talk about each of them their concepts how they work and the related work on the Internet of Things fields',NLP
'Service providers from public institutions to primary care facilities need to constantly attend to clients inquiries to provide useful information and directive guidelines Ensuring high quality service is challenging as it not only demands detailed domain-specific knowledge but also the ability to quickly understand the clients issues through their diverse and often casual descriptions This paper aims to provide a framework for the development of an automated information broker agent who performs the task of a helper The main task of the agent is to interact with the client and direct them to obtain further services that cater their personalized need To do so the agent should accomplish a sequence of tasks that include natural language inquiry knowledge gathering reasoning and giving feedback; in this way it simulates a human helper to engage in interaction with the client The framework combines a question-answering reasoning mechanism while utilizing domain-specific knowledge base When the users cannot describe clearly their needs the system tries to narrow down the possibilities by an iterative question-answering process until it eventually identifies the target In realizing our framework we make a proof-of-concept project Mandy a primary care chatbot system created to assist healthcare staffs by automating the patient intake process We describe in detail the system functionalities and design of the system and evaluate our proof-of-concept on benchmark case studies',NLP
'Plagiarism detection problem has been taken into account both individuals and organizations This problem can be used to detect the copy of documents eg publications books theses and more There are many approaches that have been proposed for plagiarism detection and they work well for English Different countries may use different languages thus natural language processing (eg processing of acute accent circumflex accent etc) as well as semantic or order of the words are still challenging This work proposes an approach for plagiarism detection especially for Vietnamese documents in learning/researching resources The input data were pre-processed extracted vectorized and represented in term of TF-IDF Then Cosine similarity and word-order similarity of the documents are computed Finally an ensemble of these similarities is combined Experimental results on a Vietnamese journal dataset show that the proposed approach is feasibility',NLP
'Machine translation as an efficient tool can achieve equivalent conversion between different languages while preserving the original semantics At present machine translation models based on deep neural networks have become a hot research topic in the fields of natural language processing and image processing However the randomness of neural networks leads to the existing neural network machine translation models unable to effectively reflect the linguistic dependencies and having unsatisfactory results when dealing with long sentence sequences To solve these two problems a new neural network machine translation model with entity tagging improvement is proposed First for the low-frequency word translation problem UNK entity tags replacement is used to compensate for the weakness of the randomness of neural networks and the encoding/decoding strategy of entity tagging is improved Then on the basis of the LSTM translation model an attention mechanism is introduced to dynamically adjust the degree of influence of the context at the source language end on the target language sequence to improve the feature learning ability of the translation model in processing long sentences The analysis of the experimental results shows that the translation evaluation index BLEU of the proposed translation model is significantly improved compared with various translation models which verifies its effectiveness',NLP
'Literature mining is the process of extracting and combining facts from scientific publications In recent years many computer programs have been designed to extract various molecular biology findings from Medline abstracts or full-text articles The present article describes the range of text mining techniques that have been applied to scientific documents It divides automated reading into four general subtasks: text categorization named entity tagging fact extraction and collection-wide analysis Literature mining offers powerful methods to support knowledge discovery and the construction of topic maps and ontologies An overview is given of recent developments in medical language processing Special attention is given to the domain particularities of molecular biology and the emerging synergy between literature mining and molecular databases accessible through Internet Crown Copyright (C) 2002 Published by Elsevier Science Ireland Ltd All rights reserved',NLP
'Lemmatization computing the canonical forms of words in running text is an important component in any NLP system and a key preprocessing step for most applications that rely on natural language understanding In the case of Arabic lemmatization is a complex task because of the rich morphology agglutinative aspects and lexical ambiguity due to the absence of short vowels in writing In this paper we introduce a new lemmatizer tool that combines a machine-learning-based approach with a lemmatization dictionary the latter providing increased accuracy robustness and flexibility to the former Our evaluations yield a performance of over 98% for the entire lemmatization pipeline The lemmatizer tools are freely downloadable for private and research purposes (C) 2018 The Authors Published by Elsevier BV',NLP
'Automated conversational agents built with medical applications in mind have the potential to reduce healthcare readmissions and improve accessibility to medical knowledge In this work we demonstrate the development and evaluation of an automated chatbot for triage and conditions assessment based on user inputs in natural language The implemented bot engages patients in conversation about symptoms experienced and provides a personalized pre-synopsis based on their symptoms and profile Our chatbot system was able to predict user conditions correctly based on two sets of patient test cases with an average precision of 082 Our implementation demonstrates that a medical chatbot can help with automatic triage and pre-assessment of patients with simple symptom analysis and a conversational approach without the use of cumbersome form-based data entry',NLP
'AT agents combining natural language interaction task planning and business ontologies can help companies provide better-quality and more cost-effective customer service Our customer-service agents use natural language to interact with customers enabling customers to state their intentions directly instead of searching for the places on the Web site that may address their concern We use planning methods to search systematically for the solution to the customers problem ensuring that a resolution satisfactory for both the customer and the company is found if one exists Our agents converse with customers guaranteeing that needed information is acquired from customers and that relevant information is provided to them in order for both parties to make the right decision The net effect is a more frictionless interaction process that improves the customer experience and makes businesses more competitive on the service front',NLP
'In this paper we describe our recent work at Microsoft Research in the project codenamed Dr Who aimed at the development of enabling technologies for speech-centric multimodal human-computer interaction In particular we present in detail MiPad as the first Dr Whos application that addresses specifically the mobile user interaction scenario MiPad is a wireless mobile PDA prototype that enables users to accomplish many common tasks using a multimodal spoken language interface and wireless-data technologies It fully integrates continuous speech recognition and spoken language understanding and provides a novel solution to the current prevailing problem of pecking with tiny styluses or typing on minuscule keyboards in todays PDAs or smart phones Despite its current incomplete implementation we have observed that speech and pen have the potential to significantly improve user experience in our user study reported in this paper We describe in this system-oriented paper the main components of MiPad with a focus on the robust speech processing and spoken language understanding aspects The detailed MiPad components discussed include: distributed speech recognition considerations for the speech processing algorithm design; a stereo-based speech feature enhancement algorithm used for noise-robust front-end speech processing; Aurora2 evaluation results for this front-end processing; speech feature compression (source coding) and error protection (channel coding) for distributed speech recognition in MiPad; HMM-based acoustic modeling for continuous speech recognition decoding; a unified language model integrating context-free grammar and N-gram model for the speech decoding; schema-based knowledge representation for the MiPads personal information management task; a unified statistical framework that integrates speech recognition spoken language understanding and dialogue management; the robust natural language parser used in MiPad to process the speech recognizers output; a machine-aided grammar learning and development used for spoken language understanding for the MiPad task; Tap & Talk multimodal interaction and user interface design; back channel communication and MiPads error repair strategy; and finally user study results that demonstrate the superior throughput achieved by the Tap & Talk multimodal interaction over the existing pen-only PDA interface These user study results highlight the crucial role played by speech in enhancing the overall user experience in MiPad-like human-computer interaction devices',NLP
'The goal of this paper is to propose measures of innovation through the study of publications in the field of speech and language processing It is based on the NLP4NLP corpus which contains the articles published in major conferences and journals related to speech and language processing over 50 years (1965-2015) It represents 65003 documents from 34 different sources conferences and journals published by 48894 different authors in 558 events for a total of more than 270 million words and 324422 bibliographical references The data was obtained in textual form or as an image that had to be converted into text This resulted in a lower quality for the most ancient papers that we measured through the computation of an unknown word ratio The multi-word technical terms were automatically extracted after parsing using a set of general language text corpora The occurrences frequencies existences and presences of the terms were then computed overall for each year and for each document It resulted in a list of 35 million different terms and 24 million term occurrences The evolution of the research topics over the year as reflected by the terms presence was then computed and we propose a measure of the topic popularity based on this computation The author(s) who introduced the terms were searched for together with the year when the term was first introduced and the publication where it was introduced We then studied the global and evolutional contributions of authors to a given topic We also studied the global and evolutional contributions of the various publications to a given topic We finally propose a measure of innovativeness for authors and publications',NLP
'Time series data mining has gained increasing attention in health domain Recently researchers attempt to employ Natural Language Processing (NLP) to health data mining in order to learn proper representations of discrete medical concepts from Electronic Health Records (EHRs) However existing models do not take continuous physiological records into account which are naturally existed in EHRs The major challenges for this task are to model non-obvious representations from observed high dimensional biosignals and to interpret the learned features To address these issues we propose Wave2Vec an end-to-end deep learning model to bridge the gap between biosignal processing and language modeling Wave2Vec jointly learns both inherent and embedding representations of biosignals at the same time To evaluate the performance of our model in clinical task we carry out experiments on two real world benchmark biosignal datasets Experimental results show that the proposed Wave2Vec model outperforms the six feature leaning baselines in biosignal processing',NLP
'The word boundary detection has an application in speech processing systems The problem this paper tries to solve is to separate words of a sequence of phonemes where there is no delimiter between phonemes In this paper at first a recurrent fuzzy neural network (RFNN) together with its relevant structure is proposed and learning algorithm is presented Next this RFNN is used to predict word boundaries Some experiments have already been implemented to determine complete structure of RFNN Here in this paper three methods are proposed to encode input phoneme and their performance have been evaluated Some experiments have been conducted to determine required number of fuzzy rules and then performance of RFNN in predicting word boundaries is tested Experimental results show an acceptable performance',NLP
'Probabilistic expectations and memory limitations are central factors governing the real-time comprehension of natural language but how the two factors interact remains poorly understood One respect in which the two factors have come into theoretical conflict is the documentation of both locality effects in which having more dependents preceding a governing verb increases processing difficulty at the verb and anti-locality effects in which having more preceding dependents facilitates processing at the verb However no controlled study has previously demonstrated both locality and anti-locality effects in the same type of dependency relation within the same language Additionally many previous demonstrations of anti-locality effects have been potentially confounded with lexical identity plausibility and sentence position Here we provide new evidence of both locality and anti-locality effects in the same type of dependency relation in a single language verb-final constructions in German while controlling for lexical identity plausibility and sentence position In main clauses we find clear anti-locality effects with the presence of a preceding dative argument facilitating processing at the final verb; in subject-extracted relative clauses with identical linear ordering of verbal dependents we find both anti-locality and locality effects with processing facilitated when the verb is preceded by a dative argument alone but hindered when the verb is preceded by both the dative argument and an adjunct These results indicate that both expectations and memory limitations need to be accounted for in any complete theory of online syntactic comprehension (C) 2012 Roger P Levy and Frank Keller Published by Elsevier Inc All rights reserved',NLP
'Background: The recognition of medical entities from natural language is a ubiquitous problem in the medical field with applications ranging from medical coding to the analysis of electronic health data for public health It is however a complex task usually requiring human expert intervention thus making it expansive and time-consuming Recent advances in artificial intelligence specifically the rise of deep learning methods have enabled computers to make efficient decisions on a number of complex problems with the notable example of neural sequence models and their powerful applications in natural language processing However they require a considerable amount of data to learn from which is typically their main limiting factor The Centre for Epidemiology on Medical Causes of Death (CepiDc) stores an exhaustive database of death certificates at the French national scale amounting to several millions of natural language examples provided with their associated human-coded medical entities available to the machine learning practitioner Objective: The aim of this paper was to investigate the application of deep neural sequence models to the problem of medical entity recognition from natural language Methods: The investigated data set included every French death certificate from 2011 to 2016 These certificates contain information such as the subjects age the subjects gender and the chain of events leading to his or her death both in French and encoded as International Statistical Classification of Diseases and Related Health Problems Tenth Revision (ICD-10) medical entities for a total of around 3 million observations in the data set The task of automatically recognizing ICD-10 medical entities from the French natural language based chain of events leading to death was then formulated as a type of predictive modeling problem known as a sequence-to-sequence modeling problem A deep neural network-based model known as the Transformer was then slightly adapted and fit to the data set Its performance was then assessed on an external data set and compared to the current state-of-the-art approach CIs for derived measurements were estimated via bootstrapping Results: The proposed approach resulted in an F-measure value of 0952 (95% CI 0946-0957) which constitutes a significant improvement over the current state-of-the-art approach and its previously reported F-measure value of 0825 as assessed on a comparable data set Such an improvement makes possible a whole field of new applications from nosologist-level automated coding to temporal harmonization of death statistics Conclusions: This paper shows that a deep artificial neural network can directly learn from voluminous data sets in order to identify complex relationships between natural language and medical entities without any explicit prior knowledge Although not entirely free from mistakes the derived model constitutes a powerful tool for automated coding of medical entities from medical language with promising potential applications',NLP
'In this 21st century ensuring the security of a place is one of the most crucial things We have to use either manual or semi-autonomous entrances and human security guards in most of the security systems Many sophisticated apartments are still now using a traditional security system to ensure safety Besides monitoring only by the camera in a certain area is not a better solution to confirm safety A lot of paperwork has been proposed to make security system as efficient as possible But a few methods have been proposed an intelligent security system with integrated hardware and automation The proposed methodology has emphasized building a robotics ecosystem of security that control entrances to give access to authentic people making decisions and taking actions in any circumstances automatically by itself These tasks are being performed based on a collective method of image processing natural language processing supervised learning IoT automation and web server technology This robotic system is capable of memorizing previously identified humans with their information talking with users in a local language gathering necessary information and detecting everything with the user By this method there has been proposed a solution to overcome the limitations of a manual or semi-autonomous security system and provide intelligent AI-based security with a robotic ecosystem at a minimum cost',NLP
'Summarization is the art of generating the main points of a lengthy text document by removing redundant and less important information without losing the meaning of the original text Summaries are significantly shorter than the original text and take a broad overview of the source material With the increasing volume of digital information today people find the manual process of summarization as hectic and time consuming Having an automated text summarization system for electronic documents would very much help to encourage people to read giving quick access to information thus helping them to a faster decision making process Although many research and commercial text summarization tools are available but no research is officially reported for Malay language Malay text summarizers are coming into demand when a lot of information in Malay language can now be accessed freely via the Internet This paper presents a hybrid approach to an automated text summarization system for Malay language The base system is built on SUMMARIST system and is expanded by combining with EstSum system Experimental results show that expanding training data size significantly contributes to the performance In general our system produced acceptable results at the best case of 76% and the worst case of 31%',NLP
'Artificial Intelligence (AI) Machine Learning (ML) Information Retrieval (IR) and Natural Language Processing (NLP) are transforming the way legal professionals and law firms approach their work The significant potential for the application of AI to Law for instance by creating computational solutions for legal tasks has intrigued researchers for decades This appeal has only been amplified with the advent of Deep Learning (DL) It is worth noting that working with legal text is far more challenging as compared to the other subdomains of IR/NLP mainly due to the typical characteristics of legal text such as considerably longer documents complex language and lack of large-scale annotated datasets In this tutorial we introduce the audience to these characteristics of legal text and with it the challenges associated with processing the legal documents We touch upon the history of AI and Law research and how it has evolved over the years from relatively simpler approaches to more complex ones such as those involving DL We organize the tutorial as follows First we provide a brief introduction to state-of-the-art research in the general domain of IR and NLP We then discuss in more detail IR/NLP tasks specific to the legal domain We outline the methodologies (both from an academic and industry perspective) and the available tools and datasets to evaluate the methodologies This is then followed by a hands-on coding/demo session',NLP
'In recent years Text Mining has seen a tremendous spurt of growth as data scientists focus their attention on analyzing unstructured data The main drivers for this growth have been big data as well as complex applications where the information in the text is often combined with other kinds of information in building predictive models These applications require highly efficient and scalable algorithms to meet the overall performance demands In this context six main directions are identified where research in text mining is heading: Deep Learning Topic Models Graphical Modeling Summarization Sentiment Analysis Learning from Unlabeled Text Each direction has its own motivations and goals There is some overlap of concepts because of the common themes of text and prediction The predictive models involved are typically ones that involve meta-information or tags that could be added to the text These tags can then be used in other text processing tasks such as information extraction While the boundary between the fields of Text Mining and Natural Language Processing is becoming increasingly blurry the importance of predictive models for various applications involving text means there is still substantial growth potential within the traditional sub-fields of text mining These data-centric directions are also likely to influence future research in Natural Language Processing especially in resource-poor languages and in multilingual texts WIREs Data Mining Knowl Discov 2015 5:155-164 doi: 101002/widm1154 For further resources related to this article please visit the ',NLP
'The Web has provided an excellent platform for business to consumer (B2C) electronic commerce B2C electronic commerce offers convenience choice lower cost and customization to consumers The electronic shopping platform allows consumers to make intelligent comparison and purchasing decision on consumer products In addition to comparing product specifications as described on electronic catalogue for Letter purchasing decision consumers also hunger for consumer reviews to identify the best products that fit their preferences For example a professional photographer would like to identify a camera with lens of high quality and zooming power but a general user may like to find a camera that is cheap light and with a large LCD screen When consumers take consumer reviews as reference they are interested in both opinion orientation and product features that they are describing Most of the prior works on consumer opinions mining focus on identifying opinion orientation Some recent works have started to classify product features but heavily rely on linguistic and natural language processing techniques However the writing in consumer reviews is usually less formal and many of them do not conform to the grammatical rules Therefore the linguistic and language processing approach is not satisfactory In this work we propose a sentiment analysis system to classify product features of consumer reviews by mining class association rules The experimental result shows that the performance is promising The content mining approach outperforms the natural language processing approach',NLP
'Poetry courses are taught in Foreign Language Teacher Education Departments either independently (the Uludag University case) or within the framework of general literature courses The relevant sources benefited from in such courses may initially aim to teach how to read poems before such poetry elements as tone diction etc (Di Yanni 2000) In the model recommended experiencing poems with subjective responses is given priority while interpreting them with intellectual processes seems to be the following step of reading poems (Di Yanni 2000 pp 1-2) In foreign language contexts like Turkey imposing this order has been observed to be problematic as learners and/or teacher trainees inherently tend to do reasoning to understand a poem before subjectively relating it to their own lives mainly because of the elliptical metaphorical and allusive language of poetry (Brindley 1980) and cultural vagueness (Zelenkova 2004) In this regard the central thesis and pedagogical implication of this discussion paper is that the interpretation section should take precedence when to approach a new poem as that is what would conform with the natural tendency of foreign language learners and the teaching processes to guide the learners should be accordingly planned and implemented (c) 2013 The Authors Published by Elsevier Ltd',NLP
'Genuine numerical multilingual text classification is almost impossible if only words are treated as the privileged unit of information Although text tokenization (in which words are considered as tokens) is relatively easy in English or French it is much more difficult for other languages such as German or Arabic Moreover stemming typically used to normalize and reduce the size of the lexicon constitutes another challenge The notion of N-grams of words (ie sequences of N words with N typically equals to 2 3 or 4) which for the last ten years seems to have produced good results both in language identification and speech analysis has recently become a privileged research axis in several areas of knowledge extraction from text In this paper we present a text classification software based on N-grams of characters (not words) evaluate its results on documents containing text written in English and French and compare these results with those obtained from a different classification tool based exclusively on the processing of words An interesting feature of our software is that it does not need to perform any language-specific processing and is thus appropriate for multilingual text classification',NLP
'The determination and classification of natural language based on specified content and data set involves a process known as spoken language identification (LID) To initiate the process useful features of the given data need to be extracted first in a mature process where the standard LID features have been previously developed by employing the use of MFCC SDC GMM and the i-vector-based framework Nevertheless optimisation of the learning process is still required to enable a comprehensive capturing of the extracted features embedded knowledge The training of a single hidden layer neural network can be done using the extreme learning machine (ELM) which is an effective learning model for conducting classification and regression analysis Nevertheless the learning process of this model is not entirely effective (ie optimised) due to the random selection of weights within the input hidden layer This study employs ELM as the LID learning model centred upon the extraction of the standard features The enhanced self-adjusting extreme learning machine (ESA-ELM) is one of the ELMs optimisation techniques which has been chosen as the benchmark and is enhanced by adopting a new alternative optimisation approach (PSO) instead of (EATLBO) in terms of achieving high performance The improved ESA-ELM is named particle swarm optimisation-extreme learning machine (PSO-ELM) The generated results are based on LID with the same benchmarked data set derived from eight languages which indicated the superior performance of the particle swarm optimisation-extreme learning machine LID (PSO-ELM LID) with an accuracy of 9875% in comparison with the ESA-ELM LID which only achieved 9625%',NLP
'Intracerebral hemorrhage is a severe problem where more than one-third of patients die within a month In diagnosing intracranial hemorrhage neuroimaging examinations are essential As a result the interpretation of neuroimaging becomes a crucial process in medical procedures However human-based image interpretation has inherent limitations as it can only handle a restricted range of tasks To address this a study on medical image captioning has been conducted but it primarily focused on single medical images However actual medical images often consist of continuous sequences such as CT scans making it challenging to directly apply existing studies Therefore this paper proposes a CT image captioning model that utilizes a 3D-CNN model and distilGPT-2 In this study four combinations of 3D-CNN models and language models were compared and analyzed for their performance Additionally the impact of applying penalties to the loss function and adjusting penalty values during the training process was examined The proposed CT image captioning model demonstrated a maximum BLEU score of 035 on the in-house dataset and it was observed that the text generated by the model became more similar to human interpretations in medical image reports with the application of loss function penalties',NLP
'In a natural setting speech is often accompanied by gestures As language speech-accompanying iconic gestures to some extent convey semantic information However if comprehension of the information contained in both the auditory and visual modality depends on same or different brain-networks is quite unknown In this fMRI study we aimed at identifying the cortical areas engaged in supramodal processing of semantic information BOLD changes were recorded in 18 healthy right-handed male subjects watching video clips showing an actor who either performed speech (S acoustic) or gestures (G visual) in more (+) or less (-) meaningful varieties In the experimental conditions familiar speech or isolated iconic gestures were presented; during the visual control condition the volunteers watched meaningless gestures (G-) while during the acoustic control condition a foreign language was presented (S-) The conjunction of the visual and acoustic semantic processing revealed activations extending from the left inferior frontal gyrus to the precentral gyrus and included bilateral posterior temporal regions We conclude that proclaiming this frontotemporal network the brains core language system is to take too narrow a view Our results rather indicate that these regions constitute a supramodal semantic processing network',NLP
'This is a non-technical paper describing how and why we organized BEST 2009 the first contest in the series of Benchmark for Enhancing the Standard of Thai language processing which is expected to help accelerate the progress of the Natural Language Processing technology in Thailand by assembling 3 essential components: common standards resources and researchers The BEST 2009 : Thai Word Segmentation Software Contest is the first shared task on Thai NLP that exercised this assemblage and aimed to find the best algorithms that could correctly divide Thai non-segmented script into words according to the guidelines previously prepared by experts from several research institutes and universities Thai word-segmented corpora of 5 million words have been developed as a training set another 600K as a test set The evaluation procedure and protocol have been designed The process and the results of the contest are reported',NLP
'Currently there is a large amount of text being shared through the Internet These texts are available in different forms-structured unstructured and semi structured There are different ways of analyzing texts but domain experts usually divide this process in some steps such as pre-processing feature extraction and a final step that could be classification clustering summarization and keyword extraction depending on the purpose over the text For this processing several approaches have been proposed in the literature based on variations of methods like artificial neural network and deep learning In this paper we conducted a systematic review of papers dealing with the use of complex networks approaches for the process of analyzing text The main results showed that complex network topological properties measures and modeling can capture and identify text structures concerning different purposes such as text analysis classification topic and keyword extraction and summarization We conclude that complex network topological properties provide promising strategies with respect of processing texts considering their different aspects and structures',NLP
'Career direction is a crucial matter not to be undermined in the development of a more efficient generation of the corporate workforce In order to obtain accurate career direction one would think of different ways of identifying attributes that would lead to an accurate classification of personality In this paper the goal is extracting personality from the use of language The paper covers all aspects of this process in terms of Text Normalization Techniques Feature Extraction Feature Selection Data Pre-Processing Data Sampling Training Predictive Models to predict personality types validating the results on test data and finally and finally compare the findings with other approaches to personality classification After having a personality type classified the process is as simple as matching career paths that are most likely suitable for the user All these processes combined by experimenting with various approaches to each operation would result in personality attribute classifiers yielding an average of 96% accuracy',NLP
'With the growth of Internet usage a massive amount of textual data is generated on social media and the Web As the text on the Web are generated by different authors with various types of writing styles and different encodings a preprocessing step is required before applying any NLP task The goal of preprocessing is to convert text into a standard format that makes it easy to extract information from documents and sentences Moreover the problem is more acute when we deal with Arabic script-based languages in which there are some different kinds of encoding schemes different kinds of writing styles and the spaces between or within the words This paper introduces a preprocessing toolkit named as Parsivar which is a comprehensive set of tools for Persian text preprocessing tasks This toolkit performs various kinds of activities comprised of normalization space correction tokenization stemming parts of speech tagging and shallow parsing To evaluate the performance of the proposed toolkit both intrinsic and extrinsic approaches for evaluation have been applied A Persian plagiarism detection system has been exploited as a downstream task for extrinsic evaluation of the proposed toolkit The results have revealed that our toolkit outperforms the available Persian preprocessing toolkits by about 8 percent in terms of F1',NLP
'The widespread of the computer technology and the Internet lead to a massive amount of textual information being available in written Arabic This that more is available it becomes more difficult to extract the relevant information To meet this challenge many researchers are directed to the development of information retrieval systems based on syntactic and semantic parsing In Arabic this field is restricted by the lack of labeled datasets Thus it is important to build systems for part-of-speech tagging and language modeling and use their results for further syntactic and semantic parsing in fields like chunking semantic role labeling information extraction named entity recognition and statistical machine translation Deep neural networks have proved efficient in fields like imaging or acoustics and recently in natural language processing In this study we used the Taguchi method to find the optimal parameter combination for a deep neural network architecture Therefore the neural network obtained the most accurate results The main use of the Taguchi method in our work is to help us to choose the best context which is the number of words before and after the word on which the training is made',NLP
'Language Models such as BERT (Bidirectional Encoder Representations from Transformers) have grown in popularity due to their ability to be pre-trained and perform robustly on a wide range of Natural Language Processing tasks Often seen as an evolution over traditional word embedding techniques they can produce semantic representations of text useful for tasks such as semantic similarity However state-of-the-art models often have high computational requirements and lack global context or domain knowledge which is required for complete language understanding To address these limitations we investigate the benefits of knowledge incorporation into the fine-tuning stages of BERT An existing K-BERT model which enriches sentences with triplets from a Knowledge Graph is adapted for the English language and extended to inject contextually relevant information into sentences As a sideeffect changes made to K-BERT for accommodating the English language also extend to other word-based languages Experiments conducted indicate that injected knowledge introduces noise We see statistically significant improvements for knowledge-driven tasks when this noise is minimised We show evidence that given the appropriate task modest injection with relevant high-quality knowledge is most performant',NLP
'Studies of artificial language learning provide insight into how learning biases and iterated learning may shape natural languages Prior work has looked at how learners deal with unpredictable variation and how a language changes across multiple generations of learners The present study combines these features exploring how word order variation is preserved or regularized over generations We investigate how these processes are affected by (1) learning biases (2) the size of the language community and (3) the amount of input provided Our results show that when the input comes from a single speaker adult learners frequency match reproducing the variability in the input across three generations However when the same amount of input is distributed across multiple speakers frequency matching breaks down When regularization occurs there is a strong bias for SOV word order (relative to OSV and VSO) Finally when the amount of input provided by multiple speakers is increased learners are able to frequency match These results demonstrate that both population size and the amount of input per speaker each play a role in language convergence',NLP
'Business processes play an important role in Service-Oriented Architectures Commonly the business processes are designed in the Business Process Modeling Notation (BPMN) which allows their development even by persons without programming skills Being abstract and high-level BPMN is not suitable for direct execution though The natural choice for implementing business processes is the Business Process Executable Language (BPEL) which is directly executable but it is also a programming language that requires programming skills in order to be used This dichotomy is typically solved by transforming BPMN into BPEL However this transformation is a complex task There have been developed a number of transformation methods but their common problem is either incompleteness or loss of intentions which makes BPEL rather difficult to modify and debug as well as to propagate changes back to BPMN In this paper we address this problem by presenting a non-trivial enhancement of the existing transformation algorithm [14] Our approach provides a complete transformation while preserving a large set of intentions expressed in the BPMN description which makes it also suitable in the context of model-driven development',NLP
'Ontology Population looks for instantiating the constituent elements of an ontology like properties and non-taxonomic relationships Manual population by domain experts and knowledge engineers is an expensive and time consuming task Fast ontology population is critical for the success of knowledge-based applications Thus automatic or semi-automatic approaches are needed This work proposes a generic process approaching the Automatic Ontology Population problem by specifying its phases and the techniques used to perform the activities on each phase The main contribution of the work here described is a domain-independent process for the automatic population of ontologies from text that applies natural language processing and information extraction techniques to acquire and classify ontology instances This is a new approach for automatic ontology population that uses an ontology to automatically generate rules to extract instances from text and classify them in ontology classes These rules can be generated from ontologies of any domain making the proposed process domain-independent and therefore allowing the instantiation of ontologies quickly and at a low cost Four experiments using a legal and a tourism corpora were conducted in order to evaluate l the proposed process Results indicate that this approach can extract and classify instances with high effectiveness with the additional advantage of domain independence Some techniques representing the state of the art of this field are also described along with the solutions they adopt for each phase of the Automatic Ontology Population process with their advantages and limitations (C) 2013 Elsevier BV All rights reserved',NLP
'Recurrent neural networks have become increasingly popular for the task of language modeling achieving impressive gains in state-of-the-art speech recognition and natural language processing (NLP) tasks Recurrent models exploit word dependencies over a much longer context window (as retained by the history states) than what is feasible with n-gram language models However the training criterion of choice for recurrent language models continues to be the local conditional likelihood of generating the current word given the (possibly long) word context thus making local decisions at each word This locally-conditional design fundamentally limits the ability of the model in exploiting whole sentence structures In this paper we present our initial results at whole sentence neural language models which assign a probability to the entire word sequence We extend the previous work on whole sentence maximum entropy models to recurrent language models while using Noise Contrastive Estimation (NCE) for training as these sentence models are fundamentally unnormalizable We present results on a range of tasks: from sequence identification tasks such as palindrome detection to large vocabulary automatic speech recognition (LVCSR) and demonstrate the modeling power of this approach',NLP
'Several Natural Language Processing applications in a particular language consider POS tagging a necessary component To develop a new language specific POS tagger targeting such particular language is a tedious job for unstructured data due to the variation in text type and complexity of text For that reason it impacts the precision of tagging as a result of the variety of a certain language Current research focused on the thought of reusability of a popular language specific Part of speech tagger for example Stanford Part of speech Tagger can be employed for tagging non-Engish phrases For generalizeability any translator can be used to translate the sentences however a well-known translator named Google translator is considered for sentence translation purpose across the languages For evaluation perspective Urdu tweets of a hot political issue Panama leaks from twittercom are extracted To measure the accuracy the kappa statistic along with confusion matrix is deliberated The precision of tagging the Urdu sentences by reusing Stanford Part of speech tagger is 9605 percent The respected approach can be globally applied to tagging the sentences of several different languages',NLP
'Language barriers in day to day communication are common in all countries In Sri Lanka we have a rising need for translation for Sinhala and Tamil to reduce language barriers and the statistical machine translation approach is more suitable for the concerned languages Statistical machine translation method is one of the most promising and efficient method to perform machine translation for Sri Lankan languages likes Sinhala and Tamil Statistical approach is more suitable for structurally dissimilar pairs of languages and efficient solution for large text translation Sinhala and Tamil have a similarity in grammar and statistical approach will help to obtain more accurate results We have developed a Real-time bi-directional translation system for both Tamil to Sinhala and Sinhala to Tamil for this research We have used the Sri Lankan parliament corpus to train the language model We have critically evaluated the both systems with parameter optimizations and have obtained the most accurate and efficient system We have also utilized the scoring techniques like BLEU [2 8] & NIST [2] for the system evaluation and we have integrated the MERT technique to tune the decoder',NLP
'Person search by natural language description is a challenging problem because of demands for modelling and learning visual-text semantic embedding While several works have been dedicated to person search by English description very few attempts have been made for other languages This paper presents the first work towards person search by Vietnamese description The contribution of the paper is threefold First the first and large-scale dataset for person search by Vietnamese natural language named 3000VnPersonSearch is built Second inspired by dual-path architecture (Zheng et al ACM Trans Multimed Comput Commit) Appl (TOMM) 16(2):1-23 2020) in which single loss for intra-modal and triple loss for cross-modal learning of text and image data distribution were considered in this paper we employ this architecture for Vietnamese description-based person search However as Vietnamese language is under-resource the existing word embedding model is still modest compared to that of English Therefore instead of using word2vec model as in Zheng et al (ACM Trans Multimed Comput Commit) Appl (TOMM) 16(2):1-23 2020) we modify the initialization process of the first convolution layer of the text-CNN path In addition we investigate in detail two online triplet mining strategies that are batch all and batch hard triplet Extensive experiments have been conducted on benchmark datasets as well as on 3000VnPersonSearch Experimental results show that the proposed method obtains 242% of improvement over the baseline method on CUHK-PEDES dataset and achieved state of the art results on VnPersonSearch dataset with a significant margin in comparison with the method in Pham et al (2020) Finally in order to illustrate the practical usage of person search by Vietnamese description language a web-based application of person search is implemented and deployed',NLP
'Semantic-based sublanguage grammars have been shown to be an efficient method for medical language processing However given the complexity of the medical domain parsers using such grammars inevitably encounter ambiguous sentences which could be interpreted by different groups of production rules and consequently result in two or more parse trees One possible solution which has not been extensively explored previously is to augment productions in medical sublanguage grammars with probabilities to resolve the ambiguity In this study we associated probabilities with production rules in a semantic-based grammar for medication findings and evaluated its performance on reducing parsing ambiguity Using the existing data set from 2009 i2b2 NLP (Natural Language Processing) challenge for medication extraction we developed a semantic-based CFG (Context Free Grammar) for parsing medication sentences and manually created a Treebank of 4564 medication sentences from discharge summaries Using the Treebank we derived a semantic-based PCFG (Probabilistic Context Free Grammar) for parsing medication sentences Our evaluation using a 10-fold cross validation showed that the PCFG parser dramatically improved parsing performance when compared to the CFG parser (C) 2011 Elsevier Inc All rights reserved',NLP
'Sentiment lexicons in the English language are widely accessible while in many other languages these resources are extremely deficient Current techniques and methods for sentiment analysis focus mainly on the English language whereas other languages are neglected due to lack of resources In order to overcome challenges faced in building non-English lexicons we propose a language-independent method that automatically builds non-English sentiment lexicons based on currently available English lexicons with an unannotated corpus from the target language The proposed method will automatically recognize and extract new polarity words from the unannotated corpus based on the initial seed lexicons that are developed by translating three reliable English lexicons The experimental results from the test datasets confirmed that a developed non-English sentiment lexicon could significantly enhance the performance of non-English sentiment classifications compared with other methods and lexicons The developed lexicon in the Arabic language outperformed other commonly used methods for developing non-English lexicons with an 074 F measure The adopted approach in this study was proven to be language independent and can be implemented in other languages as well This paper also contributes to understanding the approaches to developing sentiment resources',NLP
'This paper focuses on text-based affective computing for Jopara a code-switching language that combines Guarani and Spanish First we collected a dataset of tweets primarily written in Guarani and annotated them for three widely used dimensions in sentiment analysis: (a) emotion recognition (b) humor detection and (c) offensive language identification Then we developed several neural network models including large language models specifically designed for Guarani and compared their performance against off-the-shelf multilingual and Spanish pre-trained models for the aforementioned dimensions Our experiments show that language models incorporating Guarani during pre-training or pre-fine-tuning consistently achieve the best results despite limited resources (a single 24-GB GPU and only 800K tokens) Notably even a Guarani BERT model with just two layers of Transformers shows a favorable balance between accuracy and computational power likely due to the inherent low-resource nature of the task We present a comprehensive overview of corpus creation and model development for low-resource languages like Guarani particularly in the context of its code-switching with Spanish resulting in Jopara Our findings shed light on the challenges and strategies involved in analyzing affective language in such linguistic contexts',NLP
'The cyber threat landscape is changing rapidly thus making the process of scientific classification of incidents for the purpose of incident response management difficult Additionally there are no universal methodologies for sharing information on cyber security incidents between private and public sectors Existing efforts to automate the process of incident classification do not make a distinction between ordinary events and threatening incidents which can cause issues that permeate throughout the entire incident response process We describe a machine learning model to determine the probability that an event is an incident using contextual information of the event',NLP
'The KDD process aims at searching for interesting instances of patterns in data sets It is widely accepted that the patterns must be comprehensible One of the aspects that are under-addressed in the KDD process is the handling of uncertainty in the process of clustering classification and association rules extraction In this paper we present a classification framework for relational databases so as to support uncertainty in terms of natural language queries and assessments More specifically we present a classification scheme of non-categorical attributes into lexically defined categories based on fuzzy logic and provides decision support facilities based on related information measures',NLP
'Name identification has been worked on quite intensively for the past few years and has been incorporated into several products revolving around natural language processing tasks Many researchers have attacked the name identification problem in a variety of languages but only a few limited research efforts have focused on named entity recognition for Arabic script This is due to the lack of resources for Arabic named entities and the limited amount of progress made in Arabic natural language processing in general In this article we present the results of our attempt at the recognition and extraction of the 10 most important categories of named entities in Arabic script: the person name location company date time price measurement phone number ISBN and file name We developed the system Named Entity Recognition for Arabic (NERA) using a rule-based approach The resources created are: a Whitelist representing a dictionary of names and a grammar in the form of regular expressions which are responsible for recognizing the named entities A filtration mechanism is used that serves two different purposes: (a) revision of the results from a named entity extractor by using meta-data in terms of a Blacklist or rejecter about ill-formed named entities and (b) disambiguation of identical or overlapping textual matches returned by different name entity extractors to get the correct choice In NERA we addressed major challenges posed by NER in the Arabic language arising due to the complexity of the language peculiarities in the Arabic orthographic system non-standardization of the written text ambiguity and lack of resources NERA has been effectively evaluated using our own tagged corpus; it achieved satisfactory results in terms of precision recall and F-measure',NLP
'Current approaches for semantic management of business processes and services assume and build on semantic models which dont exist in practice A key and open research problem is how to lift semantic models from existing process and service descriptions In this paper we are interested in extracting action capabilities from their textual descriptions We first introduce a semantic frame-based capability model that features domain-specific functional properties We present an approach for retrieving top-k capabilities that match best a natural language description of an action Our approach has been evaluated using publicly available process and service descriptions',NLP
'Current Chinese segmenting method doesnt consider grammar and semantics during the process of segmenting So segmenting accuracy and speed are not good Therefore on the base of analyzing the peculiarity of Chinese a knowledge-based Chinese segment method KSM is presented It uses a dictionary of hierarchical structure to get all the re-segmented words and makes ambiguity processing easy and effective It gets support not only from letters and words knowledge but from grammar and semantics knowledge It removes wrong segmentation by checking grammar and semantics Also It can learn new words to improve the accuracy of segmenting',NLP
'Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic However the process of conducting a Systematic Literature Review manually can be difficult and time-consuming For this reason researchers aim to semi-automate this process or some of its phases Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models These models build on transformer-based deep language models (ie BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies number of additional documents identified as part of the relevant cluster and the execution time of the experiments Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process While our results represent first insights only such a technique seems to enhance SLR process promising to help researchers identify the most relevant publications more quickly and efficiently',NLP
'The increasing amount of hyperlinked human knowledge available on the Web has shifted the research focus to the search engines that will deliver the needed information quickly This demand has triggered a revisitation of the long-standing issue of semantics which was never addressed adequately by information theory or linguistic theory Fuzzy set and fuzzy logic on the other hand provide a powerful mathematical tool in modeling semantics The organization and the manipulation of information knowledge base construction and discovery information retrieval and logical inferencing all deal with semantics in a fundamental way Unless we experience a breakthrough in understanding semantics especially related to machine automation it will be difficult to make a quantum jump in accessing creating and modifying information This paper intends to bring to center stage the complex problems of semantics confronted while dealing with natural language We will demonstrate the ability of the fuzzy set and fuzzy logic to serve as a mathematical vehicle to model semantics in a significantly improved way This will include taking a closer look at the PNL Precisiated Natural Language proposed by Lotfi A Zadeh It is well established that the multiple meaning of a single word and the ambiguity of words cause the most difficulty as we attempt to automate word processing and translation There also are many other less well known difficulties A closer examination reveals that words often become confusing when they have similar forms and sounds An incorrect word choice can alter the meaning of a sentence hence making language even more complicated Not only can a single word have numerous definitions and subtle meanings but it may also take on added meanings through implications The concept of connotation is the idea suggested by its place near or in association with other words or phrases Idiomatic language expressions often cannot be translated word for word creating an obstacle for machine automation and translation Two- and three-word verbs which are indispensable in the English language illustrate further obstacles These examples offer but a birds eye view of the host of difficulties associated with machine automation and translation of natural language In this paper we will demonstrate that the fuzzy set and logic theory can serve as a mathematical modeling tool that is useful effective and optimal in solving some of the obstacles mentioned above A survey of recent publications reveals an increasing involvement and growing interest in the semantics aspect of natural language I Kobayashi M Chang & M Sugeno have worked on the meaning processing of dialogue Guoqing Chen deals with semantics in fuzzy association rules P Subasic & A Huettner have investigated the problem of fuzzy semantic typing Jonathan Lawry has studied the issue of semantic modeling Fred Petry & Patrick Perrin handle knowledge discovery in texts of which semantics of the words play important role Finally Paul P Wang & A Xia use words as features in an information retrieval system for movie selections Ultimately the electrophysiological pathways in human brains and brain regions dealing with the semantics of language must be studied It is not surprising that the gene expression patterns of different brain regions hold the key to semantics which in turn are crucial to understanding and processing language In a language that is constantly changing there is always some conflict between current usage and established practice Commas and other punctuation prepositions etc all contribute to the difficulties and ambiguities associated with machine automation and processing of the so-called common faults in semantics interpretations These difficulties can be eliminated if and only if there is effective mathematical modeling of the brain from anatomical electrophysiological and genomic standpoints',NLP
'Mining query sub-intents or sub-topics is one of the important task in information retrieval It provides the user several potential queries to explore possible search intents of the user With the development of Big Data and Natural Language Processing pre-trained language models have been applied to model complex semantic information of different text resources for mining robust query sub-intents These studies usually utilize search results and query logs independently as two important resources to generate query sub-intents However we deem that the contextual information contained in search results and user interest information contained in query logs can be incorporated together to enhance the effectiveness of user sub-intent mining which can maximize the best of both resources To generate high-quality sub-intents we design a sequence-to-sequence pre-trained language model which accepts search result texts and query suggestions extracted from query logs as the input and outputs generated sub-intent phrases For modeling the relation between search results and query logs we design two information encoder and a novel attention mechanism at the decoder part At each decoding step the model weights the attention between the input search results and query logs to determine the output token The experimental results on MIMICS dataset outperform strong baseline methods in almost all evaluation metrics illustrating the effectiveness of our proposed methods We also conduct removing studies to prove the effectiveness of search results and query logs individually and then study and compare different generation paradigms of sub-intent with experiments We finally show several generated examples to illustrate the quality of our generated sub-intents directly',NLP
'Linguistic universals arise from the interaction between the processes of language learning and language use A test case for the relationship between these factors is linguistic variation which tends to be conditioned on linguistic or sociolinguistic criteria How can we explain the scarcity of unpredictable variation in natural language and to what extent is this property of language a straightforward reflection of biases in statistical learning? We review three strands of experimental work exploring these questions and introduce a Bayesian model of the learning and transmission of linguistic variation along with a closely matched artificial language learning experiment with adult participants Our results show that while the biases of language learners can potentially play a role in shaping linguistic systems the relationship between biases of learners and the structure of languages is not straightforward Weak biases can have strong effects on language structure as they accumulate over repeated transmission But the opposite can also be true: strong biases can have weak or no effects Furthermore the use of language during interaction can reshape linguistic systems Combining data and insights from studies of learning transmission and use is therefore essential ifwe are to understand howbiases in statistical learning interact with language transmission and language use to shape the structural properties of language This article is part of the themed issue Newfrontiers for statistical learning in the cognitive sciences',NLP
'Sentiment analysis and emotion recognition are emerging research fields of research that aim to build intelligent systems able to recognize and interpret human emotions Due to the applicability of these systems to almost all kinds of markets also the interest of companies and industries is grown in an exponential way in the last years and a lot of frameworks for programming these systems are introduced IBM Watson is one of the most famous and used: it offers among others a lot of services for Natural Language Processing In spite of broad-scale multi-language services most of functions are not available in a lot of secondary languages (like Italian) The main objective of this work is to demonstrate the feasibility of a translation-based approach to emotion recognition in texts written in secondary languages We present a prototypical system using IBM Watson to extract emotions from Italian text by means of Bluemix Alchemy Language Some preliminary results are shown and discussed in order to stress pro and cons of the approach',NLP
'We have created a logic-based Turing-complete language for stochastic modeling Since the inference scheme for this language is based on a variant of Pearls loopy belief propagation algorithm we call it Loopy Logic Traditional Bayesian networks have limited expressive power basically constrained to finite domains as in the propositional calculus Our language contains variables that can capture general classes of situations events and relationships A first-order language is also able to reason about potentially infinite classes and situations using constructs such as hidden Markov models(HMMs) Our language uses an Expectation-Maximization (EM) type learning of parameters This has a natural fit with the Loopy Belief Propagation used for inference since both can be viewed as iterative message passing algorithms We present the syntax and theoretical foundations for our Loopy Logic language We then demonstrate three examples of stochastic modeling and diagnosis that explore the representational power of the language A mechanical fault detection example displays how Loopy Logic can model time-series processes using an HMM variant A digital circuit example exhibits the probabilistic modeling capabilities and finally a parameter fitting example demonstrates the power for learning unknown stochastic values',NLP
'In the world of excessive business competitiveness almost every company tries to monitor its environment to exceed the competitors Getting knowledge about competitors is the basic principal of what is called Competitive Intelligence (CI) Many applications of Competitive Intelligence can be used like Opinion Mining and Foresight studies and the process of obtaining such intelligence differs according to the companys needs In this paper we will present in more details the definitions of CI and a general process grouping the most used steps in conducting such study In the end we will present some tools useful in CI',NLP
'The Convolutional Neural Networks (ConvNets) with its proper hierarchical structure are known as powerful image-recognition processing architecture In particular the ConvNets are well suited for several image processing tasks such as image classification data set computer vision and natural language processing Nevertheless the implementation of ConvNets requires a large amount of operations as the 2-D convolutional mappings that need a very large computational power In this paper the authors will investigate alternative hardware architectures based on Cellular Neural Networks (CeNNs) in order to improve the overall performances',NLP
'The dissolution of speech and language was investigated over a 4 year period in a male with an 8 year history of isolated speech and language deterioration exhibiting a non-fluent profile of primary progressive aphasia (PPA) The course of his communication impairment began with apraxia of speech and difficulty accessing word form (eg as evidenced by frequent tip-of-the-tongue errors that typically contained correct production of word onsets or first syllables but not whole words) Ultimately all aspects of oral language use deteriorated until the individual became non-vocal His ability to communicate through non-verbal modalities remained intact Results of repeated administrations of standardized tests and analyses of connected speech obtained over 2 years were compared The ability of various assessment probes to substantiate clinical impressions of noticeable declines in expressive language use was examined It was concluded that the most sensitive indices of spoken language dissolution are likely to be measures derived from connected speech Unlike level-specific subtests (ie subtests that are designed to probe relatively isolated stages of processing such as word retrieval apart from syntactic formulation) the requisite simultaneity of processing across levels during connected speech taxes the aphasics entire language processing system Three principles guided the approach to management: (i) anticipatory implementation of treatment goals; (ii) therapy is dyad oriented; and (iii) therapy is directed at the level of the disability defined as the limitations to perform specific functions within a natural context The cornerstone of this approach labelled proactive management lies in the concept that the goals are formulated and implemented in anticipation of future declines so that the patient with PPA is prepared to maximize communication effectiveness at every stage despite the relentless progression of the disease',NLP
'Which messages are more effective at inducing a change of opinion in the listener? We approach this question within the frame of Habermas theory of communicative action which posits that the illocutionary intent of the message (its pragmatic meaning) is the key Thanks to recent advances in natural language processing we are able to operationalize this theory by extracting the latent social dimensions of a message namely archetypes of social intent of language that come from social exchange theory We identify key ingredients to opinion change by looking at more than 46k posts and more than 35M comments on Reddits r/ChangeMyView a debate forum where people try to change each others opinion and explicitly mark opinion-changing comments with a special flag called delta Comments that express no intent are about 77% less likely to change the mind of the recipient compared to comments that convey at least one social dimension Among the various social dimensions the ones that are most likely to produce an opinion change are knowledge similarity and trust which resonates with Habermas theory of communicative action We also find other new important dimensions such as appeals to power or empathetic expressions of support Finally in line with theories of constructive conflict yet contrary to the popular characterization of conflict as the bane of modern social media our findings show that voicing conflict in the context of a structured public debate can promote integration especially when it is used to counter another conflictive stance By leveraging recent advances in natural language processing our work provides an empirical framework for Habermas theory finds concrete examples of its effects in the wild and suggests its possible extension with a more faceted understanding of intent interpreted as social dimensions of language',NLP
'Pharmacovigilance is the field of science devoted to the collection analysis and prevention of Adverse Drug Reactions (ADRs) Efficient strategies for the extraction of information about ADRs from free text sources are essential to support the important task of detecting and classifying unexpected pathologies possibly related to (therapy-related) drug use Narrative ADR descriptions may be collected in different ways eg either by monitoring social networks or through the so called spontaneous reporting the main method pharmacovigilance adopts in order to identify ADRs The encoding of free-text ADR descriptions according to MedDRA standard terminology is central for report analysis It is a complex work which has to be manually implemented by the pharmacovigilance experts The manual encoding is expensive (in terms of time) Moreover a problem about the accuracy of the encoding may occur since the number of reports is growing up day by day In this paper we propose MagiCoder an efficient Natural Language Processing algorithm able to automatically derive MedDRA terminologies from free-text ADR descriptions MagiCoder is part of VigiWork a web application for online ADR reporting and analysis From a practical point of view MagiCoder reduces the encoding time of ADR reports Pharmacologists have simply to review and validate the MedDRA terms proposed by MagiCoder instead of choosing the right terms among the 70K terms of MedDRA Such improvement in the efficiency of pharmacologists work has a relevant impact also on the quality of the following data analysis Our proposal is based on a general approach not depending on the considered language Indeed we developed MagiCoder for the Italian pharmacovigilance language but preliminarily analyses show that it is robust to language and dictionary changes',NLP
'In this abstract two methods for integrating textual data and textual features into ingestion processing are summarized The first method involves integrating all features including textual features into dedicated frameworks such as by using machine learning techniques In the second method text and textual features such as keywords are used to explain results returned by heterogeneous data mining In this context it is necessary to link data (eg databases images etc) and/or obtained results with textual data (eg documents and keywords)',NLP
'Natural Language Processing (NLP) is a key technique to automate Business Process Management (BPM) at different levels The performance of existing NLP based BPM methods suffer from the limited accuracy of Part of Speech (POS) tagging which is a key step in NLP pipelines Note that the performance of POS tagging highly depends on the domain of annotated training data However most state-of-the-art POS taggers are trained from corpus in newswire domain which usually have different syntax features with business process description (BPD) The syntax features of BPD domain include usually starting with an imperative verb and containing numerous out-of-vocabulary (OOV) words In this paper we propose a novel POS tagging framework to tackle these problems The main idea is that syntax feature of starting with imperative verb could be studied by enhancing the proportion of correctly POS-annotated imperative sentences in the training data The trained POS tagger could reduce the overall POS tagging error by nearly 12% compared with newswire trained POS tagger For verbs which are key words in BPD the tagging precision could be increased by 27% The lexical ambiguity caused by OOV words is solved by extracting local contextual knowledge out of images which are attached to help users understand the process better Experimental results show that the overall POS tagging accuracy could be increased by nearly 10% with contextual OOV knowledge',NLP
'This study investigates the semantic integration of data extracted from archaeological datasets with information extracted via natural language processing (NLP) across different languages The investigation follows a broad theme relating to wooden objects and their dating via dendrochronological techniques including types of wooden material samples taken and wooden objects including shipwrecks The outcomes are an integrated RDF dataset coupled with an associated interactive research demonstrator query builder application The semantic framework combines the CIDOC Conceptual Reference Model (CRM) with the Getty Art and Architecture Thesaurus (AAT) The NLP data cleansing and integration methods are described in detail together with illustrative scenarios from the web application Demonstrator Reflections and recommendations from the study are discussed The Demonstrator is a novel SPARQL web application with CRM/AAT-based data integration Functionality includes the combination of free text and semantic search with browsing on semantic links hierarchical and associative relationship thesaurus query expansion Queries concern wooden objects (eg samples of beech wood keels) optionally from a given date range with automatic expansion over AAT hierarchies of wood types and specialised associative relationships Following a mapping pattern approach (via the STELETO tool) ensured validity and consistency of all RDF output The user is shielded from the complexity of the underlying semantic framework by a query builder user interface The study demonstrates the feasibility of connecting information extracted from datasets and grey literature reports in different languages and semantic cross-searching of the integrated information The semantic linking of textual reports and datasets opens new possibilities for integrative research across diverse resources',NLP
'In the field of text classification researchers have repeatedly shown the value of transformer-based models such as Bidirectional Encoder Representation from Transformers (BERT) and its variants Nonetheless these models are expensive in terms of memory and computational power but have not been utilized to classify long documents of several domains In addition transformer models are also often pre-trained on generalized languages making them less effective in language-specific domains such as legal documents In the natural language processing (NLP) domain there is a growing interest in creating newer models that can handle more complex input sequences and domain-specific languages Keeping the power of NLP in mind this study proposes a legal documentation classifier that classifies the legal document by using the sliding window approach to increase the maximum sequence length of the model We used the ECHR (European Court of Human Rights) publicly available dataset which to a large extent is imbalanced Therefore to balance the dataset we have scrapped the case articles from the web and extracted the data Then we employed conventional machine learning techniques such as SVM DT NB AdaBoost and transformer-based neural networks models including BERT Legal-BERT RoBERTa BigBird ELECTRA and XLNet for the classification task The experimental findings show that RoBERTa outperformed all the mentioned BERT versions by obtaining precision recall and F1-score of 891% 862% and 867% respectively While from conventional machine learning techniques AdaBoost outclasses SVM DT and NB by achieving scores of 819% 815% and 817% for precision recall and F1-score respectively',NLP
'Singing songs can be an engaging and effective activity when learning a foreign language In this paper we describe a multi-language karaoke application called SLIONS: Singing and Listening to Improve Our Natural Speaking When developing this application we followed a user-centered design process which was informed by conducting interviews with domain experts extensive usability testing and reviewing existing gamified karaoke and language learning applications The key feature of SLIONS is that we used automatic speech recognition (ASR) to provide students with personalized granular feedback based on their singing pronunciation We also provided multi-modal instruction: audio of music and singing tracks video of a professional singer and translated text of lyrics to help students learn and master each song in the foreign language To test the efficacy of SLIONS we conducted a one-week pilot study with English and Chinese language learning students (N=15) The initial quantitative results show that our application can improve pronunciation and may improve vocabulary In addition the qualitative feedback from the students suggests that SLIONS is both fun to use and motivates students to practice speaking and singing in a foreign language',NLP
'This paper focuses on identifying the polarity of figurative language in the very short text collected from Social Network Services Although this topic is not new most computer scientists have solved this issue by using natural language processing techniques This seems difficult for non-native English speakers because they have to rely on heuristics in language Therefore our target in this work is to find a language-independent approach to solve the problem without using any semantic resources (eg dictionaries and ontologies) A statistical method based on two main features (ie (i) textual terms and (ii) sentimental patterns) is proposed to determine the sentiment degree of three popular types of figurative language (ie sarcasm irony and metaphor) We experimented on two Test sets with about 3800 tweets and used Cosine similarity as the correlation measurement for evaluating the performance The results show that our Fi-Senti model (Figurative Sentiment analysis model) well performs in determining the sentiment intensity of the figurative language with the best achievement is 08952 with sarcasm and 09011 with irony',NLP
'Machine learned tasks on seismic data are often trained sequentially and separately even though they utilize the same features (ie geometrical) of the data We present StorSeismic as a dataset-centric framework for seismic data processing which consists of neural network (NN) pretraining and fine-tuning procedures We specifically utilize a NN as a preprocessing tool to extract and store seismic data features of a particular dataset for any downstream tasks After pretraining the resulting model can be utilized later through a fine-tuning procedure to perform different tasks using limited additional training Used often in natural language processing (NLP) and lately in vision tasks bidirectional encoder representations from transformer (BERT) a form of a transformer model provides an optimal platform for this framework The attention mechanism of BERT applied here on a sequence of traces within the shot gather is able to capture and store key geometrical features of the seismic data We pretrain StorSeismic on field data along with synthetically generated ones in the self-supervised step Then we use the labeled synthetic data to fine-tune the pretrained network in a supervised fashion to perform various seismic processing tasks such as denoising velocity estimation first arrival picking and normal moveout (NMO) Finally the fine-tuned model is used to obtain satisfactory inference results on the field data',NLP
'The paper describes a method for acquiring and visualizing the Polish Sign Language gestures along with mimic sub code The software allowing visualization of sign language gestures is one of the modules of the system for the translation of texts written in the Polish language into appropriate messages of the sign language Proper understanding of the information communicated in the sign language requires the information to be presented in the multipath manner In addition to the ideographic communication ie gestures of the sign language alone also lip movement (many deaf people are able to read the movement of the lips) and elements of the mimic sub code expressing emotions and intentions of the sign language users should be taken into account Additional paths improving quality of the communication are: the text displayed during the gestures visualization and the audio channel through which the speech synthesis is realized (most deaf people are not completely deaf and the sounds they hear support the communication process) In order to obtain the most accurate and the fullest understanding of the content in the sign language all these channels are transmitted simultaneously The paper describes process of the acquisition of sign language gestures modeling of face mimicry representation and storing of the animation data in the database and visualization of this data using 3D avatar',NLP
'This work carries out the sentiment analysis of the social network Twitter regarding the presidential debate on May 23 where a hashtag was left open so viewers could give their points of view on these three candidates: Gustavo Petro Federico Gutierrez and Rodolfo Hernandez Once we extracted these Tweets contained in the hashtag they were manually classified They then went through all the pre-processing and elimination of special characters links URLs images or videos Next the TextVectorization layer from the TensorFlow library was used to convert these tweets to vectors and finally to go through the two models The results show the best results for the BERT model with an accuracy of 76% and an F1 score of 85%',NLP
'Emerging sensors computers network technologies and connected platforms result potentially in an immeasurable collection of data within plant operations This creates the possibility of solving problems innovatively Because most of the data appear to be unstructured or semi-structured organizations shall design and adopt new strategies Further workflow architectures with data analytics are needed including machine learning tools and artificial intelligence techniques before proto-type solutions can be developed We shall discuss several prospects of using (big) data analytics integrated with cloud services to produce solutions for improving plant operations The paper outlines the vision and a systematic framework highlighting the data analytics lifecycle in the area of plant operation process safety and environmental protection Four rather diverse example case studies are demonstrated including (1) deep learning-based predictive maintenance monitoring modeling (2) Natural Language Processing (NLP) for mining text (3) barrier assessment for dynamic risk mapping (DRA) and (4) correlation development for sustainability indicators It further discusses the challenges in both research and implementation of proposed solutions in the industry It is concluded that a well-balanced integrated approach including machine supporting decisions integrated with expert knowledge and available information from various key resources is required to enable more informed policy strategic and operational risk decision-making leading to safer reliable and more efficient operations',NLP
'The need for improving software processes approximated the software engineering and artificial intelligence areas A growing number of researches have used artificial intelligence techniques as a support to software development processes particularly by means of intelligent assistants that offer a knowledge-based support to software process activities This paper presents a linguistic technique based on logical semantics implemented by a semantic analyser ie an intelligent assistant functionality whose purpose is enabling the extraction of object-oriented elements from requirements in natural language',NLP
'Large pre-trained language models such as GPT-3 [10] Codex [11] and Googles language model [7] are now capable of generating code from natural language specifications of programmer intent We view these developments with a mixture of optimism and caution On the optimistic side such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world On the cautionary side since these large language models do not understand program semantics they offer no guarantees about quality of the suggested code In this paper we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques that understand the syntax and semantics of programs Further we show that such techniques can make use of user feedback and improve with usage We present our experiences from building and evaluating such a tool Jigsaw targeted at synthesizing code for using Python Pandas API using multi-modal inputs Our experience suggests that as these large language models evolve for synthesizing code from intent Jigsaw has an important role to play in improving the accuracy of the systems',NLP
'This paper examines the automatic classification of discourse types in written language data Our goal was to implement an application that provides to users the grammatical analysis of content word wrapping in a speech category identifying potential mistakes related to spelling or expressing in order to facilitate the proper categorization of the discourse depending on the text category We hypothesize that we can recognize the discourse type (here we use four kinds of texts on medical economic political and religious issues) as well as avoid some errors using a trained Naive Bayes classifier This approach can be useful to the direct beneficiaries (PR staffs from four areas: political economic religious and medical area) but also to specialists in natural language processing linguists etc',NLP
'Recurrent Neural Networks (RNNs) and transformers are deep learning models that have achieved remarkable success in several Natural Language Processing (NLP) tasks since they do not rely on handcrafted features nor enormous knowledge resources Named Entity Recognition (NER) is an essential NLP task that is used in many applications such as information retrieval question answering and machine translation NER aims to locate extract and classify named entities into predefined categories such as person organization and location Arabic NER is considered a challenging task because of the complexity and the unique characteristics of Arabic Most of the previous research on deep learning based-Arabic NER focused on Modern Standard Arabic and Dialectal Arabic which are different variations from Classical Arabic In this paper we investigate deep learning-based Classical Arabic NER using different deep neural network architectures and a BERT based contextual language model that is trained on general domain Arabic text We propose two RNN-based models by fine-tunning the pretrained BERT language model to recognize and classify named entities from Classical Arabic The pre-trained BERT contextual language model representations were used as input features to a BGRU/BLSTM model and were fine-tuned using a Classical Arabic NER dataset In addition we explore variant architectures of the proposed BERT-BGRU/BLSTM-CRF models Experimentations showed that the BERT-BGRU-CRF model outperformed the other models by achieving an F-measure of 9476% on the CANERCorpus To the best of our knowledge this is the first work that aims to recognize named entities in Classical Arabic using deep learning',NLP
'Transformers have been established as one of the most effective neural approach in performing various Natural Language Processing tasks However following common trend in modern deep architectures their scale has quickly grown to an extent that reduces the concrete possibility for several enterprises to train such models from scratch Indeed despite their high-level performances Transformers have the general drawback of requiring a huge amount of training data computational resources and energy consumption to be successfully optimized For this reason more recent architectures like Bidirectional Encoder Representations from Transformers rely on unlabeled data to pre-train the model which is later fine-tuned for a specific downstream task using a relatively smaller amount of training data In a similar fashion this paper considers a plug-and-play framework that can be used to inject multiple syntactic features like Part-of-Speech Tagging or Dependency Parsing into any kind of pre-trained Transformer This novel approach allows to perform sequence-to-sequence labeling tasks by exploiting: (i) the (more abundant) available training data that is also used to learn the syntactic features (ii) the language data that is used to pre-train the transformer model The experimental results show that our approach improves over the baseline performances of the underlying model in different datasets thus proving the effectiveness of employing syntactic language information for semantic regularization In addition we show that our architecture has a huge efficiency advantage over pure large language models Indeed by using a model with limited size but whose input data are enriched with syntactic information we show that it is possible to obtain a significant reduction of CO2 emissions without decreasing the prediction performances',NLP
'Continuous Integration is a critical problem for software maintenance in global projects compromising companies performance which tends to accumulate a high-resolution time due to the approval process conflict resolution tests and validations The process of the validation involves the commit description interpretation and can be automated by NLP-mechanisms This paper presents an intelligent NLP-based approach to evaluate whether the commits can be integrated into a certain software release based only on their descriptions Our experiments showed an accuracy of 929%',NLP
'Automatic clinical coding is an essential task in the process of extracting relevant information from unstructured documents contained in electronic health records (EHRs) However most research in the development of computer-based methods for clinical coding focuses on texts written in English due to the limited availability of medical linguistic resources in languages other than English With nearly 500 million native speakers there is a worldwide interest in processing healthcare texts in Spanish In this study we systematically analyzed transformer-based models for automatic clinical coding in Spanish Using a transfer-learning-based approach the three existing transformer architectures that support the Spanish language namely multilingual BERT (mBERT) BETO and XLM-RoBERTa (XLM-R) were first pretrained on a corpus of real-world oncology clinical cases with the goal of adapting transformers to the particularities of Spanish medical texts The resulting models were fine-tuned on three distinct clinical coding tasks following a multilabel sentence classification strategy For each analyzed transformer the domain-specific version outperformed the original general domain model across those tasks Moreover the combination of the developed strategy with an ensemble approach leveraging the predictive capacities of the three distinct transformers yielded the best obtained results with MAP scores of 0662 0544 and 0884 on CodiEsp-D CodiEsp-P and Cantemist-Coding shared tasks which remarkably improved the previous state-of-the-art performance by 116% 103% and 44% respectively We publicly release the mBERT BETO and XLMR transformers adapted to the Spanish clinical domain at https://githubcom/guilopgar/ClinicalCodingTransformerES providing the clinical natural language processing community with advanced deep learning methods for performing medical coding and other tasks in the Spanish clinical domain',NLP
'There are many linguistic morphology tools available in the market for commercial and research purposes Morphology technique are incorporated into these tools to ensure its ability to study the internal structure of natural language words This technique plays an important role in reducing the number of vocabularies used at the same time retains the semantic meaning of the knowledge in NLP system Among the algorithms implemented majority of them only has to ability to carry out stemming process instead of a lemmatization process Even with technology advancement yet none of the available lemmatization algorithms able to produce 100% accurate result Inappropriate words produced by the current algorithm might alter the overall meaning it tried to represent which will directly affect the outcome of NLP system This paper proposed a new method to handle lemmatization process during the morphological analysis The method consist three layers of lemmatization process which incorporate the implementation of a well known Stanford parser API WordNet database and adaptive learning technique Stanford parser API is implemented in the first layer of lemmatization process whereas WordNet database and adaptive learning technique are implemented in the second layer and finally another lemmatization algorithm in the final layer The lemmatised words yields from the proposed method are much more appropriate compare to the previous algorithms due to user participation in the adaptive learning technique which will ultimately improve the semantic knowledge represented and stored in the knowledge base',NLP
'The paper addresses a possible way of introducing core concepts of Computational Linguistics through problems given at the linguistic contests organized for high school students in Bulgaria and abroad Following a brief presentation of the foundation and the underlying objective of these contests we outline some of the types of problems as reflecting the different levels of language processing and the diversity of approaches and tasks to be solved By presenting the variety of problems given so far through the years we would like to attract the attention of the academic community to this captivating method through which high school students might be acquainted with the challenges and the main goals of Computational Linguistics (CL) and Natural Language Processing (NLP)(1)',NLP
'The development of Web services has changed the process of requirements engineering as well as software paradigm Traditionally requirements engineering provides information for the phase of software design whereas information for Web service discovery and composition is needed in the age of Web service This paper proposes an approach for service-oriented requirements analysis which bridges the gap between the customers initial requirements and the service requirements specifications which are the information needed for Web services discovery and composition A natural language based requirements description language is utilized to describe customers initial requirements and domain ontologies domain requirements assets are reused in the process of requirements A prototype tool is developed to support this approach',NLP
'Design patterns have proven useful in many creative fields providing content creators with archetypal reusable guidelines to leverage in projects Creating such patterns however is a time-consuming manual process typically relegated to a few experts in any given domain In this paper we describe an algorithmic method for learning design patterns directly from data using techniques from natural language processing and structured concept learning Given a set of labeled hierarchical designs as input we induce a probabilistic formal grammar over these exemplars Once learned this grammar encodes a set of generative rules for the class of designs which can be sampled to synthesize novel artifacts We demonstrate the method on geometric models and Web pages and discuss how the learned patterns can drive new interaction mechanisms for content creators',NLP
'We present a study of improving the efficiency complexity and performance of language translation process by a translator The goal of this research is to develop an efficient translation system for any language by optimizing the memory consumption and also identifying the names as nouns efficiently Although a number of researches can be found on natural language processing in different areas those were performed keeping English as the only target language mostly However a good number of languages remain nearly unexplored in the research fields yet This study basically focuses on Bengali Language as an example of the unexplored languages Some noticeable studies on Bengali language are on Bangla keyboard layout design English to Bangla translator etc so far However very few researches have been done to translate Bengali text to English till now To develop an efficient translation system is very complex and expensive as it requires huge amount of time and resources In all the languages there are many words having multiple meanings and multiple forms and also some sentences having multiple grammatical structures to express the same meaning Besides the names of people may not be easily identified due to the vast diversity of names and also the tags (prefix/suffix) attached to emphasize names Therefore it remains a great challenge to recognize a sentence of a particular language with accurate semantic analysis However it is very important to have a generalised translation system which can compute various possible outputs in reasonable time and space In this paper we focus on the correct interpretation of the names as nouns in a sentence and also the optimization of space requirement using semantic analysis',NLP
'The perception and production of nonnative phones in second language (L2) learners can be improved via auditory training but L2 learning is often characterized by large differences in performance across individuals This study examined whether success in learning L2 vowels via five sessions of high-variability phonetic training related to the learners native (L1) vowel processing ability or their frequency discrimination acuity A group of native speakers of Greek received training while another completed the pre-/post-tests but without training Pre-/post-tests assessed different aspects of their L2 and L1 vowel processing and frequency acuity L2 and L1 vowel processing were assessed via: (a) Natural English (L2) vowel identification in quiet and in multi-talker babble and natural Greek (L1) vowel identification in babble; (b) the categorization of synthetic English and Greek vowel continua; and (c) discrimination of the same continua Frequency discrimination acuity was assessed for a nonspeech continuum Frequency discrimination acuity was related to measures of both L1 and L2 vowel processing a finding that favors an auditory processing over a speech-specific explanation for individual variability in L2 vowel learning The most efficient frequency discriminators at pre-test were also the most accurate both in English vowel perception and production after training (C) 2010 Acoustical Society of America [DOI: 101121/13506351]',NLP
'This paper presents a method of combining Conditional Random Fields (CRFs) model with a post-processing layer using Google n-grams statistical information tailored to detect word selection and word order errors made by learners of Chinese as Foreign Language (CFL) We describe the architecture of the model and its performance in the shared task of the ACL 2018 Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA) This hybrid approach yields comparably high false positive rate (FPR = 01274) and precision (P-d = 07519; P-i = 06311) but low recall (R-d = 03035; R-i = 01696) in grammatical error detection and identification tasks Additional statistical information and linguistic rules can be added to enhance the model performance in the future',NLP
'Ontologies play an important role in the Semantic Web as well as in digital library and knowledge portal applications This project seeks to develop an automatic method to enrich existing ontologies especially in the identification of semantic relations between concepts in the ontology The initial study investigates an approach of identifying pairs of related concepts in a medical domain using association rule induction and inferring the type of semantic relation using the UMLS (Unified Medical Language System) semantic net This is evaluated by comparing the result with manually assigned semantic relations based on an analysis of medical abstracts containing each pair of concepts Our initial finding shows that the automatic process is promising achieving a 68% coverage compared to manually tagging However natural language processing of medical abstracts is likely to improve the identification of semantic relations',NLP
'We describe FieldBroker a software architecture dedicated to data parallel computations on fields over ZZ(n) Fields are a natural extension of the parallel array data structure From the application point of view field operations are processed by a field server leading to a client/server architecture Requests are translated successively in three languages corresponding to a tower of three virtual machines processing respectively mappings on ZZ(n) sets of arrays and flat vectors in core memory The server is itself designed as a master/multithreaded-slaves program The aim of FieldBroker is to mutually incorporate approaches found in distributed computing functional programming and the data parallel paradigm It provides a testbed for experiments with language constructs evaluation mechanisms on-the-fly optimizations load-balancing strategies and data field implementations',NLP
'For a number of languages web crawling allows researchers to collect huge text samples to build corpora However only part of the material found on the internet is useful for Natural Language Processing as e g parsers typically cannot handle lists and tables or very short or very long sentences There are methods (cf eg [3]) for cleaning the downloaded data before adding it to a corpus collection - but even when these are applied not all remaining textual material might be suitable for certain research requirements This paper describes methods utilized to prepare deWaC a freely available German web corpus of the WaCky project for automatic processing up to the parsing level It then discusses ways in which this corpus called SdeWaC has been used since its release',NLP
'We present an interdisciplinary approach to the investigation of strategies and heuristics reflecting the cognitive processes underlying human wayfinding To achieve this we symmetrically investigate navigation behavior and associated language This novel approach combines two completely different and independent directions of research that complement each other naturally and necessarily but which have seldom been directly combined so far The current focus on wayfinding strategies and heuristics is a fairly new scientific goal both in behavioral and linguistic research areas; also the methods of discourse analysis have rarely been directly adopted to systematically investigate parallels between natural discourse and navigation behavior In this paper we outline and motivate our approach and present first results gained in combined empirical investigation',NLP
'In this study the concept of first language and its relationship with the theme of passion in Rousseaus Essay on the Origin of Languages is studied Some of Rousseaus basic arguments related to the origin of language coincide with the first invention of speech that is based solely on passions Such impulses make human beings get together in a state of nature while the role of passion in human beings lives cannot be reduced to that state of nature because this relationship is seen by Rousseau as at the border between the state of society and the state of nature In this paper it is argued that passions are the social in the natural that is to say they work both in the foundations of the first as well as in the complicated process of civilized language For Rousseau the first language is figurative poetic melodic and songlike which includes a more immediate way of communication than our writing-centered language (C) 2012 Published by Elsevier Ltd Selection and/or peer-review under responsibility of ALSC 2012',NLP
'No human being can understand every text or dialog in his or her native language and no one should expect a computer to do so However people have a remarkable ability to learn and to extend their understanding without explicit training Fundamental to human understanding is the ability to learn and use language in social interactions that Wittgenstein called language games Those language games use and extend prelinguistic knowledge learned through perception action and social interactions This article surveys the technology that has been developed for natural language processing and the successes and failures of various attempts Although many useful applications have been implemented the original goal of language understanding seems as remote as ever Fundamental to understanding is the ability to recognize an utterance as a move in a social game and to respond in terms of a mental model of the game the players and the environment Those models use and extend the prelinguistic models learned through perception action and social interactions Secondary uses of language such as reading a book are derivative processes that elaborate and extend the mental models originally acquired by interacting with people and the environment A computer system that relates language to virtual models might minfic some aspects of understanding but full understanding requires the ability to learn and use new knowledge in social and sensory-motor interactions These issues are illustrated with an analysis of some NLP systems and a recommended strategy for the future None of the systems available today can understand language at the level of a child but with a shift in strategy there is hope of designing more robust and usable systems in the future',NLP
'Sweep mathematical morphology allows one to select varying shapes and orientations of structuring elements during the sweeping process The sweep dilation/erosion provides a natural representation of sweep motion in the manufacturing processes and the sweep opening/closing provides variant degrees of smoothing in image filtering and edge linking A set of grammatical rules that govern the generation of objects belonging to the same group can be defined Earleys parser serves in the screening process to determine whether a pattern is a part of the language Sweep mathematical morphology is an intuitive and efficient tool for object representation',NLP
'SAFEDPI is a distributed version of the PICALCULUS in which processes are located at dynamically created sites Parametrised code may be sent between sites using so-called ports which are essentially higher-order versions of PICALCULUS communication channels A host location may protect itself by only accepting code which conforms to a given type associated to the incoming port We define a sophisticated static type system for these ports which restrict the capabilities and access rights of any processes launched by incoming code Dependent and existential types are used to add flexibility allowing the behaviour of these launched processes encoded as process types to depend on the hosts instantiation of the incoming code We also show that a natural contextually defined behavioural equivalence can be characterised coinductively using bisimulations based on typed actions The characterisation is based on the idea of knowledge acquisition by a testing environment and makes explicit some of the subtleties of determining equivalence in this language of highly constrained distributed code',NLP
'Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades As a result the most accurate parsers are domain specific complex and inefficient In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset when trained on a large synthetic corpus that was annotated using existing parsers It also matches the performance of standard parsers when trained only on a small human-annotated dataset which shows that this model is highly data-efficient in contrast to sequence-to-sequence models without the attention mechanism Our parser is also fast processing over a hundred sentences per second with an unoptimized CPU implementation',NLP
'Requirements engineerings continuing dependence on natural language description has made it the focus of several efforts to apply language engineering techniques The raw textual material that forms an input to early phase requirements engineering and which informs the subsequent formulation of the requirements is inevitably uncontrolled and this makes its processing very hard Nevertheless sufficiently robust techniques do exist that can be used to aid the requirements engineer provided that the scope of what can be achieved is understood In this paper we show how combinations of lexical and shallow semantic analysis techniques developed from corpus linguistics can help human analysts acquire the deep understanding needed as the first step towards the synthesis of requirements',NLP
'We present a modular framework for the rapid-prototyping of linguistic web-based visual analytics applications Our framework gives developers access to a rich set of machine learning and natural language processing steps through encapsulating them into micro-services and combining them into a computational pipeline This processing pipeline is auto-configured based on the requirements of the visualization front-end making the linguistic processing and visualization design detached independent development tasks This paper describes the constellation and modality of our framework which continues to support the efficient development of various human-in-the-loop linguistic visual analytics research techniques and applications',NLP
'Process Modeling has been a very active research topic for the last decades One of its main issues is the externalization of knowledge and its acquisition for further use as this remains deeply related to the quality of the resulting process models produced by this task This paper presents a method and a graphical supporting tool for process elicitation and modeling combining the Group Storytelling technique with the advances of Text Mining and Natural Language Processing The implemented tool extends its previous versions with several functionalities to facilitate group story telling by the users as well as to improve the results of the acquired process model from the stories',NLP
'The Radiotherapy Incident Reporting and Analysis System (RIRAS) receives incident reports from Radiation Oncology facilities across the US Veterans Health Affairs (VHA) enterprise and Virginia Commonwealth University (VCU) In this work we propose a computational pipeline for analysis of radiation oncology incident reports Our pipeline uses machine learning (ML) and natural language processing (NLP) based methods to predict the severity of the incidents reported in the RIRAS platform using the textual description of the reported incidents These incidents in RIRAS are reviewed by a radiation oncology subject matter expert (SME) who initially triages some incidents based on the salient elements in the incident report To automate the triage process we used the data from the VHA treatment centers and the VCU radiation oncology department We used NLP combined with traditional ML algorithms including support vector machine (SVM) with linear kernel and compared it against the transfer learning approach with the universal language model fine-tuning (ULMFiT) algorithm In RIRAS severities are divided into four categories; A B C and D with A being the most severe to D being the least In this work we built models to predict High (A & B) vs Low (C & D) severity instead of all the four categories Models were evaluated with macro-averaged precision recall and F1-Score The Traditional ML machine learning (SVM-linear) approach did well on the VHA dataset with 078 F1-Score but performed poorly on the VCU dataset with 05 F1-Score The transfer learning approach did well on both datasets with 081 F1-Score on VHA dataset and 068 F1-Score on the VCU dataset Overall our methods show promise in automating the triage and severity determination process from radiotherapy incident reports',NLP
'Increased industrialization and new markets have led to an accumulation of used technical consumer goods which results in greater exploitation of raw materials energy and landfill sites In order to reduce the use of natural resources conserve precious energy and limit the increase in waste volume The application of disassembly techniques is the first step towards this prevention of waste These techniques form a reliable and clean approach: noble or high-graded recycling This paper presents a multi agent system for disassembly process which is implemented in a computer-aided application for supervising of the disassembling system: the Interactive Intelligent Interface for Disassembling System Unified modeling language diagrams are used for an internal and external definition of the disassembling system',NLP
'The paper discusses the possibilities of applying modern natural language processing technologies of opinion mining to investigate and improve the user experience of online-courses students We analyzed 27 000 student reviews of projects within the Python programming language course First we applied keyword extraction algorithms as a way of semantic compression to receive a generalized picture of what users main impressions are Then we performed sentiment analysis to understand the feelings of students towards the learning process The used methodology proved to be effective for analyzing user experience and allowed to find out some discrepancies between information in project descriptions and what users reflection on the project Two instruments of SA were applied to receive data on users feelings in general',NLP
'Light verb constructions (LVCs) are verb and noun combinations in which the verb has lost its meaning to some degree and the noun is used in one of its original senses typically denoting an event or an action They exhibit special linguistic features especially when regarded in a multilingual context In this paper we focus on the automatic detection of LVCs in raw text in four different languages namely English German Spanish and Hungarian First we analyze the characteristics of LVCs from a linguistic point of view based on parallel corpus data Then we provide a standardized (ie language-independent) representation of LVCs that can be used in machine learning experiments After we experiment on identifying LVCs in different languages: we exploit language adaptation techniques which demonstrate that data from an additional language can be successfully employed in improving the performance of supervised LVC detection for a given language As there are several annotated corpora from several domains in the case of English and Hungarian we also investigate the effect of simple domain adaptation techniques to reduce the gap between domains Furthermore we combine domain adaptation techniques with language adaptation techniques for these two languages Our results show that both out-domain and additional language data can improve performance We believe that our language adaptation method may have practical implications in several fields of natural language processing especially in machine translation',NLP
'Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example by segregating information and organizing spike timing Recent data show that delta theta and gamma oscillations are specifically engaged by the multi-timescale quasi-rhythmic properties of speech and can track its dynamics We argue that they are foundational in speech and language processing packaging incoming information into units of the appropriate temporal granularity Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition',NLP
'Sentiment analysis (SA) is the automatic process of understanding peoples feelings or beliefs expressed in texts such as emotions opinions attitudes appraisals and others The main task is to identify the polarity level (positive neutral or negative) of a given text This task has been the subject of several research competitions in many languages for instance English Spanish and Arabic However developing a multilingual sentiment analysis method remains a challenge In this paper we propose a new approach called BPA based on BiLSTM neural networks pooling operations and attention mechanism which is able to automatically classify the polarity level of a text We evaluated the BPA approach using five different data sets in three distinct languages: English Spanish and Portuguese Experimental results evidence the suitability of the proposed approach to multilingual and domain-independent polarity classification BPAs best results achieved an accuracy of 0901 0865 and 0923 for English Spanish and Portuguese respectively',NLP
'This paper discusses a business process model and notation (BPMN) extension that includes new elements designed to improve its expressiveness In previous works different shortcomings concerning the BPMN language were detected As a result a set of requirements to overcome these issues was collected and used to guide this work The proposed extension supports the representation of information commonly used by experts in the hazard analysis and critical control points domain usually expressed in natural language in a machine-understandable fashion To take full advantage of the features introduced in this BPMN extension tools such as ProM can be easily upgraded with appropriate plugins to support the new elements In this line an advanced conformance checking plugin was developed for process mining on BPMN models A real-world example of use showing the benefits of applying the new elements is also discussed This proposal paves the way for novel advanced analysis mechanisms for traceability systems',NLP
'The channels for expressing opinions seem to increase daily and hence they are important sources of business insight For product planning marketing and customer service it is necessary to capture and analyse these opinions The social blogs are massive corpus for text and opinion mining So in this competitive and greedy world we find the need of analysis of the opinions about the products a vital task to evaluate and improve the product quality and as a result these opinions are relevant to companies These opinions are easily accessed through the social blogs This paper discusses various methods in performing sentiment analysis We can see that presently we are provided facility to post the opinions in different languages and this paper tried to consider the processing of texts of different languages that are posted in blogs Along with that the relevance of the quality of dataset used for the analysis are also the subject of discussion in this paper',NLP
'Gestures are an important part of human communication However little is known about the neural correlates of gestures accompanying speech comprehension The goal of this study is to investigate the neural basis of speech-gesture interaction as reflected in activation increase and decrease during observation of natural communication Fourteen German participants watched video clips of 5s duration depicting an actor who performed metaphoric gestures to illustrate the abstract content of spoken sentences Furthermore video clips of isolated gestures (without speech) isolated spoken sentences (without gestures) and gestures in the context of an unknown language (Russian) were additionally presented while functional magnetic resonance imaging (fMRI) data were acquired Bimodal speech and gesture processing led to left hemispheric activation increases of the posterior middle temporal gyrus the premotor cortex the inferior frontal gyrus and the right superior temporal sulcus Activation reductions during the bimodal condition were located in the left superior temporal gyrus and the left posterior insula Gesture related activation increases and decreases were dependent on language semantics and were not found in the unknown-language condition Our results suggest that semantic integration processes for bimodal speech plus gesture comprehension are reflected in activation increases in the classical left hemispheric language areas Speech related gestures seem to enhance language comprehension during the face-to-face communication (C) 2008 Elsevier Ltd All rights reserved',NLP
'Coreference resolution is an important part of natural language processing used in machine translation semantic search and various other information retrieval and understanding systems One of the challenges in this field is an evaluation of resolution approaches There are many different metrics proposed but most of them rely on certain assumptions like equivalence between different mentions of the same discourse-world entity and do not account for overrepresentation of certain types of coreferences present in the evaluation data In this paper a new coreference evaluation strategy that focuses on linguistic and semantic information is presented that can address some of these shortcomings Evaluation model was developed in the broader context of developing coreference resolution capabilities for Lithuanian language; therefore the experiment was also carried out using Lithuanian language resources but the proposed evaluation strategy is not language-dependent',NLP
'Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang We perform rigorous evaluation on two large-scale online slang dictionaries and show that our approach not only achieves state-of-the-art accuracy for slang interpretation in English but also does so in zero-shot and few-shot scenarios where training data is sparse Furthermore we show how the same framework can be applied to enhancing machine translation of slang from English to other languages Our work creates opportunities for the automated interpretation and translation of informal language',NLP
'In an era that searching the WWW for information becomes a tedious task it is obvious that mainly search engines and other data mining mechanisms need to be enhanced with characteristics such as NLP in order to better analyze and recognize user queries and fetch data We present an efficient mechanism for stemming and tagging for the Greek language Our system is constructed in such a way that can be easily adapted to any existing system and support it with recognition and analysis of Greek words We examine the accuracy of the system and its ability to support peRSSonal a medium constructed for offering meta-portal news services to internet users We present experimental evaluation of the system compared to already existing stemmers and taggers of the Greek language and we prove the higher efficiency and quality of results of our system',NLP
'Essential for the success of FAQ systems is their ability to systematically manage knowledge including the intelligent retrieval of useful FAQ documents and the continuous evolution of the knowledge base Based on our experience we propose a hybrid approach for the management of FAQ documents on programming languages written in Portuguese Spanish or other latin languages Our approach integrates various types of knowledge and provides intelligent mechanisms for knowledge access as well as the continuous evolution and improvement of the FAQ system throughout its life cycle The principal strength of this approach lies in the integration of Case-Based Reasoning and Information Retrieval techniques customized to the specific requirements and characteristics of FAQ document management Our work is currently being implemented and evaluated in the context of an international research project',NLP
'Over the past two decades the construction of models for medical concept representation and for understanding of the deep meaning of medical narrative texts have been challenging areas of medical informatics research This review highlights how these two inter-related domains have evolved emphasizing aspects of medical modeling as a tool for medical language understanding A representation scheme which balances partially but accurately with complete but complex representations of domain-specific knowledge must be developed to facilitate language understanding Representative examples are drawn from two major independent efforts undertaken by the authors: the elaboration and the subsequent adjustment of the RECIT multilingual analyzer to include a robust medical concept model and the recasting of a frame-based interlingua system originally developed to map equivalent concepts between controlled clinical vocabularies to invoke a similar concept model',NLP
'In this article we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements examples and metamodels We introduce a synthesis process based on metamodelmatching correspondence patterns between metamodels and completeness and consistency analysis ofmatches We describe how the limitations of metamodelmatching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically thus potentially reducing development effort We also evaluate the efficiency of synthesised transformations Our novel contributions are: The concept of correspondence patterns between metamodels of a transformation Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML) Symbolic MTBE using predictive specification to infer transformations from examples Transformation generation in multiple MT languages and in Java from an abstract intermediate language',NLP
'Transliteration consists of automatically transforming a graphemes transcription from one writing system to another while preserving its pronunciation It is usually used in the context of machine translation and cross language information retrieval mainly to deal with the issue of named entities and technical terms In the case of some Arabic dialects which are used on the social web in both Latin and Arabic scripts and which are still low-resource languages transliteration is of great benefit for the automatic generation of various linguistic resources (parallel corpora and lexica) useful for their automatic processing In this work we focus on the Tunisian dialect transliteration We propose a deep learning based Sequence-to-Sequence approach to perform a word-level transliteration of the user generated Tunisian dialect on the social web in both Latin to Arabic and Arabic to Latin senses (C) 2018 The Authors Published by Elsevier BV',NLP
'In this paper we present automatic Unified Modeling Language (UML) documents generation method using Natural Language Processing (NLP) This method consists of following five steps: 1) apply NLP to requirement specifications written in Japanese pick up all nouns and predications and develop noun and predication lists 2) clarify all objects and classes from the noun list add some operations to them and develop the Object and Class diagrams 3) define all message sequences and directions between classes and develop the Sequence diagrams 4) give a name to each Sequence diagram as a Use-Case gather all Use-Cases and develop the Use-Case diagrams 5) define the states and transition of the classes and develop the State Machine diagrams We also developed the support tools for processing this method and could develop UML documents adequately and efficiently Average correct answer rate of UML documents improved 20[%] in comparison with no method',NLP
'Language model pre-training has spurred a great deal of attention for tasks involving natural language understanding and has been successfully applied to many downstream tasks with impressive results Within information retrieval many of these solutions are too costly to stand on their own requiring multi-stage ranking architectures Recent work has begun to consider how to backport salient aspects of these computationally expensive models to previous stages of the retrieval pipeline One such instance is DeepCT which uses BERT to re-weight term importance in a given context at the passage level This process which is computed offline results in an augmented inverted index with re-weighted term frequency values In this work we conduct an investigation of query processing efficiency over DeepCT indexes Using a number of candidate generation algorithms we reveal how term re-weighting can impact query processing latency and explore how Deep CT can be used as a static index pruning technique to accelerate query processing without harming search effectiveness',NLP
'Twitter Sentiment Classification is undergoing great appeal from the research community; also user posts and opinions are producing very interesting conclusions and information In the context of this paper a pre-processing tool was developed in Python language This tool processes text and natural language data intending to remove wrong values and noise The main reason for developing such a tool is to achieve sentiment analysis in an optimum and efficient way The most remarkable characteristic is considered the use of emojis and emoticons in the sentiment analysis field Moreover supervised machine learning techniques were utilized for the analysis of users posts Through our experiments the performance of the involved classifiers namely Naive Bayes and SVM under specific parameters such as the size of the training data the employed methods for feature selection (unigrams bigrams and tri-grams) are evaluated Finally the performance was assessed based on independent datasets through the application of k-fold cross validation',NLP
'Introduction It is amazing how all normal children systematically exposed to a natural language speak using well-formed sentences by the time they are about 3 years old This is a universally observed fact that is made possible by a series of mechanisms which mostly remain unexplained This article reviews a series of studies concerning the linguistic capabilities of infants aged up to twelve months which were evaluated using both behavioural and neuroimaging methods Development Learning a language means learning its sounds its words and its grammatical rules During the first year of life this learning is not very apparent since it is essentially perceptive and seems to occur without the need to make an effort Yet acquiring ones native language is no trivial matter because speech does not offer any known physical evidence that systematically indicates the occurrence of sounds words and/or grammatical rules A human being starts to process speech just a few hours after birth; nevertheless this early sensitivity is not enough to know a particular language In fact during their early years children will have to learn the properties that are relevant to their mother tongue while at the same time ignoring those that are irrelevant Processing language is accompanied by changes in brain activity which can be explored using safe neuroimaging methods such as electroencephalogram recordings and optical topography Conclusion A deeper understanding of the cognitive and neural foundations of the early linguistic skills of children will make a significant contribution to dealing with both normal development and language disorders',NLP
'SAFEDPI is a distributed version of the PICALCULUS in which processes are located at dynamically created sites Parametrised code may be sent between sites using so-called ports which are essentially higher-order versions of PICALCULUS communication channels A host location may protect itself by only accepting code which conforms to a given type associated to the incoming port We define a sophisticated static type system for these ports which restrict the capabilities and access rights of any processes launched by incoming code Dependent and existential types are used to add flexibility allowing the behaviour of these launched processes encoded as process types to depend on the hosts instantiation of the incoming code We also show that a natural contextually defined behavioural equivalence can be characterised coinductively using bisimulations based on typed actions The characterisation is based on the idea of knowledge acquisition by a testing environment and makes explicit some of the subtleties of determining equivalence in this language of highly constrained distributed code',NLP
'In this paper we conduct research on cognitive theory and its applications on contemporary development of English language and the literature The development of cognitive linguistics is not an accident it reveals a kind of natural law in the development of the literature as is a result of the collision theory of new things is a kind of literary criticism of the overwhelming trend however the creation of a new theory in any must experience a long process of the evolution and development Cognitive poetics development trend from abroad and also can see emerging discipline localization in the future Foreign interdisciplinary cognitive poetics is strong and in the process of its localization also should apply the principle of psychology anthropology and other disciplines to the construction of cognitive poetics theory Under the background we integrate the related research to propose our new perspective that is meaningful',NLP
'Cognates are words in different languages that have similar spelling and meaning The identification of cognates is very useful for many different Natural Language Processing tasks and also in the process of learning a second language This paper presents a new approach to classify pairs of words into cognates/false friends or not related classes The proposed approach uses a fuzzy system to combine complementary string similarity measures in order to improve the cognate identification task The underlying hypothesis is that the combination of different string measures by applying heuristic knowledge can outperform those measures working separately The results obtained by the proposed system confirm the previous hypothesis and furthermore it also outperforms other systems that combine string measures by using a supervised approach As an additional contribution we have created a bilingual test data set which include pairs of cognates false friends and unrelated words in Spanish and English that is freely available for research purposes',NLP
'Much of the statistical learning literature has focused on adjacent dependency learning which has shown that learners are capable of extracting adjacent statistics from continuous language streams In contrast studies on non-adjacent dependency learning have mixed results with some showing success and others failure We review the literature on non-adjacent dependency learning and examine various theories proposed to account for these results including the proposed necessity of the presence of pauses in the learning stream or proposals regarding competition between adjacent and non-adjacent dependency learning such that high variability of middle elements is beneficial to learning Here we challenge those accounts by showing successful learning of non-adjacent dependencies under conditions that are inconsistent with predictions of previous theories We show that non-adjacent dependencies are learnable without pauses at dependency edges in a variety of artificial language designs Moreover we find no evidence of a relationship between non-adjacent dependency learning and the robustness of adjacent statistics We demonstrate that our two-step statistical learning model can account for all of our non-adjacent dependency learning results and provides a unified learning account of adjacent and non-adjacent dependency learning Finally we discussed the theoretical implications of our findings for natural language acquisition and argue that the dependency learning process can be a precursor to other language acquisition tasks that are vital to natural language acquisition',NLP
'Humans differ substantially in their ability to implicitly extract structural regularities from experience as required for learning the grammar of a language The mechanisms underlying this fundamental inter-individual difference which may determine initial success in language learning are incompletely understood Here we use diffusion tensor magnetic resonance imaging (DTI) to determine white matter integrity around Brocas area which is crucially involved in both natural and artificial language processing Twelve young right-handed individuals completed an artificial grammar learning task and DTI of their brains were acquired Inter-individual variability in performance correlated with white matter integrity (increasing fractional anisotropy (FA)) in fibres arising from Brocas area (left BA 44/45) but not from its right-hemispheric homologue Variability in performance based on superficial familiarity did not show this association Moreover when Brocas area was used as a seed mask for probabilistic tractography we found that mean FA values within the generated tracts was higher in subjects with better grammar learning Our findings provide the first evidence that integrity of white matter fibre tracts arising from Brocas area is intimately linked with the ability to extract grammatical rules The relevance of these findings for acquisition of a natural language has to be established in future studies (C) 2009 Elsevier Inc All rights reserved',NLP
'Entity and relation extraction is the necessary step in structuring medical text However the feature extraction ability of the bidirectional long short term memory network in the existing model does not achieve the best effect At the same time the language model has achieved excellent results in more and more natural language processing tasks In this paper we present a focused attention model for the joint entity and relation extraction task Our model integrates well-known BERT language model into joint learning through dynamic range attention mechanism thus improving the feature representation ability of shared parameter layer Experimental results on coronary angiography texts collected from Shuguang Hospital show that the F-1-scores of named entity recognition and relation classification tasks reach 9689% and 8851% which outperform state-of-the-art methods by 165% and 122% respectively',NLP
'Word sense disambiguation (WSD) the task of identifying the intended sense of words has been a growing research area in the field of natural language processing In this paper the authors focused on word sense disambiguation for Hindi language using graph connectivity measures and Hindi WordNet[1] To construct the graph for the sentence each sense of the ambiguous word is taken as a source node and all the paths which connects the sense to other words present in the sentence are added The importance of nodes in the constructed graph are identified using node neighbor based measures (various centrality) and graph clustering based measures (denseness graph randomness edge density) The proposed method disambiguates all open class words (noun verb adjective adverb) and disambiguates all the words present in the sentence simultaneously',NLP
'This paper describes the so-called ill-formed nature of spontaneous conversational speech as observed from the study of a 1500-hour corpus of recorded dialogue speech We note that the structure is quite different from that of more formal speech or writing and propose a Statistical Machine Translation approach for mapping between the spoken and written forms of the language as if they were two entirely separate languages We further posit that the particular nature of the spoken language is especially well suited for the display of affective states inter-speaker relationships and discourse management information In summary both modes of communication appear to be particularly suited to their pragmatic function neither is ill-formed and it appears possible to map automatically between the two This mapping has applications in speech technology for the processing of conversational speech',NLP
'Transformer based language models such as BERT have been widely applied to many domains through model pretraining and fine tuning However in low-resource scenarios such as clinical cases customizing a BERT-based language model is still a challenging task In this paper we focus on the radiotherapy domain and train a ClinicalRadioBERT model for analyzing clinical notes through a two-step procedure First we fine tune a BioBERT model by exploiting full texts of radiotherapy literature and name this model as RadioBERT Second we propose a knowledge-infused few-shot learning (KI-FSL) approach that leverages domain knowledge and trains the ClinicalRadioBERT model for understanding radiotherapy clinical notes We evaluate ClinicalRadioBERT on a newly collected clinical notes dataset and demonstrate its superiority over baselines on few-shot named entity recognition We will apply the ClinicalRadioBERT to link BERT and medical imaging for radiotherapy',NLP
'The ways we express ourselves in writing and speaking reveal who we are Historically most psychologists social media experts and even computer scientists have focused more on what people were saying rather than how they were saying what they were saying Language content is of course critical to basic communication Equally interesting is an analysis of common words associated with speaking style - words such as pronouns prepositions articles and other function words An increasing number of studies have found that function words (also thought of as stop words) provide clues to deception status intelligence emotional state the quality of social relationships and personality In addition to summarizing recent research on the social dynamics of language the talk will point to the natural alliance of computer science and the field of social and personality psychology (C) 2017 The Authors Published by Elsevier BV',NLP
'Neuronal oscillations putatively track speech in order to optimize sensory processing However it is unclear how isochronous brain oscillations can track pseudo-rhythmic speech input Here we propose that oscillations can track pseudo-rhythmic speech when considering that speech time is dependent on content-based predictions flowing from internal language models We show that temporal dynamics of speech are dependent on the predictability of words in a sentence A computational model including oscillations feedback and inhibition is able to track pseudo-rhythmic speech input As the model processes it generates temporal phase codes which are a candidate mechanism for carrying information forward in time The model is optimally sensitive to the natural temporal speech dynamics and can explain empirical data on temporal speech illusions Our results suggest that speech tracking does not have to rely only on the acoustics but could also exploit ongoing interactions between oscillations and constraints flowing from internal language models',NLP
'While identifying the intention of an utterance has played a major role in natural language understanding this work is the first to extend intention recognition to the domain of information graphics A tenet of this work is the belief that information graphics are a form of language This is supported by the observation that the overwhelming majority of information graphics from popular media sources appear to have some underlying goal or intended message As Clark noted language is more than just words It is any signal (or lack of signal when one is expected) where a signal is a deliberate action that is intended to convey a message (Clark 1996 [15]) As a form of language information graphics contain communicative signals that can be used in a computational system to identify the message that the graphic conveys We identify the communicative signals that appear in simple bar charts and present an implemented Bayesian network methodology for reasoning about these signals and hypothesizing a bar charts intended message Once the message conveyed by an information graphic has been inferred it can then be used to facilitate access to this information resource for a variety of users including 1) users of digital libraries 2) visually impaired users and 3) users of devices where graphics are impractical or inaccessible (C) 2010 Elsevier BV All rights reserved',NLP
'Translation of named entities (NE) including proper names temporal and numerical expressions is very important in multilingual natural language processing like crosslingual information retrieval and statistical machine translation In this paper we present an integrated approach to extract a named entity translation dictionary from a bilingual corpus while at the same time improving the named entity annotation quality Starting from a bilingual corpus where the named entities are extracted independently for each language a statistical alignment model is used to align the named entities An iterative process is applied to extract named entity pairs with higher alignment probability This leads to a smaller but cleaner named entity translation dictionary and also to a significant improvement of the monolingual named entity annotation quality for both languages Experimental result shows that the dictionary size is reduced by 518% and the annotation quality is improved from 7003 to 7815 for Chinese and 7338 to 8146 in terms of F-score',NLP
'Place descriptions are used in everyday communication as a common way to convey spatial information Processing the information from place descriptions poses multiple significant challenges because these descriptions are written in natural language In particular corpora of place descriptions provide a plethora of human spatial knowledge beyond geographical information system even if these descriptions refer to the same places in various ways This article focuses on resolving ambiguous or synonymous place names from place descriptions by exploring the given relationships with other spatial features It matches place names from multiple descriptions by developing a novel labelled graph matching process that relies solely on the comparison of string linguistic and spatial similarities between identified places This process uses unstructured place descriptions as an input and produces a composite place graph with qualitative spatial relations from the descriptions The performance of this novel process exceeds current toponym resolution by coping with non-gazetteered places',NLP
'Developing international multilingual terminologies is a time-consuming process We present a methodology which aims to ease this process by automatically acquiring new translations of medical terms based on word alignment in parallel text corpora and test it on English and French After collecting a parallel English-French corpus we detected French translations of English terms from three terminologies-MeSH SNOMED CT and the MedlinePlus Health Topics We obtained respectively for each terminology 748% 778% and 763% of linguistically correct new translations A sample of the MeSH translations was submitted to expert review and 615% were deemed desirable additions to the French MeSH In conclusion we successfully obtained good quality new translations which underlines the suitability of using alignment in text corpora to help translating terminologies Our method may be applied to different European languages and provides a methodological framework that may be used with different processing tools (C) 2009 Elsevier Inc All rights reserved',NLP
'EspyInsideFunction allows to write software in the Julia programming language Julia (2017) [1] to make the value of variables within a functions local scope - variables that are neither arguments nor return values available to the caller This is relevant for functions within a solution process (eg a function which return value is to be minimized by some iterative scheme) In such a setting it is natural to tailor the functions interface to the solution process However internal results within the function while not relevant to the solution process may be wanted output from the analysis The package allows to write such a function with an interface tailored for the solution process and then uses meta-programming to create a second version of the function with a modified interface which can be called to extract relevant intermediate results (c) 2022 The Author(s) Published by Elsevier BV This is an open access article under the CC BY license (http://creativecommonsorg/licenses/by/40/)',NLP
'Designing and developing a system that assists the users in digesting and understanding information available has been a difficult challenge In this paper we discuss the design and development of an automatic interactive keyphrase extraction system called KPSpotter which is capable of processing various formats of data such as XML HTML and plain text through Internet KPSpotter combines Information Gain data mining measure and several Natural Language Processing (NLP) techniques such as Part of Speech (POS) technique and First Occurrence of Term To improve extraction accuracy WordlNet is incorporated into KPSpotter In designing and developing KPSpotter we utilized Unified Modeling Language (UML) UML modeling helps in the formalization of the preliminary analysis model and accomplishes iterative system design and development We also conducted experiments for system performance testing by comparing keyphrases extracted by KPSPotter and KEA a well-known naive Baysiean-based keyphrase extraction system The experiments show that KPSpotter outperforms KEA in most test cases',NLP
'User generated texts on the web are freely-available and lucrative sources of data for language technol-ogy researchers Unfortunately these texts are often dominated by informal writing styles and the lan-guage used in user generated content poses processing difficulties for natural language tools Experienced performance drops and processing issues can be addressed either by adapting language tools to user generated content or by normalizing noisy texts before being processed In this article we propose a Turkish text normalizer that maps non-standard words to their appropriate standard forms using a graph-based methodology and a context-tailoring approach Our normalizer benefits from both contex-tual and lexical similarities between normalization pairs as identified by a graph-based subnormalizer and a transformation-based subnormalizer The performance of our normalizer is demonstrated on a tweet dataset in the most comprehensive intrinsic and extrinsic evaluations reported so far for Turkish In this article we present the first graph-based solution to Turkish text normalization with a novel context-tailoring approach which advances the state-of-the-art results by outperforming other publicly available normalizers For the first time in the literature we measure the extent to which the accuracy of a Turkish language processing tool is affected by normalizing noisy texts before being pro-cessed An analysis of these extrinsic evaluations that focus on more than one Turkish NLP task (ie part-of-speech tagger and dependency parser) reveals that Turkish language tools are not robust to noisy texts and a normalizer leads to remarkable performance improvements once used as a preprocessing tool in this morphologically-rich language(c) 2022 Karabuk University Publishing services by Elsevier BV This is an open access article under the CC BY-NC-ND license (http://creativecommonsorg/licenses/by-nc-nd/40/)',NLP
'Background: Patients with brain damage often exhibit difficulty understanding sentences with certain grammatical constructions posing more of a problem than others Differences between sentences that are hard or easy to process have been characterised in terms of modern syntactic theory leading to a number of insightful proposals regarding the nature of sentence comprehension problems in aphasia However little attention has been devoted to the semantic aspects of aphasic sentence comprehension Aims: The primary aim of this research is to validate the use of a computational semantic approach for modelling aphasic sentence comprehension Methods Procedures: The model presented here is an extension of natural language processing software designed by Blackburn and Bos (2006) The original program parses a natural language expression by means of a simple definite clause grammar assigning a semantic representation to each node in the parse tree The final result of a successful parse is a sentence from first-order logic that describes the meaning of the natural language sentence The program was made relevant for the study of aphasia by extending the grammar to parse 14 sentences that present variable degrees of difficulty to aphasic patients In addition each constituent in the grammar was endowed with an integrity feature that contained an integer with a maximum value of 100 This number constituted the percent chance that the node would be successfully realised and was reduced in proportion to the nodes height in the syntactic tree The constant of proportionality was then manipulated to simulate various degrees of aphasia severity Qualities of the models performance were compared to qualities of aphasic patient performance on four key sentences The models quantitative performance on 12 sentences was compared to that of 46 patients with left hemisphere lesions Outcomes Results: There was a significant correlation between the performance of the model and that of the patients (r = 85 p 001) Sentence length was not significantly correlated with patient performance (r = -52 ns) The probabilistic output of the model resembles variability in performance by aphasic patients Conclusions: A computational semantics approach to aphasic sentence comprehension may provide a means for explaining continuous variation in degrees of aphasia severity as well as qualitative patterns of agrammatic comprehension',NLP
'PurposeDeep learning (DL) is a state-of-the-art technique for developing artificial intelligence in various domains and it improves the performance of natural language processing (NLP) Therefore we aimed to develop a DL-based NLP model that classifies the status of bone metastasis (BM) in radiology reports to detect patients with BMMaterials and MethodsThe DL-based NLP model was developed by training long short-term memory using 1749 free-text radiology reports written in Japanese We adopted five-fold cross-validation and used 200 reports for testing the five models The accuracy sensitivity specificity precision and area under the receiver operating characteristics curve (AUROC) were used for the model evaluationResultsThe developed model demonstrated classification performance with mean +/- standard deviation of 0912 +/- 0012 0924 +/- 0029 0901 +/- 0014 0898 +/- 0012 and 0968 +/- 0004 for accuracy sensitivity specificity precision and AUROC respectivelyConclusionThe proposed DL-based NLP model may help in the early and efficient detection of patients with BM',NLP
'The number of international collaborations in research and development (R&D) has been increasing in the energy sector to solve global environmental problems-such as climate change and the energy crisis-and to reduce the time cost and risk of failure Successful international project planning requires the analysis of research fields and the technology expertise of cooperative partner institutions or countries but this takes time and resources In this study we developed a method to analyze the information on research organizations and topics taking advantage of data analysis as well as deep learning natural language processing (NLP) models A method to evaluate the relative superiority of efficient international collaboration was suggested assuming international collaboration of the National Renewable Energy Laboratory (NREL) and the Korea Institute of Energy Research (KIER) Additionally a workflow of an automated executive summary and a translation of tens of web-posted articles is also suggested for a quick glance The valuation of the suggested methodology is estimated as much as the annual salary of an experienced employee',NLP
'An increasing number of studies have reported using natural language processing (NLP) to assist observational research by extracting clinical information from electronic health records (EHRs) Currently no standardized reporting guidelines for NLP-assisted observational studies exist The absence of detailed reporting guidelines may create ambiguity in the use of NLP-derived content knowledge gaps in the current research reporting practices and reproducibility challenges To address these issues we conducted a scoping review of NLP-assisted observational clinical studies and examined their reporting practices focusing on NLP methodology and evaluation Through our investigation we discovered a high variation regarding the reporting practices such as inconsistent use of references for measurement studies variation in the reporting location (reference appendix and manuscript) and different granularity of NLP methodology and evaluation details To promote the wide adoption and utilization of NLP solutions in clinical research we outline several perspectives that align with the six principles released by the World Health Organization (WHO) that guide the ethical use of artificial intelligence for health',NLP
'ABSTR A C T This paper shows a computational model that classifies Cybergrooming attacks in the context of COP (child online protection) using Natural Language Processing (NLP) and Convolutional Neural Networks (CNN) The model predicts a high number of false positives therefore low precision and F-score but a high accuracy In this issue where the number of messages in the context of grooming are so low com-pared to the number of conversations and messages from other contexts it can be concluded that is a very consistent and useful result as it captures a high number of true positives considering that the clas-sifier works for messages Performing the training of machine learning algorithms with neural networks semantic analysis and NLP allows approximate representation of knowledge contributing to discovery of pseudo-intelligent information in these environments and reducing human intervention for characteriza-tion of underlying abnormal behavior and detecting messages that potentially represent these attacks(c) 2021 Elsevier BV All rights reserved',NLP
'Ontology of any domain formally represents the knowledge based on concepts along with its relationship and properties Legal ontology has a significant contribution in capturing knowledge of legal domain The legal professionals analyse legal documents to extract knowledge based on the parameters mentioned in legal sections to prepare their draft This research work mainly contributes in capturing the knowledge of legal section via ontology The proposed ontology has mapped the entities and their relationship of textual rule related to Criminal major Act on Domestic Violence as stated in Indian Penal Code Section 498A It mainly focuses on relevant portions on which legal professionals give major emphasis in Domestic Violence cases Authors have extracted major entities present in Domestic Violence cases using Natural Language Processing techniques like named entity recognition regex etc and generated a parse tree The entities and relationship generated by the parse tree is further represented as classes object properties instances etc using Protege Authors have tried to represent the concepts and their relationship through ontology visualizer ie OntoGraf',NLP
'Advances in natural language processing (NLP) and computer vision are now being applied to many agricultural problems These techniques take advantage of nontraditional (or nonnumeric) data sources such as text in libraries and images from field operations However these techniques could be more powerful if combined with Artificial Intelligence (AI) and numeric sources of data in multimodal pipelines We present several recent examples where United States Department of Agriculture (USDA) Agricultural Research Service (ARS) researchers and collaborators are using AI methods with text and images to improve core scientific knowledge the management of agricultural research and agricultural practice NLP enables automated indexing clustering and classification for agricultural research project management We explore two case studies where combining techniques and data sources in new ways could accelerate progress in personalized nutrition and invasive pest detection One challenge in applying these techniques is the difficulty in obtaining high-quality training data Other challenges are a lack of machine learning (ML) techniques customized for use and ML skills or experience among researchers and other stakeholders Initiatives are underway at USDA-ARS to address these challenges',NLP
'Interactive virtual assistants serve as a cost-effective solution to assist elderly people in several ways They act as means to provide social support manage loneliness medium of communication reminder systems and even instill positive moods in their users The relevance of advancement in virtual assistant technology is not only confined to advancement in functionalities in virtual assistants but it also involves analyzing levels of social acceptance of these virtual assistants that feature on different kinds of interactive devices This analysis would help to set up paradigms of trust and belief in human-technology partnerships and serve as means to enhance the utilization of virtual assistants through increasing their social acceptance typically by elderly people To analyze the social acceptance of virtual assistants on different interactive devices this work uses the concept of sentiment analysis using Natural Language Processing to analyze the views expressions and beliefs expressed by elderly people while interacting with virtual assistants on different devices',NLP
'The advent of online news outlets and the recent explosion in number of users who consume news using this medium has led to several websites competing to grab users attention This has led to outlets creating creative ways to lure to reader to their website One of the most widely used techniques is employing clickbait headlines These headlines are specifically designed to pique the readers interest in what is promised when the article is clicked on The article usually does not deliver the content the reader is looking for In this work we aim to create a model to detect such headlines We analyse what constitutes as clickbait and extract features which may be indicative of clickbait The features extracted from the article are both syntactic and semantic and encompass the variability that is found in such headlines Finally we build a classifier that can distinguish between actual news and what is likely to be clickbait We use multiple classification algorithms and weigh the advantages presented by them',NLP
'Reading understanding writing and re-writing stories playing different roles are considered powerful strategies for sharing human knowledge The system developed in this paper based on Propps theory of folk tales and on Natural Language Processing techniques is able to recognize main characters of folk tales providing a summary of the text and permitting users to edit the story in a controlled way The list of main charactersas well as the summary highlight the structure of the story allowing students to easily understand its plot Once students understood the story they can modify it editing the parts of the text where main characters play their own roles This way students can create new stories starting from the original one and respecting its ground structure The proposed model is not only based on word frequencies and therefore is able to recognize characters even if the related words are not frequent in the text The proposed algorithm has been compared with pure statistical predictors observing that the proposed approach significantly outperform them',NLP
'Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing Current PLMs are obtained by sentence-level pre-training which is different from the basic processing unit ie element discourse unit (EDU) To this end we propose a second-stage EDU-level pre-training approach in this work which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models Concretely the two tasks are (1) next EDU prediction (NEP) and (2) discourse marker prediction (DMP) We take a state-of-the-art transition-based neural parser as baseline and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation Experimental results on a benckmark dataset show that our method is highly effective leading a 21-point improvement in F1-score All codes and pre-trained models will be released publicly to facilitate future studies(1)',NLP
'By building a part-of-speech (POS) tagger for Middle High German we investigate strategies for dealing with a low resource diverse and non-standard language in the domain of natural language processing We highlight various aspects such as the data quantity needed for training and the influence of data quality on tagger performance Since the lack of annotated resources poses a problem for training a tagger we exemplify how existing resources can be adapted fruitfully to serve as additional training data The resulting POS model achieves a tagging accuracy of about 91% on a diverse test set representing the different genres time periods and varieties of MHG In order to verify its general applicability we evaluate the performance on different genres authors and varieties of MHG separately We explore self-learning techniques which yield the advantage that unannotated data can be utilized to improve tagging performance on specific subcorpora',NLP
'The present study aims to compare and analyze the performance of two tokenizers Mecab-Ko and SentencePiece in the context of natural language processing for sentiment analysis The study adopts a comparative approach employing five algorithms - Naive Bayes (NB) k-Nearest Neighbor (kNN) Support Vector Machine (SVM) Artificial Neural Networks (ANN) and Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) - to evaluate the performance of each tokenizer The performance was assessed based on four widely used metrics in the field accuracy precision recall and F1-score The results indicated that SentencePiece performed better than Mecab-Ko To ensure the validity of the results paired t-tests were conducted on the evaluation outcomes The study concludes that SentencePiece demonstrated superior classification performance especially in the context of ANN and LSTM-RNN when used to interpret customer sentiment based on Korean online reviews Furthermore SentencePiece can assign specific meanings to short words or jargon commonly used in product evaluations but not defined beforehand',NLP
'The use of artificial intelligence (AI) techniques to uncover customer sentiment is not uncommon However the integration of sentiment analysis with research in design change prediction remains an untapped potential This paper presents a method that uses social media sentiment analysis to identify opportunities for design change and the set of product components affected by the change The method builds on natural language processing to determine change candidates from textual data and uses dependency modeling to reveal direct and indirect change propagation paths arising from the change candidates The method was applied in a case example where 3665 YouTube comments on a diesel engine were analyzed Based on the results two engine components were recommended for design change with six others predicted as likely to be affected through change propagation The findings suggest that the method can be used to aid decision quality in product planning through a better understanding of the change impact associated with the opportunities identified',NLP
'In construction schedule mistakes causing delays beyond substantial completion dates cost contractors expensive liquidated damages Hence several industry guidelines such as the DCMAs 14 point assessment define schedule quality and offer systematic methods for ensuring it These guidelines list logic as an essential control metric and they require planners to ensure their schedules are free of missing or wrong logical dependencies Checking the logic requires extensive construction domain knowledge and planners perform it entirely manually as there are no available software solutions that support it This paper offers a novel machine learning-based solution that learns construction scheduling domain knowledge from existing records completely automatically and applies it to validate the logic in input schedules achieving an F1 score of 883% Furthermore we tailor our method to use the learned knowledge to schedule a list of unordered activities The details of the method experimental results benefits and limitations are discussed',NLP
'Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents spreadsheets images etc This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC) In this paper we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation The proposed pipeline primarily consists of the following components: 1) AutoOntology an ontology that has been derived by analyzing several industry scale automotive domain software systems 2) AutoRE a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain and 3) AutoVec a neural embedding based algorithm for triplet matching and context-based search We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG',NLP
'The neural network model in natural language processing (NLP) has become the mainstream Bidirectional Long Short-Term Memory Network (BiLSTM) has proved to be a very effective sequence model in text semantic understanding The Siamese framework has achieved excellent performance in text semantic similarity calculation However the feature granularity obtained from BiLSTM is single and more extensive information cannot be obtained using traditional neural networks In order to solve these problems text semantic understanding model based on knowledge enhancement and multi-granular feature extraction (KE-MGFE) is proposed KE-MGFE integrates knowledge base and three granular text representations and an attention mechanism is introduced in the sub-networks of the Siamese framework to enhance information interaction KE-MGFE is formed to capture the deep entity relationships of text and the features of each granularity Finally experimental verifications are conducted on two long text datasets The results clearly show that the proposed model KE-MGFE can achieve higher performance than other latest text similarity calculation methods',NLP
'Power dispatching systems currently receive massive complicated and irregular monitoring alarms during their operation which prevents the controllers from making accurate judgments on the alarm events that occur within a short period of time In view of the current situation with the low efficiency of monitoring alarm information this paper proposes a method based on natural language processing (NLP) and a hybrid model that combines long short-term memory (LSTM) and convolutional neural network (CNN) for the identification of grid monitoring alarm events Firstly the characteristics of the alarm information text were analyzed and induced and then preprocessed Then the monitoring alarm information was vectorized based on the Word2vec model Finally a monitoring alarm event identification model based on a combination of LSTM and CNN was established for the characteristics of the alarm information The feasibility and effectiveness of the method in this paper were verified by comparison with multiple identification models',NLP
'Goal-Oriented Requirements Engineering approaches in which the KAOS framework plays a key role have been widely used for eliciting software requirements because they provide an easier way of communicating among stakeholders However the goal-oriented requirements modeling is not an easy way for novice requirements engineers These professionals need more support in creating KAOS models Recent studies have focused on the applicability of Artificial Intelligence techniques (eg Natural Language Processing - NLP) to support Requirements Engineering activities In this sense this paper aims to describe a way to support requirements elicitation for novice requirements engineers through the use of NLP within a chatbot The chatbot (KAOSbot) acts as a KAOS modeling assistant To evaluate our hypotheses about perceived efficacy from the novice requirements engineers perspective we performed a quasi-experiment concerning KAOSbots perceived ease of use perceived usefulness and intention to use The results show that KAOSbot tool is a promising approach for specifying KAOS models because it was perceived as easy to use useful and the participants intend to use it in the future',NLP
'Word sense disambiguation is one of the most important open problems in natural language processing applications such as information retrieval and machine translation Many approach strategies can be employed to resolve word ambiguity with a reasonable degree of accuracy These strategies are: knowledge-based corpus-based and hybrid-based This paper pays attention to the corpus-based strategy that employs an unsupervised learning method for disambiguation We report our investigation of Latent Semantic Indexing (LSI) an information retrieval technique and unsupervised learning to the task of Thai noun and verbal word sense disambiguation The Latent Semantic Indexing has been shown to be efficient and effective for Information Retrieval For the purposes of this research we report experiments on two Thai polysemous words namely no /hua4/ and inu /kep1/ that are used as a representative of Thai nouns and verbs respectively The results of these experiments demonstrate the effectiveness and indicate the potential of applying vector-based distributional information measures to semantic disambiguation',NLP
'Language instruction plays an essential role in the natural language grounded navigation tasks However navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps leading to poor navigation performance In this paper we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction by using an adversarial attacking paradigm Specifically we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker) which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps By formulating the perturbation generation as a Markov Decision Process DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation according to a learnable attack score Then the perturbed instructions which serve as hard samples are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods Moreover the visualization analysis shows the effectiveness of the proposed DR-Attacker which can successfully attack crucial information in the instructions at different timesteps Code is available at https://githubcom/expectorlin/DR-Attacker',NLP
'Current medical science has not yet found a cure for dementia The most important measures to combat dementia are to detect the tendency toward cognitive decline as early as possible and to intervene at an early stage For this reason screening for dementia based on language ability has attracted much attention in recent years However in most of the previous studies the cohort of people with dementia has been smaller than the control cohort In this paper we use a pre-trained Japanese language model for text analysis and evaluate the effectiveness of text augmentation on a dataset consisting of Japanese-speaking healthy older adults and those with mild cognitive impairment (MCI) We also examined what tasks contributed to the results This experimental setting can also be used to detect other diseases that may affect the language areas of the brain outside of the hospital',NLP
'Text mining for the identification of emerging technology is becoming increasingly important as the number of scientific and technical documents grows However algorithms for developing text mining models require a large amount of training data which carries heavy costs associated with data annotation and model development The need for avoiding these associated costs has in part motivated recent work in text mining which indicate value in leveraging language representation models (LRMs) on domain-specific text corpora for domain-specific tasks However these results are demonstrated predominantly on large text corpora which do not address concerns associated with the ability of LRMs to transfer to domains where training data may be scarce Due to this we benchmarked the performance of LRMs on identifying quantities and units of measure from text when the number of training samples is small',NLP
'Grammatical Framework (GF) is a special-purpose functional language for defining grammars It uses a Logical Framework (LF) for a description of abstract syntax and adds to this a notation for defining concrete syntax GF grammars themselves are purely declarative but can be used both for linearizing syntax trees and parsing strings GF can describe both formal and natural languages The key notion of this description is a grammatical object which is not just a string but a record that contains all information on inflection and inherent grammatical features such as number and gender in natural languages or precedence in formal languages Grammatical objects have a type system which helps to eliminate run-time errors in language processing In the same way as a LF GF uses dependent types in abstract syntax to express semantic conditions such as well-typedness and proof obligations Multilingual grammars where one abstract syntax has many parallel concrete syntaxes can be used for reliable and meaning-preserving translation They can also be used in authoring systems where syntax trees are constructed in an interactive editor similar to proof editors based on LF While being edited the trees can simultaneously be viewed in different languages This paper starts with a gradual introduction to GF going through a sequence of simpler formalisms till the full power is reached The introduction is followed by a systematic presentation of the GF formalism and outlines of the main algorithms: partial evaluation and parser generation The paper concludes by brief discussions of the Haskell implementation of GF existing applications and related work',NLP
'Virtual screening (VS) is a critical technique in understanding biomolecular interactions particularly in drug design and discovery However the accuracy of current VS models heavily relies on three-dimensional (3D) structures obtained through molecular docking which is often unreliable due to the low accuracy To address this issue we introduce a sequence-based virtual screening (SVS) as another generation of VS models that utilize advanced natural language processing (NLP) algorithms and optimized deep K-embedding strategies to encode biomolecular interactions without relying on 3D structure-based docking We demonstrate that SVS outperforms state-of-the-art performance for four regression datasets involving protein-ligand binding protein-protein protein-nucleic acid binding and ligand inhibition of protein-protein interactions and five classification datasets for protein-protein interactions in five biological species SVS has the potential to transform current practices in drug discovery and protein engineering A sequence-based virtual screening method uses natural language processing algorithms and optimized deep K-embedding strategies to encode biomolecular interactions without relying on 3D structure-based docking',NLP
'Sentiment Analysis (SA) is one of the subfields in Natural Language Processing (NLP) which focuses on identification and extraction of opinions that exist in the text provided across reviews social media blogs news and so on SA has the ability to handle the drastically-increasing unstructured text by transforming them into structured data with the help of NLP and open source tools The current research work designs a novel Modified Red Deer Algorithm (MRDA) Extreme Learning Machine Sparse Autoencoder (ELMSAE) model for SA and classification The proposed MRDA-ELMSAE technique initially performs preprocessing to transform the data into a compatible format Moreover TF-IDF vectorizer is employed in the extraction of features while ELMSAE model is applied in the classification of sentiments Furthermore optimal parameter tuning is done for ELMSAE model using MRDA technique A wide range of simulation analyses was carried out and results from comparative analysis establish the enhanced efficiency of MRDA-ELMSAE technique against other recent techniques',NLP
'Schema matching aims to identify the correspondences among attributes of database schemas It is frequently considered as the most challenging and decisive stage existing in many contemporary web semantics and database systems Low-quality algorithmic matchers fail to provide improvement while manually annotation consumes extensive human efforts Further complications arise from data privacy in certain domains such as healthcare where only schema-level matching should be used to prevent data leakage For this problem we propose SMAT a new deep learning model based on state-of-the-art natural language processing techniques to obtain semantic mappings between source and target schemas using only the attribute name and description SMAT avoids directly encoding domain knowledge about the source and target systems which allows it to be more easily deployed across different sites We also introduce a new benchmark dataset OMAP based on real-world schema-level mappings from the healthcare domain Our extensive evaluation of various benchmark datasets demonstrates the potential of SMAT to help automate schema-level matching tasks',NLP
'It is important and informative to compare and contrast major economic crises in order to confront novel and unknown cases such as the COVID-19 pandemic The 2006 Great Recession and then the 2019 pandemic have a lot to share in terms of unemployment rate consumption expenditures and interest rates set by Federal Reserve In addition to quantitative historical data it is also interesting to compare the contents of Federal Reserve statements for the period of these two crises and find out whether Federal Reserve cares about similar concerns or there are some other issues that demand separate and unique monetary policies This paper conducts an analysis to explore the Federal Reserve concerns as expressed in their statements for the period of 2005 to 2020 The concern analysis is performed using natural language processing (NLP) algorithms and a trend analysis of concern is also presented We observe that there are some similarities between the Federal Reserve statements issued during the Great Recession with those issued for the 2019 COVID-19 pandemic',NLP
'Natural language processing is a technique to process data such as text and speech Some fundamental research includes named-entity recognition which recognizes name entities (ie persons companies) from texts; semantic parsing which is used to convert a natural language utterance to the representation of logical form; and co-reference resolution which extracts nouns (including pronouns noun phrases) pointing to the same reference body In this paper we mainly focus on the task of mention extraction which extract and classify overlapping or nested structure mentions We proposed a neural-encoded mention-hypergraph (NEMH) model to use hypergraph to model overlapping or nested structure mentions and use neural networks to extract features for hypergraph automatically Unlike the existing approaches our hypergraph model can effectively capture nested mention entities with unlimited lengths Also the proposed model is highly scalable and the time complexity of the proposed model is linear in the number of mention classes and the number of input words Extensive experiments are conducted on several standard datasets to demonstrate the effectiveness of the proposed model',NLP
'The second track of the 2014 i2b2/UTHealth natural language processing shared task focused on identifying medical risk factors related to Coronary Artery Disease (CAD) in the narratives of longitudinal medical records of diabetic patients The risk factors included hypertension hyperlipidemia obesity smoking status and family history as well as diabetes and CAD and indicators that suggest the presence of those diseases In addition to identifying the risk factors this track of the 2014 i2b2/UTHealth shared task studied the presence and progression of the risk factors in longitudinal medical records Twenty teams participated in this track and submitted 49 system runs for evaluation Six of the top 10 teams achieved F 1 scores over 090 and all 10 scored over 087 The most successful system used a combination of additional annotations external lexicons hand-written rules and Support Vector Machines The results of this track indicate that identification of risk factors and their progression over time is well within the reach of automated systems (C) 2015 Elsevier Inc All rights reserved',NLP
'Spear phishing is a widespread concern in the modern network security landscape but there are few metrics that measure the extent to which reconnaissance is performed on phishing targets Spear phishing emails closely match the expectations of the recipient based on details of their experiences and interests making them a popular propagation vector for harmful malware In this work we use Natural Language Processing techniques to investigate a specific real- world phishing campaign and quantify attributes that indicate a targeted spear phishing attack Our phishing campaign data sample comprises 596 emails - all containing a web bug and a Curriculum Vitae (CV) PDF attachment - sent to our institution by a foreign IP space The campaign was found to exclusively target specific demographics within our institution Performing a semantic similarity analysis between the senders CV attachments and the recipients LinkedIn profiles we conclude with high statistical certainty (p < 10(-4)) that the attachments contain targeted rather than randomly selected material Latent Semantic Analysis further demonstrates that individuals who were a primary focus of the campaign received CVs that are highly topically clustered These findings differentiate this campaign from one that leverages random spam',NLP
'Objective: The authors developed a natural language processing (NLP) framework that could be used to extract clinical findings and diagnoses from dictated physician documentation Design: De-identified documentation was made available by i2b2 Bio-informatics research group as a part of their NLP challenge focusing on obesity and its co-morbidities The authors describe their approach which used a combination of concept detection context validation and the application of a variety of rules to conclude patient diagnoses Results: The framework was successful at correctly identifying diagnoses as judged by NLP challenge organizers when compared with a gold standard of physician annotations The authors overall kappa values for agreement with the gold standard were 092 for explicit textual results and 091 for intuited results The NLP framework compared favorably with those of the other entrants placing third in textual results and fourth in intuited results in the i2b2 competition Conclusions: The framework and approach used to detect clinical conditions was reasonably successful at extracting 16 diagnoses related to obesity The system and methodology merits further development targeting clinically useful applications',NLP
'Bug localization utilizes the collected bug reports to locate the buggy source files The state of the art falls short in handling the following three aspects including (L1) the subtle difference between natural language and programming language (L2) the noise in the bug reports and (L3) the multi-grained nature of programming language To overcome these limitations we propose a novel deep multimodal model named DeMoB for bug localization It embraces three key features each of which is tailored to address each of the three limitations To be specific the proposed DeMoB generates the multimodal coordinated representations for both bug reports and source files for addressing L1 It further incorporates the AttL encoder to process bug reports for addressing L2 and the MDCL encoder to process source files for addressing L3 Extensive experiments on four large-scale real-world data sets demonstrate that the proposed DeMoB significantly outperforms existing techniques',NLP
'Unsupervised speech processing methods are essential for applications ranging from zero-resource speech technology to modelling child language acquisition One challenging problem is discovering the word inventory of the language: the lexicon Lexical clustering is the task of grouping unlabelled acoustic word tokens according to type We propose a novel lexical clustering model: variable-length word segments are embedded in a fixed-dimensional acoustic space in which clustering is then performed We evaluate several clustering algorithms and find that the best methods produce clusters with wide variation in sizes as observed in natural language The best probabilistic approach is an infinite Gaussian mixture model (IGMM) which automatically chooses the number of clusters Performance is comparable to that of non-probabilistic Chinese Whispers and average-linkage hierarchical clustering We conclude that IGMM clustering of fixed-dimensional embeddings holds promise as the lexical clustering component in unsupervised speech processing systems',NLP
'This paper addresses risk assessment issues while conceiving complex systems Indeed project stakeholders have to share the same problems understanding allowing to undertake rational and optimal decisions We propose an approach based on Natural Language Processing (NLP) techniques to improve systems quality requirements such as consistency and completeness We assess the relevancy of our approaches through experimentations and highlighted feedbacks from project stakeholders and players',NLP
'Sepsis results in various patient complications and is due to a heightened inflammatory response against infection This condition requires further exploration of biomarkers We employed an in silico method comprised of text-mining and additionally clinical validation through the Medical Information Mart for Intensive Care We highlight that Factor VIII shows potential as a pertinent septic shock biomarker',NLP
'We present a method for automatic extraction of frames from a dependency graph Our method uses machine learning applied to a dependency tree to identify frames and assign frame elements The system is evaluated by cross-validation on FrameNet sentences and also on the test data from the SemEval 2007 task 19 Our system is intended for use in natural language processing applications such as summarization entailment and novelty detection',NLP
'Sentence similarity measures have taken an increasingly important role in a variety of applications of text knowledge presentation and discovery Current methods utilize only semantic or syntactic information We propose a novel method to calculate sentence similarity which takes into account both semantic information and word order The experimental results show that our method outperforms existing methods',NLP
'While the application of quantum computing to natural language processing (NLP) has seen substantial work in recent years not much has been presented on sentiment analysis to systematically identify extract quantify and study affective states and subjective information This preliminary study investigates how sentiment can be represented correctly and efficiently for quantum natural language processing (QNLP) In particular we present four possible approaches for representing sentiment words in the quantum case We propose to utilize two quantum operations (ie X gate and Z-X gate) each with two different placements to represent a negative sentence in a quantum circuit Subsequently we train the corresponding state vector using several classical classifiers: SVM SPSA and a combination of both then evaluate the result in terms of accuracy Experimental results show that the third approach ie adding an X-gate at the end of the sentence outperforms other settings Additionally the third approach along with the combined classifier of SVM and SPSA reaches up to 8167% accuracy The proposed method can be helpful for applications of sentiment analysis in quantum computers',NLP
'Data normalization methodologies are extremely welcome to link extracted information from textual data to different semantic resources These methodologies have been previously well researched especially in the biomedical domain where health concepts were normalized and described using semantic tags Recently a methodology for normalizing food concepts has been proposed based on Named-Entity Recognition methods resulting in the FoodOntoMap semantic resource In this paper we propose and evaluate a new architecture for linking phrases (ie textual name for foods) to concepts from semantic resources in the Food and Nutrition domain We represent the food phrases (ie their textual name) in continuous vector space using state-of-the-art Natural Language Processing (NLP) embedding algorithms and evaluate their proximity with respect to the annotated semantic food concepts Additionally indexing was incorporated to improve efficiency The GloVe embedding with mean pooling provided best evaluation results with maximum recall of 74% for the Snomed CT semantic dataset which is promising result but also opens a space for future improvement of the phrase representations and their incorporation in this system',NLP
'Within the domain of biomedical natural language processing (bioNLP) researchers have used many token features for machine learning models With recent progress in word embeddings algorithms it is no longer clear if most of these features are still useful In this paper we survey the features which have been used in bioNLP and evaluate each features utility in a sample bioNLP task: the N2C2 2018 named entity recognition challenge The features we test include two types of word embeddings syntactic lexical and orthographic features character-embeddings and clustering and distributional word representations We find that using fastText word embeddings results in a significantly higher F1 score than using any other individual feature (09142 compared to 08750 for the next-best feature) Furthermore we conducted several experiments using combinations of features and found that all tested combinations attained a lower F1 score than using word embeddings only This indicates that supplementing word embeddings with additional features is not beneficial and may even be detrimental',NLP
'Software Requirements are the basis of high-quality software development process each step is related to SR these represent the needs and expectations of the software in a very detailed form The software requirement classification (SRC) task requires a lot of human effort specially when there are huge of requirements therefore the automation of SRC have been addressed using Natural Language Processing (NLP) and Information Retrieval (IR) techniques however generally requires human effort to analyze and create features from corpus (set of requirements) In this work we propose to use Deep Learning (DL) to classify software requirements without labor intensive feature engineering The model that we propose is based on Convolutional Neural Network (CNN) that has been state of art in other natural language related tasks To evaluate our proposed model PROMISE corpus was used contains a set of labeled requirements in functional and 11 different categories of nonfunctional requirements We achieve promising results on SRC using CNN even without handcrafted features',NLP
'Application Programming Interface (API) documents represent one of the most important references for API users However it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs In this paper we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing Particularly we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results We evaluate our approach on parts of well documented JDK 18 APIs Experiment results show that out of around 2000 API usage constraints our approach can detect 1158 defective document directives with a precision rate of 816% and a recall rate of 820% which demonstrates its practical feasibility',NLP
'Named entity recognition and disambiguation are of primary importance for extracting information and for populating knowledge bases Detecting and classifying named entities has traditionally been taken on by the natural language processing community whilst linking of entities to external resources such as those in DBpedia has been tackled by the Semantic Web community As these tasks are treated in different communities there is as yet no oversight on the performance of these tasks combined We present an approach that combines the state-of-the art from named entity recognition in the natural language processing domain and named entity linking from the semantic web community We report on experiments and results to gain more insights into the strengths and limitations of current approaches on these tasks Our approach relies on the numerous web extractors supported by the NERD framework which we combine with a machine learning algorithm to optimize recognition and linking of named entities We test our approach on four standard data sets that are composed of two diverse text types namely newswire and microposts',NLP
'We develop and present the deep-structured conditional random field (CRF) a multi-layer CRF model in which each higher layers input observation sequence consists of the previous layers observation sequence and the resulted frame-level marginal probabilities Such a structure can closely approximate the long-range state dependency using only linear-chain or zeroth-order CRFs by constructing features on the previous layers output (belief) Although the final layer is trained to maximize the log-likelihood of the state (label) sequence each lower layer is optimized by maximizing the frame-level marginal probabilities In this deep-structured CRF both parameter estimation and state sequence inference are carried out efficiently layer-by-layer from bottom to top We evaluate the deep-structured CRF on two natural language processing tasks: search query tagging and advertisement field segmentation The experimental results demonstrate that the deep-structured CRF achieves word labeling accuracies that are significantly higher than the best results reported on these tasks using the same labeled training set',NLP
'Structural ambiguity is one of the most difficult problems in natural language processing Two disambiguation mechanisms for unrestricted text analysis are commonly used: lexical knowledge and context considerations Our parsing method includes three different mechanisms to reveal syntactic structures and an additional voting module to obtain the most probable structures for a sentence The developed tools do not require any tagging or syntactic marking of texts',NLP
'Opinion Analysis is an articulate methodology that is indivisibly crypt to the sector of Emotional Sciences which concern the individual supposition emotions or thoughts and thereby identifies the personal deeds Natural language processing techniques presume that in common most of the user annotations are written in English dialect but as focus shifts onto processing comments from internet sources such as microblogging services this becomes progressively harder to guarantee Conveying the empathy in their own language can be well thought-out as the homey means for expressing the exact opinion and it leads to the generation of multilinguistic societal data So the challenge is to analyze the formal textual content along with the informal and mixed linguistic nature of social data This prompts the need of Sentiment Analysis in multilingual dialects This article surveys the methodologies used for analyzing the sentiment of multilingual data and proposed a model built using Long Short Term Memory approach',NLP
'Most of the successful commercial applications in language processing (text and/or speech) dispense of any explicit concern on semantics with the usual motivations stemming from the computational high costs required by dealing with semantics in case of large volumes of data With recent advances in corpus linguistics and statistical-based methods in NLP revealing useful semantic features of linguistic data is becoming cheaper and cheaper and the accuracy of this process is steadily improving Lately there seems to be a growing acceptance of the idea that multilingual lexical ontologies might be the key towards aligning different views on the semantic atomic units to be used in characterizing the general meaning of various and multilingual documents Depending on the granularity at which semantic distinctions are necessary the accuracy of the basic semantic processing (such as word sense disambiguation) can be very high with relatively low complexity computing The paper substantiates this statement by presenting a statistical/based system for word alignment (WA) and word sense disambiguation (WSD) in parallel corpora',NLP
'This paper presents a novel hybrid approach for extracting cancer biomarkers from pathology reports and a prototype system CBEx (Cancer Biomarker Extractor) It integrates a Long Short-Time Memory Neural Network that identifies biomarker-containing sentences and a dictionary-based method to extract biomarker mentions from those sentences Preliminary results show that CBEx outperforms existing methods',NLP
'With the rapid increase of multimodal remote sensing (RS) data cross-modal retrieval maximally benefits us to give us more flexibility in image retrieval tasks However image retrieval across different modalities is still an open challenge in RS community Inspired by the recent achievement of the transformers on natural language processing and computer vision applications we present a transformer-based method for text-image retrieval tasks which consists of separate encoders for textual and visual features Specifically we adopted Arabic and English captions at the text modality Afterward we investigate two paradigms In the first paradigm We consider learning each language independently In the second paradigm we jointly learned both Arabic and English languages The experimental results on two cross-modal confirm the promising capabilities of the proposed method',NLP
'Cross-lingual text retrieval (CLTR) refers to the selection of documents in one language based on query expressed in another language An intelligent CLTR system must be rich in semantic knowledge of multilingual keywords delicate in semantic processing and smart in semantic reasoning In this paper the framework of a fuzzy expert system for CLTR is presented With the application of fuzzy set theory intrinsic imprecision of natural languages are dealt with in two ways First imprecise knowledge of cross-lingual keyword associations is represented by a fuzzy multilingual thesaurus Second the ability of fuzzy reasoning with the imprecise semantic knowledge is provided by fuzzy inference With this framework instead of rigid exact match flexible soft match ie match by degree of relevance between document and query expressed in different languages is made possible to emulate a human experts decision making in CLTR',NLP
'This paper presents ArhiNet an integrated system for generating processing and querying semantically enhanced archival eContent related to the middle age history of Transylvania targeting cultural heritage preservation The main workflows of the system aim to perform semantically enhanced eContent generation and knowledge acquisition as well as knowledge processing and retrieval Through knowledge acquisition the relevant information is extracted from historical documents and structured as a domain ontology In knowledge processing the extracted ontological information is further processed and enhanced both through automatic reasoning and with the help of knowledge engineers Finally through knowledge retrieval relevant documents and information are provided in response to natural language guided user queries',NLP
'Background: Adverse drug reactions (ADRs) occur in nearly all patients on chemotherapy causing morbidity and therapy disruptions Detection of such ADRs is limited in clinical trials which are underpowered to detect rare events Early recognition of ADRs in the postmarketing phase could substantially reduce morbidity and decrease societal costs Internet community health forums provide a mechanism for individuals to discuss real-time health concerns and can enable computational detection of ADRs Objective: The goal of this study is to identify cutaneous ADR signals in social health networks and compare the frequency and timing of these ADRs to clinical reports in the literature Methods: We present a natural language processing-based ADR signal-generation pipeline based on patient posts on Internet social health networks We identified user posts from the Inspire health forums related to two chemotherapy classes: erlotinib an epidermal growth factor receptor inhibitor and nivolumab and pembrolizumab immune checkpoint inhibitors We extracted mentions of ADRs from unstructured content of patient posts We then performed population-level association analyses and time-to-detection analyses Results: Our system detected cutaneous ADRs from patient reports with high precision (090) and at frequencies comparable to those documented in the literature but an average of 7 months ahead of their literature reporting Known ADRs were associated with higher proportional reporting ratios compared to negative controls demonstrating the robustness of our analyses Our named entity recognition system achieved a 0738 microaveraged F-measure in detecting ADR entities not limited to cutaneous ADRs in health forum posts Additionally we discovered the novel ADR of hypohidrosis reported by 23 patients in erlotinib-related posts; this ADR was absent from 15 years of literature on this medication and we recently reported the finding in a clinical oncology journal Conclusions: Several hundred million patients report health concerns in social health networks yet this information is markedly underutilized for pharmacosurveillance We demonstrated the ability of a natural language processing-based signal-generation pipeline to accurately detect patient reports of ADRs months in advance of literature reporting and the robustness of statistical analyses to validate system detections Our findings suggest the important contributions that social health network data can play in contributing to more comprehensive and timely pharmacovigilance',NLP
'Nowadays with the fruitful achievements in Natural Language Processing (NLP) studies the concern of using NLP technologies for education has called much attention As two of the most spoken languages in the world Spanish and Chinese occupy important positions in both NLP studies and bilingual education In this paper we present a Spanish-Chinese parallel corpus with annotated discourse information that aims to serve for bilingual language education The theoretical framework of this work is Rhetorical Structure Theory (RST) The corpus is composed of 100 Spanish-Chinese parallel texts and all the discourse markers (DM) have been annotated to form the education source With pedagogical aim we also present two programs that generate automatic exercises for both Spanish and Chinese students using our corpus The reliability of this work has been evaluated using Kappa coefficient',NLP
'Sentiment analysis is the Natural Language Processing (NLP) task that aims to classify text to different classes such as positive negative or neutral In this paper we focus on sentiment analysis for Arabic language Most of the previous works use machine learning techniques combined with hand engineering features to do Arabic sentiment analysis (ASA) More recently Deep Neural Networks (DNNs) were widely used for this task especially for English languages In this work we developed a system called CNN-ASAWR where we investigate the use of Convolutional Neural Networks (CNNs) for ASA on 2 datasets: ASTD and SemEval 2017 datasets We explore the importance of various unsupervised word representations learned from unannotated corpora Experimental results showed that we were able to outperform the previous state-of-the-art systems on the datasets without using any kind of hand engineering features',NLP
'Treatment of Multiword Expressions (MWEs) is one of the most complicated issues in natural language processing especially in Machine Translation (MT) The paper presents dictionary of MWEs for a English-Latvian MT system demonstrating a way how MWEs could be handled for inflected languages with rich morphology and rather free word order The proposed dictionary of MWEs consists of two constituents - a lexicon of phrases and a set of MWE rules The lexicon of phrases is rather similar to translation lexicon of the MT system while MWE rules describe syntactic structure of the source and target sentence allowing correct transformation of different MWE types into the target language and ensuring correct syntactic structure The paper demonstrates this approach on different MWE types starting from simple syntactic structures followed by more complicated cases and including fully idiomatic expressions Automatic evaluation shows that the described approach increases the quality of translation by 06 BLEU points',NLP
'One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships Recently pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks However it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings To explore this question we propose a methodology for distilling relational knowledge from a pre-trained language model Starting from a few seed instances of a given relation we first use a large text corpus to find sentences that are likely to express this relation We then use a subset of these extracted sentences as templates Finally we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation when given an instantiated template for that relation as input',NLP
'In this article we present a method to validate a multi-lingual (English Spanish Russian and Farsi) corpus on imageability ratings automatically expanded from MRCPD (Liu et al 2014) We employed the corpus (Brysbaert et al 2014) on concreteness ratings for our English MRCPD+ validation because of lacking human assessed imageability ratings and high correlation between concreteness ratings and imageability ratings (eg r = 83) For the same reason we built a small corpus with human imageability assessment for the other language corpus validation The results show that the automatically expanded imageability ratings are highly correlated with human assessment in all four languages which demonstrate our automatic expansion method is valid and robust We believe these new resources can be of significant interest to the research community particularly in natural language processing and computational sociolinguistics',NLP
'The shortage of the annotated training data is still an important challenge to building many Natural Language Process (NLP) tasks such as Named Entity Recognition NER requires a large amount of training data with a high degree of human supervision whereas there is not enough labeled data for every language In this paper we use an unlabeled bilingual corpora to extract useful features from transferring information from resource-rich language toward resource-poor language and by using these features and a small training data make a NER supervised model Then we utilize a graph-based semi-supervised learning method that trains a CRF-based supervised classifier using that labeled data and uses high-confidence predictions on the unlabeled data to expand the training set and improve efficiency of NER model with the new training set',NLP
'This article deals with the discourse structure analysis of scientific and technical Japanese text sentences for developing a Japanese writing CALL (Computer Assisted Language Learning) system This system can be used for learning the writing of the scientific and technical Japanese text sentences on any World Wide Web (WWW) browser This approach can thus be considered as a new means of language learning in the future To analyze the discourse structure the cohesion expressions were used as the clue information Moreover the headline information was also employed to let learners grasp the entire text structure The present Japanese writing CALL system is developed under these considerations The result of a trial analysis showed that the present system obtained a high degree of accuracy on discourse structure analysis',NLP
'In the past few years we observed a renewed interest in conversational recommender systems (CRS) that interact with users in natural language Most recent research efforts use neural models trained on recorded recommendation dialogs between humans supporting an end-to-end learning process Given the users utterances in a dialog these systems aim to generate appropriate responses in natural language based on the learned models An alternative to such language generation approaches is to retrieve and possibly adapt suitable sentences from the recorded dialogs Approaches of this latter type are explored only to a lesser extent in the current literature In this work we revisit the potential value of retrieval-based approaches to conversational recommendation To that purpose we compare two recent deep learning models for response generation with a retrieval-based method that determines a set of response candidates using a nearest-neighbor technique and heuristically reranks them We adopt a user-centric evaluation approach where study participants (N=60) rated the responses of the three compared systems We could reproduce the claimed improvement of one of the deep learning methods over the other However the retrieval-based system outperformed both language generation based approaches in terms of the perceived quality of the system responses Overall our study suggests that retrieval-based approaches should be considered as an alternative or complement to modern language generation-based approaches',NLP
'Chinese word segmentation is one of the foundation and core tasks for Chinese natural language processing Although some achievements have been made for Chinese word segmentation system in general domains it is far away to meet practical requirements in energy domain We focus on Chinese word segmentation standard and segmentation technology in the energy domain which consists of 13283 energy basic terms This paper firstly proposes a conditional random field segmentation model Then the character features character type features and conditional entropy features which influence the word segmentation performance are chose and described Finally the proposed model is tested on the dataset of the State Grid energy literature and compared with current word segmentation tools such as the Harbin Institute of Technologys Language Technology Platform and the Tsinghuas THU Lexical Analyzer for Chinese language processing tools The F1 value of the best result of the proposed model is 08319',NLP
'Following the earlier formalism of the categorical representation learning we discuss the construction of the RG-flow-based categorifier Borrowing ideas from the theory of renormalization group (RG) flows in quantum field theory holographic duality and hyperbolic geometry and combining them with neural ordinary differential equation techniques we construct a new algorithmic natural language processing architecture called the RG-flow categorifier or for short the RG categorifier which is capable of data classification and generation in all layers We apply our algorithmic platform to biomedical data sets and show its performance in the field of sequence-to-function mapping In particular we apply the RG categorifier to particular genomic sequences of flu viruses and show how our technology is capable of extracting the information from given genomic sequences finding their hidden symmetries and dominant features classifying them and using the trained data to make a stochastic prediction of new plausible generated sequences associated with a new set of viruses which could avoid the human immune system',NLP
'Bidirectional encoder representations from transformers (BERT) have achieved great success in many natural language processing tasks However BERT generally takes the embedding of the first token to represent sentence meaning in the tasks such as sentiment analysis and textual similarity which does not properly treat different sentence parts Different sentence parts have different levels of importance for different downstream tasks For example main parts (subject predicate and object) play crucial roles in textual similarity calculation while secondary parts (adverbial and complement) are more important than the main parts in sentiment analysis To this end we propose a sentence part-enhanced BERT (SpeBERT) model that uses sentence parts with respect to downstream tasks to enhance sentence representations Specifically we encode sentence parts based on dependency parsing and downstream tasks and extract embeddings through a pooling operation Furthermore we design several fusion strategies to incorporate different embeddings We evaluate the proposed SpeBERT model on two downstream tasks sentiment classification and semantic textual similarity with six benchmark datasets The experimental results show that our model achieves better performance than competitor models',NLP
'There are efforts been made globally to switch from the take-make-dispose linear economy to Circular Economy (CE) which lead us in rethink how to consume to produce and to discard products However there are challenges and barriers that are keeping businesses from this transition despite the benefits One of such challenge is to identify the effects of New Product Development (NPD) in supply chain This research aimed to unfold the contextual analysis related to Sustainability NPD and CE by using a Natural Language Processing (NLP) approach scientific contributions were retrieved from Scopus and Web of Science (WoS) databases totaling in 633 papers without period restrictions and generated data was analyzed via statistical software RStudio Main results evidenced negative and positive trends in the field proposed three important topics that represents current literature and created a circular network to reveal potential themes In this strand suggestions for future quantitative qualitative and mixed methods research are addressed both in theoretical and empirical ways',NLP
'Software testing depends on effective oracles Implicit oracles such as checks for program crashes are widely applicable but narrow in scope Oracles based on formal specifications can reveal application-specific failures but specifications are expensive to obtain and maintain Metamorphic oracles are somewhere in-between They test equivalence among different procedures to detect semantic failures Until now the identification of metamorphic relations has been a manual and expensive process except for few specific domains where automation is possible We present MeMo a technique and a tool to automatically derive metamorphic equivalence relations from natural language documentation and we use such metamorphic relations as oracles in automatically generated test cases Our experimental evaluation demonstrates that 1) MeMo can effectively and precisely infer equivalence metamorphic relations 2) MeMo complements existing state-of-the-art techniques that are based on dynamic program analysis and 3) metamorphic relations discovered with MeMo effectively detect defects when used as test oracles in automatically-generated or manually-written test cases (C) 2021 The Author(s) Published by Elsevier Inc',NLP
'In aspect of the natural language processing field previous studies have generally analyzed sound signals and provided related responses However in various conversation scenarios image information is still vital Without the image information misunderstanding may occur and lead to wrong responses In order to address this problem this study proposes a recurrent neural network (RNNs) based multi-sensor context-aware chatbot technology The proposed chatbot model incorporates image information with sound signals and gives appropriate responses to the user In order to improve the performance of the proposed model the long short-term memory (LSTM) structure is replaced by gated recurrent unit (GRU) Moreover a VGG16 model is also chosen for a feature extractor for the image information The experimental results demonstrate that the integrative technology of sound and image information which are obtained by the image sensor and sound sensor in a companion robot is helpful for the chatbot model proposed in this study The feasibility of the proposed technology was also confirmed in the experiment',NLP
'In this paper a new two-level recurrent neural network language model (RNNLM) based on the continuous bag-of-words (CBOW) model for application to sentence classification is presented The vector representations of words learned by a neural network language model have been shown to carry semantic sentiment and are useful in various natural language processing tasks A disadvantage of CBOW is that it only considers the fixed length of a context because its basic structure is a neural network with a fixed length of input In contrast the RNNLM does not have a size limit for a context but only considers the previous contexts words Therefore the advantage of RNNLM is complementary to the disadvantage of CBOW Herein our proposed model encodes many linguistic patterns and improves upon sentiment analysis and question classification benchmarks compared to previously reported methods',NLP
'We introduce a new language representation model called BERT which stands for Bidirectional Encoder Representations from Transformers Unlike recent language representation models (Peters et al 2018a; Radford et al 2018) BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers As a result the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks such as question answering and language inference without substantial task-specific architecture modifications BERT is conceptually simple and empirically powerful It obtains new state-of-the-art results on eleven natural language processing tasks including pushing the GLUE score to 805% (77% point absolute improvement) MultiNLI accuracy to 867% (46% absolute improvement) SQuAD v11 question answering Test F1 to 932 (15 point absolute improvement) and SQuAD v20 Test F1 to 831 (51 point absolute improvement)',NLP
'Psychologists have long theorized about the effects of birth order on intellectual development and verbal abilities Several studies within the field of psychology have tried to prove such theories however no concrete evidence has been found yet Therefore in this paper we present an empirical analysis on the pertinence of traditional Author Profiling techniques Thus we re-formulate the problem of identifying developed language abilities by firstborns as a classification problem Particularly we measure the importance of lexical and syntactic features extracted from a set of 129 speech transcriptions which were gathered from videos of approximately three minutes length each Obtained results indicate that both bag of words n-grams and bag of part-of-speech n-grams are able to provide useful information for accurately characterize the language properties employed by firstborns and later-borns Consequently our performed experiments helped to validate the presence of distinct language abilities among firstborns and later-borns',NLP
'With the continuous advancement of artificial intelligence in natural language processing technology machine translation based on machine learning technology has been fully transformed from traditional machine translation methods to neural network machine translation methods In particular the tremendous development of large families has made data-driven a reality With deep learning as the research and design background the neural network structure of random matrix multivariate statistical analysis is designed according to the language characteristics of English The model was tested on a Chinese-English panning model and the optimal model fused with a Bleu value of 3953 The model results were applied to a real system to achieve language detection multidirectional language translation and manual correction of results to be able to learn long dependencies and overcome the limitations of recurrent neural networks to translate long sentences more fluently',NLP
'The increasing amount of Nepali content on the web has opened doors for the research and development of a number of Natural Language Processing applications including Sentiment Analysis (SA) However to best of our knowledge there has been no work in this area for Nepali language In this paper we present two main approaches for sentiment detection of Nepali texts We have developed Nepali Sentiment Corpus and Nepali SentiWordNet In our first approach we develop a lexical resource called Bhavanakos which is a Nepali SentiWordNet and implement a strategy in which sentiment words are detected in Nepali texts to detect the sentiment in documents The second of our approach we train a machine learning based text classifier with annotated Nepali text data to classify the document',NLP
'The purpose of this study is to explore the grammatical properties and features of string matching based on n-grams techniques and to apply them to electronic testing at programming languages Because of the intensive and extensive availability of internet in the academic environment there exists a real need for computational procedures within artificial intelligence methods that support assessment of examination questions for uniformity and consistency There are many computer-aided assessment packages mainly designed for single and multiple choice tests but they are not suitable for electronic testing at programming languages n-grams based string matching is being successfully applied to document retrieval and other natural language processing applications Generalized n-grams matching during substring processing tends to be time-consuming since there are N-2 n-grams extracted where n is the length of a (sub) string The choice of selecting parameter n in n-grams approximately is an important task since the large size of n leads to polynomial growth and its small size may shorten search time significantly As the result some new string matching methods based on n-grams are proposed for the improvement of generalized n-grams Experiments are conducted with the method using programming language codes as both pattern and text matching The results are compared to chosen existing methods We found the obtained results very promising and suggest that the proposed methods can be successfully applied to electronic testing at programming languages as an intelligent support for teachers involved in e-learning processes',NLP
'Most of the machine learning algorithms requires the input to be denoted as a fixed-length feature vector In text classifications (bag-of-words) is a popular fixed-length features Despite their simplicity they are limited in many tasks; they ignore semantics of words and loss ordering of words In this paper we propose a simple and efficient neural language model for sentence-level classification task Our model employs Recurrent Neural Network Language Model (RNN-LM) Particularly Long Short-Term Memory (LSTM) over pre-trained word vectors obtained from unsupervised neural language model to capture semantics and syntactic information in a short sentence We achieved outstanding empirical results on multiple benchmark datasets IMDB Sentiment analysis dataset and Stanford Sentiment Treebank (SSTb) dataset The empirical results show that our model is comparable with neural methods and outperforms traditional methods in sentiment analysis task',NLP
'As one of the fundamental tasks in natural language processing Multi-Label Text Classification (MLTC) is used to mark one or more relevant labels for a given text from a large set of labels Existing MLTC methods have increasingly focused on improving classification effectiveness by fusing the correlations of labels Still the research suffers from difficulties in comprehensively extracting text features and distinguishing similar labels This paper proposed a multi-label text classification model based on keyword extraction and attention mechanism The model proposed using keywords to represent labels adopting both self-attention and interactive attention mechanisms (between labels and text) to extract text features and create text vectors Finally fusing text vectors as the classifiers input Experiments were conducted on two public datasets and a self-built dataset of illegal advertisements The experimental results showed that the keyword-based label representation approach proposed in this paper can better obtain label semantics avoid noise and improve the performance of the multi-label text classification',NLP
'This paper presents a pilot study of NaturalLanguageProcessing4All (NLP4All) a Constructionist low-threshold XAI learning tool designed to bring Natural Language Processing methods into high school classrooms Specifically NLP4All is designed to let non-programmers explore different corpora of text through classification activities Together with a high school Social Studies teacher I developed a 2-week (6-hour) learning unit focusing on analyzing tweets from political parties to explore the differences and similarities between their policy views and communication styles In the analysis I find that text classification shows unexplored promise as a learning activity; that students were able to draw on their prior knowledge to classify tweets; that using NLP4All to collaboratively classify tweets led to productive classroom discussions; and that while students were able to build good machine learning models for classifying tweets their rationales often focused on identifying one party rather than distinguishing between parties Finally I discuss other educational contexts where NLP and ML can be productive for children and future design features that may be worth exploring',NLP
'Natural Language Processing (NLP) is an important subfield within Machine Learning and various deep learning architectures and preprocessing techniques have led to many improvements Long short-term memory (LSTM) is the most well-known architecture for time series and textual data Recently models like Bidirectional Encoder Representations from Transformers (BERT) which rely on pre-training with unsupervised data and using transfer learning have made a huge impact on NLP All of these models work well on short to average-length texts but they are all limited in the sequence lengths they can accept In this paper we propose inserting an encoder in front of each model to overcome this limitation If the data contains long texts doing so substantially improves classification accuracy (by around 15% in our experiments) Otherwise if the corpus consists of short texts which existing models can handle the presence of the encoder does not hurt performance Our encoder can be applied to any type of model that deals with textual data and it will empower the model to overcome length limitations',NLP
'Access to hospital data is commonly a difficult costly and time-consuming process requiring extensive interaction with network administrators This leads to possible delays in obtaining insights from data such as diagnosis or other clinical outcomes Healthcare administrators medical practitioners researchers and patients could benefit from a system that could extract relevant information from healthcare data in real-time In this paper we present a question answering system that allows health professionals to interact with a large-scale database by asking questions in natural language This system is built upon the BERT and SQLOVA models which translate a users request into an SQL query which is then passed to the data server to retrieve relevant information We also propose a deep bilinear similarity model to improve the generated SQL queries by better matching terms in the users query with the database schema and contents This system was trained with only 75 real questions and 455 back-translated questions and was evaluated over 75 additional real questions about a real health information database achieving a retrieval accuracy of 78%',NLP
'Although most phylogenetic investigations are motivated by questions about the evolution of morphological attributes morphological data are increasingly rare as a source of characters for reconstructing phylogeny in part because these attributes are time consuming to collect Here we describe methods to mine the information contained in classifications as a source of phylogenetic characters using the classification of actiniarian sea anemones (Cnidaria: Anthozoa) as our exemplar system Our natural language processing pipeline recovers more than 400 characters in the most widely-used classification of sea anemones However the majority of these are problematic reflecting semantic or logical inconsistencies or being scored for only a single taxon and thus inappropriate for phylogenetic reconstruction Although the classification cannot be directly translated into a phylogenetic matrix the exposure of the characters that underlie a classification provide important perspective into the basis and limits of a classification system and offer a valuable starting point for the creation of a phylogenetic matrix (C) 2015 The Authors Published by Elsevier GmbH',NLP
'The retrieval effectiveness of Query Expansion (QE) is very much dependent on the ability to accurately identify and expand core concepts which are truly representative of the intended search goal Two characteristics of natural language queries which hinder the performance of query expansion for information retrieval are query length and structure The varying lengths of a query translate to the number of core concepts that may exist and the possibility of there being multiple query intents embedded within a single query On the other hand the structure of queries reveals the linguistic properties which allows for the determination of whether they take the form of well-formed sentences or are simply bags-of-words which in the strictest sense are a series of words with no obvious relations amongst them Whilst query lengths are easily assessed we propose a two-level automated classification technique consisting of linguistics based and statistical processing for query structure classification The proposed method has revealed high levels of classification accuracy on TREC ad hoc test queries',NLP
'Crushing of textual information is available in electronic form on internet As a result finding the answer to user query essential in natural Language processing information retrieval and question answering Semantic based question reformulation is frequently used in question answering system to retrieve answer from large document collection The goal of this paper is to find useful and standard reformulation pattern which can be used in our question answering (QA) system to find exact candidate answer In this paper we used TREC-8 TREC-9 and TREC-10 collection as training set Different types of question and corresponding answer can use from TREC collection The QA system will automatically extract the pattern from sentences retrieved from search engine With help of word net it will check syntactic tags semantic relation between question and answer pair Next weight can assigned to each extracted pattern according to is length the distance between keyword and the level of semantic similarity between the extracted question and answer The proposed systems vary from most former other reformulation learning system',NLP
'Many natural language processing tasks including information extraction question answering and recognizing textual entailment require analysis of the polarity focus of polarity tense aspect mood and source of the event mentions in a text in addition to its predicate argument structure analysis We refer to modality polarity and other associated information as extended modality In this paper we propose a new annotation scheme for representing the extended modality of event mentions in a sentence Our extended modality consists of the following seven components: Source Time Conditional Primary modality type Actuality Evaluation and Focus We reviewed the literature about extended modality in Linguistics and Natural Language Processing (NLP) and defined appropriate labels of each component In the proposed annotation scheme information of extended modality of an event mention is summarized at the core predicate of the event mention for immediate use in NLP applications We also report on the current progress of our manual annotation of a Japanese corpus of about 50000 event mentions showing a reasonably high ratio of inter-annotator agreement',NLP
'Textual Emotion Recognition (TER) is an important task in Natural Language Processing (NLP) due to its high impact in real-world applications Prior research has tackled the automatic classification of emotion expressions in text by maximising the probability of the correct emotion class using cross-entropy loss However this approach does not account for intra- and inter-class variations within and between emotion classes To overcome this problem we introduce a variant of triplet centre loss as an auxiliary task to emotion classification This allows TER models to learn compact and discriminative features Furthermore we introduce a method for evaluating the impact of intra- and inter-class variations on each emotion class Experiments performed on three datasets demonstrate the effectiveness of our method when applied to each emotion class in comparison to previous approaches Finally we present analyses that illustrate the benefits of our method in terms of improving the prediction scores as well as producing discriminative features',NLP
'The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps) Developers heavily rely on bug reports in issue tracking systems to reproduce failures (eg crashes) However the process of crash reproduction is often manually done by developers making the resolution of bugs inefficient especially given that bug reports are often written in natural language To improve the productivity of developers in resolving bug reports in this paper we introduce a novel approach called ReCDroid+ that can automatically reproduce crashes from bug reports for Android apps ReCDroid+ uses a combination of natural language processing (NLP) deep learning and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash We have evaluated ReCDroid+ on 66 original bug reports from 37 Android apps The results show that ReCDroid+ successfully reproduced 42 crashes (636% success rate) directly from the textual description of the manually reproduced bug reports A user study involving 12 participants demonstrates that ReCDroid+ can improve the productivity of developers when resolving crash bug reports',NLP
'The recent COVID-19 (novel coronavirus disease) pandemic induced a deep polarization among regional as well as global communities The sentiments regarding the pandemic and its impact on lifestyle and economy often expressed via social networks are regarded as critical metrics for capturing such polarization and formulating appropriate intervention by the relevant authorities While there exist a myriad of Natural Language Processing (NLP) models for mining social media data we demonstrate the shortcomings of the individual models in this paper and explore how to improve the COVID-19 sentiment analysis in social media network data via two hybrid predictive models based on a Long-Short-Term-Memory (LSTM)-based autoencoder and a Convolutional Neural Network (CNN) model coupled with a bi-directional LSTM Through extensive experiments on the recently acquired Twitter dataset we compare the COVID-19 sentiments exhibited in the USA and Canada using our proposed hybrid predictive models and demonstrate their superiority over individual Artificial Intelligence (AI) models',NLP
'Authorship Attribution (AA) is currently applied in several applications among which fraud detection and anti-plagiarism checks: this task can leverage stylometry and Natural Language Processing techniques In this work we explored some strategies to enhance the performance of an AA task for the automatic detection of false and misleading information (eg fake news) We set up a text classification model for AA based on stylometry exploiting recurrent deep neural networks and implemented two learning tasks trained on the same collection of fake and real news comparing their performances: one is based on Federated Learning architecture the other on a centralized architecture The goal was to discriminate potential fake information from true ones when the fake news comes from heterogeneous sources with different styles Preliminary experiments show that a distributed approach significantly improves recall with respect to the centralized model As expected precision was lower in the distributed model This aspect coupled with the statistical heterogeneity of data represents some open issues that will be further investigated in future work',NLP
'The number of scientific publications in the literature is steadily growing containing our knowledge in the biomedical health and clinical sciences Since there is currently no automatic archiving of the obtained results much of this information remains buried in textual details not readily available for further usage or analysis For this reason natural language processing (NLP) and text mining methods are used for information extraction from such publications In this paper we review practices for Named Entity Recognition (NER) and Relation Detection (RD) allowing eg to identify interactions between proteins and drugs or genes and diseases This information can be integrated into networks to summarize large-scale details on a particular biomedical or clinical problem which is then amenable for easy data management and further analysis Furthermore we survey novel deep learning methods that have recently been introduced for such tasks',NLP
'In Chinese societies superstition is of paramount importance and vehicle license plates with desirable numbers can fetch very high prices in auctions Unlike other valuable items license plates are not allocated an estimated price before auction I propose that the task of predicting plate prices can be viewed as a natural language processing (NLP) task as the value depends on the meaning of each individual character on the plate and its semantics I construct a deep recurrent neural network (RNN) to predict the prices of vehicle license plates in Hong Kong based on the characters on a plate I demonstrate the importance of having a deep network and of retraining Evaluated on 13 years of historical auction prices the deep RNNs predictions can explain over 80% of price variations outperforming previous models by a significant margin I also demonstrate how the model can be extended to become a search engine for plates and to provide estimates of the expected price distribution (C) 2019 Elsevier Ltd All rights reserved',NLP
'Call Centers have always played a highly significant role in the service industry from retail to technical support Government of India (GOI) has launched Kisan Call Centers (KCC) across the country to deliver extension services to the farming community for resolving their diverse problems Every time a farmer makes a call the query asked by the farmer is recorded manually and the number of calls generated in a day will be many and that leads to Big Data As the questions asked by farmers are recorded manually the same question asked by different farmers can be framed differently in terms of the words used for the query The idea of this paper is to analyze the large KCC datasets using Hadoop based MapReduce algorithms to draw interesting insights such as the hour during which highest number of calls are made the crop which has been questioned a lot by farmers and use Natural Language Processing (NLP) techniques to group the similar queries to know which queries are frequently asked by farmers project these findings graphically',NLP
'Configuration and parameterization of optimization frameworks for the computational support of design exploration can become an exclusive barrier for the adoption of such systems by engineers This work addresses the problem of defining the elements that constitute a multiple-objective design optimization problem that is design variables constants objective functions and constraint functions In light of this contributions are reviewed from the field of evolutionary design optimization with respect to their concrete implementation for design exploration Machine learning and natural language processing are supposed to facilitate feasible approaches to the support of configuration and parameterization Hence the authors further review promising machine learning and natural language processing methods for automatic knowledge elicitation and formalization with respect to their implementation for evolutionary design optimization These methods come from the fields of product attribute extraction clustering of design solutions relationship discovery computation of objective functions metamodeling and design pattern extraction',NLP
'Software maintainers are often challenged with source code changes to improve software systems or eliminate defects in unfamiliar programs To undertake these tasks a sufficient understanding of the system (or at least a small part of it) is required One of the most time consuming tasks of this process is locating which parts of the code are responsible for some key functionality or feature Feature (or concept) location techniques address this problem This paper introduces CONCLAVE an environment for software analysis and in particular the CONCLAVE-Mapper tool that provides a feature location facility This tool explores natural language terms used in programs (e g function and variable names) and using textual analysis and a collection of Natural Language Processing techniques computes synonymous sets of terms These sets are used to score relatedness between program elements and search queries or problem domain concepts producing sorted ranks of program elements that address the search criteria or concepts An empirical study is also discussed to evaluate the underlying feature location technique',NLP
'Ancient Chinese is the essence of Chinese culture There are several natural language processing tasks of ancient Chinese domain such as ancient-modern Chinese translation poem generation and couplet generation Previous studies usually use the supervised models which deeply rely on parallel data However it is difficult to obtain large-scale parallel data of ancient Chinese In order to make full use of the more easily available monolingual ancient Chinese corpora we release AnchiBERT a pre-trained language model based on the architecture of BERT which is trained on large-scale ancient Chinese corpora We evaluate AnchiBERT on both language understanding and generation tasks including poem classification ancient-modern Chinese translation poem generation and couplet generation The experimental results show that AnchiBERT outperforms BERT as well as the non-pretrained models and achieves state-of-the-art results in all cases',NLP
'Sentiment analysis and opinion mining are designed to detect peoples emotions and opinions Many public and private sectors are interested in extracting information regarding opinions that consist of subjective expressions across a variety of products or political decisions Recently the Arab region has had a significant role in international politics and the global economy that has grasped the attention of political and social scientists Detecting Arabic tweets will be helpful for politicians in predicting global events based on the popular news and peoples comments Still the Arabic language has not received a proper attention from modern computational linguists This literature review provides a comprehensive study of sentiment analysis on Twitter data and analyzes the existing work that has been done in order to detect and analyze Arabic tweets',NLP
'Sequential modeling is a fundamental task in scientific fields especially in speech and natural language processing where many problems of sequential data can be cast as a sequential labeling or a sequence classification In many applications the two problems are often correlated for example named entity recognition and dialog act classification for spoken language understanding This paper presents triangular-chain conditional random fields (CRFs) a unified probabilistic model combining two related problems Triangular-chain CRFs jointly represent the sequence and meta-sequence labels in a single graphical structure that both explicitly encodes their dependencies and preserves uncertainty between them An efficient inference and parameter estimation method is described for triangular-chain CRFs by extending linear-chain CRFs This method outperforms baseline models on synthetic data and real-world dialog data for spoken language understanding',NLP
'Lately with the increasing popularity of social media technologies applying natural language processing for mining information in tweets has posed itself as a challenging task and has attracted significant research efforts In contrast with the news text and others formal content tweets pose a number of new challenges due to their short and noisy nature Thus over the past decade different Named Entity Recognition (NER) architectures have been proposed to solve this problem However most of them are based on handcrafted-features and restricted to a particular domain which imposes a natural barrier to generalize over different contexts In this sense despite the long line of work in NER on formal domains there are no studies in NER for tweets in Portuguese (despite 1797 million monthly active users) To bridge this gap we present a new gold-standard corpus of tweets annotated for Person Location and Organization (PLO) Additionally we also perform multiple NER experiments using a variety of Long Short-Term Memory (LSTM) based models without resorting to any handcrafted rules Our approach with a centered context input window of word embeddings yields 5278 F1 score 3868% higher compared to a state of the art baseline system',NLP
'Radiological reporting has generated large quantities of digital content within the electronic health record which is potentially a valuable source of information for improving clinical care and supporting research Although radiology reports are stored for communication and documentation of diagnostic imaging harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative from which it is a major challenge to obtain structured data Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation and thus enables computers to derive meaning from human (ie natural language) input Used on radiology reports NLP techniques enable automatic identification and extraction of information By exploring the various purposes for their use this review examines how radiology benefits from NLP A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology This review takes a close look at the individual studies in terms of tasks (ie the extracted information) the NLP methodology and tools used and their application purpose and performance results Additionally limitations future challenges and requirements for advancing NLP in radiology will be discussed (C) RSNA 2016',NLP
'Paraphrase detection and generation are important natural language processing (NLP) tasks Yet the term paraphrase is broad enough to include many fine-grained relations This leads to different tolerance levels of semantic divergence in the positive paraphrase class among publicly available paraphrase datasets Such variation can affect the generalisability of paraphrase classification models It may also impact the predictability of paraphrase generation models This paper presents a new model which can use few corpora of fine-grained paraphrase relations to construct automatically using language inference models The fine-grained sentence level paraphrase relations are defined based on word and phrase level counterparts We demonstrate that the fine-grained labels from our proposed system can make it possible to generate paraphrases at desirable semantic level The new labels could also contribute to general sentence embedding techniques',NLP
'Among the new forms of technology-facilitated abuses cyberstalking has become a growing and important problem Cyberstalking involves the use of technology to stalk threaten or harass one or more individuals For example it can include tracking and intimidating a victim over social media email or text messages or threatening to expose someones intimate photographs (sextortion) Cyberstalking has become a mechanism used by current or former domestic or intimate-partners lone perpetrators individuals targeting victims based on their employment or public image and members of extremist groups The innovations of this research are twofold First using multiple data sets we developed an automated capability to identify and collect the complete set of all federally prosecuted cyberstalking cases in the US Second we employ natural language processing network and regression methods to code and analyze the court records We apply these methods in order to answer three main research questions: how many federal cyberstalking cases are there?; what kinds of stalking behavior are being committed?; and what characteristics are correlated with conviction and severity of punishment?',NLP
'Exploring legal documents such as laws judgments and contracts is known to be a time-consuming task To support domain experts in efficiently browsing their contents legal documents in electronic form are commonly enriched with semantic annotations They consist of a list of headwords indicating the main topics Annotations are commonly organized in taxonomies which comprise both a set of is-a hierarchies expressing parent/child-sibling relationships and more arbitrary related-to semantic links This paper addresses the use of Deep Learning-based Natural Language Processing techniques to automatically extract unknown taxonomy relationships between pairs of legal documents Exploring the document content is particularly useful for automatically classifying legal document pairs when topic-level relationships are partly out-of-date or missing which is quite common for related-to links The experimental results collected on a real heterogeneous collection of Italian legal documents show that word-level vector representations of text are particularly effective in leveraging the presence of domain-specific terms for classification and overcome the limitations of contextualized embeddings when there is a lack of annotated data',NLP
'The size of data on the Internet has risen in an exponential manner over the past decade Thus the need for a solution emerges that transforms this vast raw information into useful information which a human brain can understand One such common technique in research that helps in dealing with enormous data is text summarization Automatic summarization is a renowned approach which is used to reduce a document to its main ideas It operates by preserving substantial information by creating a shortened version of the text Text Summarization is categorized into Extractive and Abstractive methods Extractive methods of summarization minimize the burden of summarization by choosing from the actual text a subset of sentences that are relevant Although there are a ton of methods researchers specializing in Natural Language Processing (NLP) are particularly drawn to extractive methods Based on linguistic and statistical characteristics the implications of sentences are calculared A study of extractive and abstract methods for summarizing texts has been made in this paper This paper also analyses above mentioned methods which yields a less repetitive and a more concentrated summary',NLP
'We apply Natural Language Processing and supervised Machine Learning to predict stock price movements based on approximately 30000 ad-hoc disclosures issued by publicly traded companies on Nasdaq OMX Stockholm Three different labeling methods based on financial theory are defined and assessed The best results using Logistic Regression and TF-IDF with character-grams achieve an increase of 63 percentage points above a majority class baseline These results show that corporate ad-hoc disclosures which are regulated to represent novel and value-relevant information are particularly well-suited for this task Furthermore the most sophisticated labeling technique used Jensens Alpha in the context of the Capital Asset Pricing Model helps the model achieve its highest accuracy The results therefore show that financial theory can help isolate the effect of an informational event on stock prices improving the supervised Machine Learning approach Finally an algorithmic trading strategy is simulated with the best model yielding positive abnormal returns',NLP
'We present an overview of the CLEF-2019 Lab ProtestNews on Extracting Protests from News in the context of generalizable natural language processing The lab consists of document sentence and token level information classification and extraction tasks that were referred as task 1 task 2 and task 3 respectively in the scope of this lab The tasks required the participants to identify protest relevant information from English local news at one or more aforementioned levels in a cross-context setting which is cross-country in the scope of this lab The training and development data were collected from India and test data was collected from India and China The lab attracted 58 teams to participate in the lab 12 and 9 of these teams submitted results and working notes respectively We have observed neural networks yield the best results and the performance drops significantly for majority of the submissions in the cross-country setting which is China',NLP
'Convolutional neural network (CNN) has lately received great attention because of its good performance in the field of computer vision and speech recognition It has also been widely used in natural language processing But those methods for English cannot be transplanted due to phrase segmentation Those for Chinese are not good enough for poorly semantic retrieving We propose a Chinese sentiment classification model on the concept of convolution control block (CCB) It aims at classifying Chinese sentences into the positive or the negative CCB based model considers short and long context dependencies Parallel convolution of different kernel sizes is designed for phrase segmentation gate convolution for merging and filtering abstract features and tiering 5 layers of CCBs for word connection in sentence Our model is evaluated on Million Chinese Hotel Review dataset Its positive emotion accuracy reaches 9258% which outperforms LR_all and DCN by 289% and 403% respectively Model depth and sentence length are positively related to the accuracy Gate convolution indeed improves model accuracy (C) 2017 Elsevier Inc All rights reserved',NLP
'Comprehensive data mining of the scientific literature has become an increasing challenge To address this challenge Elseviers Pathway Studio software uses the techniques of natural language processing to systematically extract specific biological information from journal articles and abstracts that is then used to create a very large structured and constantly expanding literature knowledgebase Highly sophisticated visualization tools allow the user to interactively explore the vast number of connections created and stored in the Pathway Studio database We demonstrate the value of this structured information approach by way of a biomarker use case example and describe a comprehensive collection of biomarkers and biomarker candidates as reported in the literature We use four major neuropsychiatric diseases to demonstrate common and unique biomarker elements demonstrate specific enrichment patterns and highlight strategies for identifying the most recent and novel reports for potential biomarker discovery Finally we introduce an innovative new taxonomy based on brain region identifications which greatly increases the potential depth and complexity of information retrieval related to and now accessible for neuroscience research',NLP
'Unlike most online social networks where explicit links among individual users are defined the relations among commercial entities (eg firms) may not be explicitly declared in commercial Web sites One main contribution of this article is the development of a novel computational model for the discovery of the latent relations among commercial entities from online financial news More specifically a CRF model which can exploit both structural and contextual features is applied to commercial entity recognition In addition a point-wise mutual information (PMI)-based unsupervised learning method is developed for commercial relation identification To evaluate the effectiveness of the proposed computational methods a prototype system called CoNet has been developed Based on the financial news articles crawled from Google finance the CoNet system achieves average F-scores of 0681 and 0754 in commercial entity recognition and commercial relation identification respectively Our experimental results confirm that the proposed shallow natural language processing methods are effective for the discovery of latent commercial networks from online financial news',NLP
'The aim of this study is to explore the word sense disambiguation (WSD) problem across two biomedical domains-biomedical literature and clinical notes A supervised machine learning technique was used for the WSD task One of the challenges addressed is the creation of a suitable clinical corpus with manual sense annotations This corpus in conjunction with the WSD set from the National Library of Medicine provided the basis for the evaluation of our method across multiple domains and for the comparison of our results to published ones Noteworthy is that only 20% of the most relevant ambiguous terms within a domain overlap between the two domains having more senses associated with them in the clinical space than in the biomedical literature space Experimentation with 28 different feature sets rendered a system achieving an average F-score of 082 on the clinical data and 086 on the biomedical literature (c) 2008 Elsevier Inc All rights reserved',NLP
'Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender This is a crucial issue which may harm minorities and underrepresented groups if such systems were integrated in real-world applications In this paper we create ad hoc tests through the CheckList tool (Ribeiro et al 2020) to detect biases within abusive language classifiers for English We compare the behaviour of two BERT-based models one trained on a generic abusive language dataset and the other on a dataset for misogyny detection Our evaluation shows that although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks they perform very poorly as regards fairness and bias in particular on samples involving implicit stereotypes expressions of hate towards minorities and protected attributes such as race or sexual orientation We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList',NLP
'Object-oriented development of software systems and their components is based on the application of object-oriented models technologies and tools that support them Real systems are characterized by complexity which is caused by problems describing the properties and behavior of subject domain objects A description of the subject domain is given in the natural language which can be considered an input language at the stage of object-oriented analysis Syntax-oriented processing of sentences in the input language forms the output of the model software system The conversion of the input language into a certain output is reduced to the construction of some translator which for any transformations of the sentences of the input language uses the structure of this sentence The ideas on which this concept is based include the ideas of formal grammars and semantic calculations for symbolic chains These dquestions are the basis of the work under consideration',NLP
'Automatic speech recognition systems have gained popularity due to their gain in terms of usability and integration in cross domain applications While traditional approaches are developed over elaborated pipelines that need specific pre-trained models for a language (acoustic model a phonetic dictionary etc) deep learning architectures like Recurrent Neural Networks have been trained for automatic speech recognition using only large datasets of speech corpora (audio and aligned transcript files) Starting from the DeepSpeech architecture we present the performance of the model trained for Romanian language over the SWARA speech corpus which contains almost 21 hours of speech data using 17 different speakers The experiments were focused on obtaining the best performance of the network in terms of Word Error Rate by tweaking the parameters of the model on the SWARA dataset We present preliminary results obtained for this Romanian dataset alongside with the encountered limitations while training the model on other languages besides English',NLP
'Multilingual linguistic resources are usually constructed from parallel corpora but since these corpora are available only for selected text domains and language pairs the potential of other resources is being explored as well This article seeks to explore and exploit the idea of using multilingual Web-based encyclopaedias such as Wikipedia as comparable corpora for multilingual terminology extraction We propose an approach to extract terms and their translations from different types of Wikipedia link information and texts The next step will be using the linguistics information to re-rank and filter the extracted term candidates in the target language Preliminary evaluations using the combined statistics-based and linguistic-based approaches were applied on Japanese-French-English languages These evaluations showed a real open improvement and good quality of the extracted term candidates for building or enriching multilingual ontologies dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query',NLP
'A top-down parsing algorithm has been constructed to accommodate any form of ambiguous context-free grammar augmented with semantic rules with arbitrary attribute dependencies A memoization technique is used with this non-strict method for efficiently processing ambiguous input This one-pass approach allows Natural Language (NL) processors to be constructed as executable modular and declarative specifications of Attribute Grammars',NLP
'This paper adopts a function of Natural Language Processing (NLP) which based on Comprehensive Information (CI) to analyze the text after Automatic Speech Recognition (ASR) for error detection and correction in order to improve the accuracy and of recognition Tests have been done based on the demo terminal for the National 863 Project of Olympics oriented Multilingual Intelligent Information Service System',NLP
'This paper describes a new probabilistic sentence reduction method using maximum entropy model In contrast to previous methods the proposed method has the ability to produce multiple best results for a given sentence which is useful in text summarization applications Experimental results show that the proposed method improves on earlier methods in both accuracy and computation time',NLP
'is a core tenet of human cognition and communication When composing natural language instructions humans naturally evoke abstraction to convey complex procedures in an efficient and concise way Yet interpreting and grounding abstraction expressed in NL has not yet been systematically studied in NLP with no accepted benchmarks specifically eliciting abstraction in NL In this work we set the foundation for a systematic study of processing and grounding abstraction in NLP First we deliver a novel abstraction elicitation method and present Hexagons a 2D instruction-following game Using Hexagons we collected over 4k naturally occurring visually-grounded instructions rich with diverse types of abstractions From these data we derive an instruction-to-execution task and assess different types of neural models Our results show that contemporary models and modeling practices are substantially inferior to human performance and that model performance is inversely correlated with the level of abstraction showing less satisfying performance on higher levels of abstraction These findings are consistent across models and setups confirming that abstraction is a challenging phenomenon deserving further attention and study in NLP/AI research',NLP
'Students learn more from doing activities and practicing their skills on assessments yet it can be challenging and time consuming to generate such practice opportunities In our work we examine how advances in natural language processing and question generation may help address this issue In particular we present a pipeline for generating and evaluating questions from text-based learning materials in an introductory data science course The pipeline includes applying a text-to-text transformer (T5) question generation model and a concept hierarchy extraction model on the text content then scoring the generated questions based on their relevance to the extracted key concepts We further evaluated the question quality with three different approaches: information score automated rating by a trained model (Google GPT-3) and manual review by human instructors Our results showed that the generated questions were rated favorably by all three evaluation methods We conclude with a discussion of the strengths and weaknesses of the generated questions and outline the next steps towards refining the pipeline and promoting natural language processing research in educational domains',NLP
'Part-of-Speech (POS) tagging for sentences is a basic and widely-used Natural Language Processing (NLP) technique People rely heavily on it to predict POS tags that serve as the base for many advanced NLP tasks such as sentiment analysis word sense disambiguation and information retrieval However POS tagging tools could make wrong predictions which bring consequent error propagation to the advanced tasks and even cause serious threats in critical application domains In this paper we propose to test POS tagging tools with Metamorphic Testing against some properties that they should follow The preliminary exploration with two groups of Metamorphic Relations shows that our method can effectively reveal defects of three common POS tagging tools (ie spaCy NLTK and Flair) on handling fairly simple intra- and inter-sentence transformation regarding adverbial clause and sentence appending This demonstrates the great potential of our method to deliver a systematic test and reveal the unaware issues which may benefit the validation repair and improvement for POS tagging tools',NLP
'In the last few years online social media networks have witnessed an amazing growth in their worldwide usage with millions of users constantly publishing messages containing opinions on virtually any imaginable topic including opinions about companies Accurately understanding these opinions could provide an almost real-time overview of how the company and its actions are perceived by the general public While existing approaches used for analyzing the opinions expressed in social media messages commonly limit themselves in discovering the polarity of the messages expressed as a positive negative or neutral value in the present paper we use semantic web technologies and natural language processing in order to uncover actual feelings such as happiness surprise or disappointment The emotions are structured in a hierarchy using an ontology thus offering the possibility to analyze the overall opinion regarding the company at different levels of granularity The proposed approach is validated by performing an analysis of the public perception towards four well-known technology companies',NLP
'Text classification is a basic and important work in natural language processing (NLP) The existing text classification models are powerful However training such a model requires a large number of labeled training sets but in the actual scene insufficient data is often faced with The lack of data is mainly divided into two categories: cold start and low resources To solve this problem text enhancement methods are usually used In this paper the source text enhancement and representation enhancement are combined to improve the enhancement effect Five sets of experiments are designed to verify that our method is effective on different data sets and different classifiers The simulation results show that the accuracy is improved and the generalization ability of the classifier is enhanced to some extent We also find that the enhancement factor and the size of the training data set are not positively related to the enhancement effect Therefore the enhancement factor needs to be selected according to the characteristics of the data',NLP
'Cyber attackers gain access into systems networks and cyberin-frastructure by escalating privileges to confidential information regardless of the efforts systems engineers put into security The chess game between cyber offense and defense destabilizes the ability of organizations to protect their information assets This research employs the lens of Activity Theory to study the interaction through the contradictions embedded between the cyber attackers and cyber defenders These types of contradictions were forcefully created and simulated in the cyber security virtual lab at Florida State University for the purpose of facilitating real-world scenario-based learning experiences Both network traffic data and interviews were collected in order to identify the boundary objects that intersect the two activity systems Natural language processing (NLP) was adopted to explore and extract topics frequently used by both activity systems Consciousness of cyber defense was expanded by creating contradictions and boundary objects were identified by comparing the interactions between these two activity systems',NLP
'We propose a novel approach to multimodal sentiment analysis using deep neural networks combining visual analysis and natural language processing Our goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment; instead we aim to infer the latent emotional state of the user Thus we focus on predicting the emotion word tags attached by users to their Tumblr posts treating these as self-reported emotions We demonstrate that our multi-modal model combining both text and image features outperforms separate models based solely on either images or text Our models results are interpretable automatically yielding sensible word lists associated with emotions We explore the structure of emotions implied by our model and compare it to what has been posited in the psychology literature and validate our model on a set of images that have been used in psychology studies Finally our work also provides a useful tool for the growing academic study of images-both photographs and memes-on social networks',NLP
'Violence is an epidemic in Brazil and a problem on the rise world-wide Mobile devices provide communication technologies which can be used to monitor and alert about violent situations However current solutions like panic buttons or safe words might increase the loss of life in violent situations We propose an embedded artificial intelligence solution using natural language and speech processing technology to silently alert someone who can help in this situation The corpus used contains 400 positive phrases and 800 negative phrases totaling 1200 sentences which are classified using two well-known extraction methods for natural language processing tasks: bag-of-words and word embeddings and classified with a support vector machine We describe the proof-of-concept product in development with promising results indicating a path towards a commercial product More importantly we show that model improvements via word embeddings and data augmentation techniques provide an intrinsically robust model The final embedded solution also has a small footprint of less than 10 MB',NLP
'Language and speech are the primary source of data for psychiatrists to diagnose and treat mental disorders In psychosis the very structure of language can be disturbed including semantic coherence (eg derailment and tangentiality) and syntactic complexity (eg concreteness) Subtle disturbances in language are evident in schizophrenia even prior to first psychosis onset during prodromal stages Using computer-based natural language processing analyses we previously showed that among English-speaking clinical (eg ultra) high-risk youths baseline reduction in semantic coherence (the flow of meaning in speech) and in syntactic complexity could predict subsequent psychosis onset with high accuracy Herein we aimed to cross-validate these automated linguistic analytic methods in a second larger risk cohort also English-speaking and to discriminate speech in psychosis from normal speech We identified an automated machine-learning speech classifier - comprising decreased semantic coherence greater variance in that coherence and reduced usage of possessive pronouns - that had an 83% accuracy in predicting psychosis onset (intra-protocol) a cross-validated accuracy of 79% of psychosis onset prediction in the original risk cohort (cross-protocol) and a 72% accuracy in discriminating the speech of recent-onset psychosis patients from that of healthy individuals The classifier was highly correlated with previously identified manual linguistic predictors Our findings support the utility and validity of automated natural language processing methods to characterize disturbances in semantics and syntax across stages of psychotic disorder The next steps will be to apply these methods in larger risk cohorts to further test reproducibility also in languages other than English and identify sources of variability This technology has the potential to improve prediction of psychosis outcome among at-risk youths and identify linguistic targets for remediation and preventive intervention More broadly automated linguistic analysis can be a powerful tool for diagnosis and treatment across neuropsychiatry',NLP
'Unified modeling language (UML) activity diagrams depict the internal behavior of different program operations with the help of nodes and edges describing the business logic in user requirements Traditionally requirements engineers and practitioners refer to business process documents and analyze them to build UML activity diagrams manually which makes labor and time consuming Recently deep learning technology has been utilized in various fields and has achieved excellent results We propose a novel pipeline named TAG for automatically generating UML activity diagrams based on deep learning The inspiration for TAG is as follows: (1) Semantic roles such as signal and condition entities in texts can be obtained via sequence labeling; (2) A business process document corresponds to a semantic role sequence According to the predefined rules the semantic role sequence is used to construct the graph neural network and the temporal activity relationship in a business process document is predicted via multi-layer semantic fusion; (3) Use temporal activity relationships to generate UML activity diagrams automatically The entire process was automatically completed SAP is the largest non-American software company by revenue We obtained the original data from the SAP website and sorted them as business process documents After preliminary experiments a temporal activity relationship prediction accuracy rate of 7987% was achieved Simultaneously some business process documents are available from https://githubcomdwx142857/bussiness-process',NLP
'In corpus preparation we do part-of-speech (POS) tagging where we add POS information into the corpus in the form of tags The POS information contains a number of tags such as noun pronoun verb adjective adverbs preposition conjunction etc Literature shows the lack of corpora for Santali language In this paper we have created and described a Santali language corpus using Sketch Engine corpus query tool that is very much useful for linguistic research This paper shows statistical information about the Santali corpus such as number of words tokens and sentences We have shown lexicons such as number of words tags lemmas and tempos This paper also shows parts of speech tags and lemmatized corpus in terms of noun numeral preposition pronoun verb adjective adverb conjunction We have added 590314 tokens 425238 words and 63199 sentences in our Santali corpus',NLP
'During requirements elicitation different stakeholders with diverse backgrounds and skills need to effectively communicate to reach a shared understanding of the problem at hand Linguistic ambiguity due to terminological discrepancies may occur between stakeholders that belong to different technical domains If not properly addressed ambiguity can create frustration and distrust during requirements elicitation meetings and lead to problems at later stages of development This paper presents a natural language processing approach to identify ambiguous terms between different domains The approach is based on building domain-specific language models one for each stakeholders domain Word embeddings from each language model are compared in order to measure the differences of use of a word thus estimating its potential ambiguity across the domains of interest The proposed strategy can be useful to prepare lists of dangerous terms to take into account during requirements elicitation meetings such as workshops or focus groups when these involve stakeholders from distant domains',NLP
'We present on extraction of Co-reference chains from a document Co-reference chains show cohesiveness of the document The cohesiveness in the document is marked by cohesive markers such as Reference Substitution Ellipsis Conjunction and Lexical cohesion In this work we will take up Pronominal Reflexives R-expressions and form Co-reference chains for each of the above markers The Co-reference chains are very essential in building sophisticated natural language processing applications such as information extraction profile generator entity specific text summarization etc It is also needed in machine translation and information retrieval task Though pronominal resolution is dealt in few Indian languages such as Tamil Hindi Bengali Malayalam extraction of co-reference chains in Indian languages is not attempted We extract co-reference chains from Tamil language text We have evaluated the system with real time data and results are encouraging',NLP
'In the domain of high-energy physics (HEP) query languages in general and SQL in particular have found limited acceptance This is surprising since HEP data analysis matches the SQL model well: the data is fully structured and queried using mostly standard operators To gain insights on why this is the case we perform a comprehensive analysis of six diverse general-purpose data processing platforms using an HEP benchmark The result of the evaluation is an interesting and rather complex picture of existing solutions: Their query languages vary greatly in how natural and concise HEP query patterns can be expressed Furthermore most of them are also between one and two orders of magnitude slower than the domain-specific system used by particle physicists today These observations suggest that while database systems and their query languages are in principle viable tools for HEP significant work remains to make them relevant to HEP researchers',NLP
'Linguistic diversity is one of the core challenges faced by natural language processing technology and paraphrase reflects the diversity of language In recent years Chinese paraphrase technology has received wide attention from academic and business researchers There is a number of Chinese paraphrase datasets are published and paraphrase detection evaluation tasks have been released However most of these Chinese paraphrase datasets have not been widely used To resolve the above issue we try to evaluate the value of current Chinese paraphrase datasets through paraphrase detection task and whether the value of paraphrase is weakened in the era of pre-trained language models such as Bert The experimental results show that the current Chinese paraphrase dataset has a significant enhancement effect on the paraphrase detection task And the performances of paraphrase detection achieved by Bert can be further enhanced with paraphrase dataset The result proves the value of Chinese paraphrase dataset on paraphrase detection task',NLP
'Latest development of neural models has connected the encoder and decoder through a self-attention mechanism In particular Transformer which is solely based on self-attention has led to breakthroughs in Natural Language Processing (NLP) tasks However the multi-head attention mechanism as a key component of Transformer limits the effective deployment of the model to a resource-limited setting In this paper based on the ideas of tensor decomposition and parameters sharing we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD) We test and verify the proposed attention method on three language modeling tasks (ie PTB WikiText-103 and One-billion) and a neural machine translation task (ie WMT-2016 English-German) Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements compared with a number of language modeling approaches such as Transformer Transformer-XL and Transformer with tensor train decomposition',NLP
'In this paper we present an agent-based add-on for the Social-Net Tourism Recommender System that uses information extraction and natural language processing techniques in order to automatically extract and classify information from the Web Its goal is to maintain the system updated and obtain information about third parties services that are not offered by service providers inside the system',NLP
'Disentangled representations have attracted increasing attention recently However how to transfer the desired properties of disentanglement to word representations is unclear In this work we propose to transform typical dense word vectors into disentangled embeddings featuring improved interpretability via encoding polysemous semantics separately We also found the modular structure of our disentangled word embeddings helps generate more efficient and effective features for natural language processing tasks',NLP
'We present CAM (comparative argumentative machine) a novel open-domain IR system to argumentatively compare objects with respect to information extracted from the Common Crawl In a user study the participants obtained 15% more accurate answers using CAM compared to a traditional keyword-based search and were 20% faster in finding the answer to comparative questions',NLP
'Semantic Textual Similarity (STS) is to decide the level of semantic comparability between pairs of sentences STS assumes an essential part in Natural Language Processing errands which has drawn considerable attention in the field of research in recent years This survey discusses multiple features including word alignment-based similarity sentence vector-based similarity and sentence constituent similarity to assess the correctness of sentence pairs',NLP
'This discussion paper reviews both finance and computer science literature on sentiment The toolset for addressing the sentiment analysis problem in computer science goes beyond methods commonly used in finance literature The broader toolset of computer science has potential to improve the performance of sentiment measurement and predictability in financial markets',NLP
'MorAz is an open-source morphological analyzer for Azerbaijani Turkish The analyzer is available through both as a website for interactive exploration and as a RESTful web service for integration into a natural language processing pipeline MorAz implements the morphology of Azerbaijani Turkish following a two-level approach using Helsinki finite-state transducer and wraps the analyzer with python scripts in a Django instance',NLP
'This paper describes some automatic conversion methods and a system which converts Japanese specifications to UML class diagrams The characteristics and limitation of the methods was also discussed in terms of two experiments with use of specifications in some UML textbooks The results of evaluation experiments showed the fundamental validity of the methods and system',NLP
'A number of different IoT devices are already available and newer types would appear on the market A technical difficulty in IoT computing is to manage the heterogeneity of IoT devices in terms of their network protocol supported interface language data exchange scheme and type of mobility provided Business Process Execution Language (BPEL) has been effectively utilized to collaborate multiple services by defining a business workflow Due to the effectiveness of BPEL a natural attempt to effectively develop IoT applications with complex workflows would be the adoption of BPEL and integrating IoT devices in BPEL-based collaborations However there exists two technical challenges; (1) Mismatch between BPEL collaboration models and IoT services and (2) heterogeneity of IoT devices In this paper we propose an effective method to remedy the technical challenges by presenting Adapters between BPEL documents and IoT services With our method heterogeneous IoT services can be seamlessly collaborated with the BPEL-based collaborations',NLP
'Information distillation aims to extract the most useful pieces of information related to a given query from massive possibly multilingual audio and textual document sources One critical component in a distillation engine is detecting sentences to be extracted from each relevant document In this paper we present a statistical sentence extraction approach for distillation Basically we frame this task as a classification problem where each candidate sentence in documents is classified as relevant to the query or not These documents may be in textual or audio format and in a number of languages For audio documents we use both manual and automatic transcriptions for non-English documents we use automatic translations In this work we use AdaBoost a discriminative classification method with both lexical and semantic features The results indicate 11%-13% relative improvement over a baseline keyword-spotting-based approach We also show the robustness of our method on the audio subset of the document sources using manual and automatic transcriptions',NLP
'Grading system is a mechanism used to determine students ability of the given material of the studying process Essay is one form of grading where there are no choices of answer and student must answer in sentence Essay answers may vary greatly between each exam participant depending on their own thought Methods used in automated essay grading system nowadays are still under research because they need to follow some specific rules Some of the methods are Natural Language Processing Statistical Approach Bayesian Text Classification and Latent Semantic Analysis [1] One of the grading methods ie Latent Semantic Analysis (LSA) which uses matrix algebra to compare between expected answer and students essay answer [2] In this paper we are using the LSA technique for Web Based Automated Essay Grading for Indonesian Language (Bahasa) and prove that we achieve 83% for the agreement with Human Raters',NLP
'NLP (Natural Language Processing) is a technology that enables computers to understand human languages Deep-level grammatical and semantic analysis usually uses words as the basic unit and word segmentation is usually the primary task of NLP In order to solve the practical problem of huge structural differences between different data modalities in a multi-modal environment and traditional machine learning methods cannot be directly applied this paper introduces the feature extraction method of deep learning and applies the ideas of deep learning to multi-modal feature extraction This paper proposes a multi-modal neural network For each mode there is a multilayer sub-neural network with an independent structure corresponding to it It is used to convert the features in different modes to the same-modal features In terms of word segmentation processing in view of the problems that existing word segmentation methods can hardly guarantee long-term dependency of text semantics and long training prediction time a hybrid network English word segmentation processing method is proposed This method applies BI-GRU (Bidirectional Gated Recurrent Unit) to English word segmentation and uses the CRF (Conditional Random Field) model to annotate sentences in sequence effectively solving the long-distance dependency of text semantics shortening network training and predicted time Experiments show that the processing effect of this method on word segmentation is similar to that of BI-LSTM-CRF (Bidirectional- Long Short Term Memory-Conditional Random Field) model but the average predicted processing speed is 194 times that of BI-LSTM-CRF effectively improving the efficiency of word segmentation processing',NLP
'With the increase in internet access and the ease of writing comments in the Nepali language fine-grained sentiment analysis of social media comments is becoming more and more pertinent There are a number of benchmarked datasets for high-resource languages (English French and German) in specific domains like restaurants hotels or electronic goods but not in low-resource languages like Nepali In this paper we present our work to create a dataset for the targeted aspect-based sentiment analysis in the social media domain set up a dataset benchmark and evaluate using various machine learning models The dataset comprises of code-mixed and code-switched comments extracted from Nepali YouTube videos We present convincing baselines using a multilingual BERT model for the Aspect Term Extraction task and BiLSTM model for the Sentiment Classification Task achieving 57978% and 8160% F1 score respectively',NLP
'Part of Speech (POS) tagger is a necessary module in many natural language text processing tasks A POS tagger is a program that accepts an unprepared raw text in input and to each word adds a tag specifying its grammatical properties such as part of speech number person etc One of popular POS taggers-TnT tagger-has been extensively tested for English and some other languages This paper reports on it evaluation for Spanish language Error analysis is reported explaining how some specific features of Spanish language affect tagger performance It is reported that on Spanish texts TnT shows overall tagging accuracy between 9295% and 9584% specifically between 9547% and 9856% on known words and between 7557% and 8349% on unknown words Results show that TnT has reached a good level of maturity and is helpful enough for NLP tasks',NLP
'One of the first steps in comprehending natural or spoken language is named-entity recognition It plays a critical role in natural language processing applications such as text clustering subject detection topic detection and text summarization The most common techniques in information extraction using named-entity recognition can be applied to building automatic question-answering systems semantic web technology automatic translation machines and so on For input text with correct formatting studies on the named entity recognition (NER) task have achieved excellent and nearly humanequal results However for the output documents of the speech recognition system (ASR) the normative signs of the text such as punctuation and uppercase no longer exist causing difficulties for the researchers In this paper we present the process of building a Vietnamese speech dataset for the NER task and propose a new end-to-end approach for the NER of Vietnamese speech In addition we combine the multi-task learning model with the punctuation and uppercase (CaPu) recovery model and demonstrate the combinations effectiveness when documenting the approximately 5% improvement in the F1 score',NLP
'Machines have achieved a broad and growing set of linguistic competencies thanks to recent progress in Natural Language Processing (NLP) Psychologists have shown increasing interest in such models comparing their output to psychological judgments such as similarity association priming and comprehension raising the question of whether the models could serve as psychological theories In this article we compare how humans and machines represent the meaning of words We argue that contemporary NLP systems are fairly successful models of human word similarity but they fall short in many other respects Current models are too strongly linked to the text-based patterns in large corpora and too weakly linked to the desires goals and beliefs that people express through words Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not We discuss promising approaches to grounding NLP systems and argue that they will be more successful with a more human-like conceptual basis for word meaning',NLP
'Background: Nurses often document patient symptoms in narrative notes Purpose: This study used a technique called natural language processing (NLP) to: (1) Automatically identify documentation of seven common symptoms (anxiety cognitive disturbance depressed mood fatigue sleep disturbance pain and well-being) in homecare narrative nursing notes and (2) examine the association between symptoms and emergency department visits or hospital admissions from homecare Method: NLP was applied on a large subset of narrative notes (25 million notes) documented for 89825 patients admitted to one large homecare agency in the Northeast United States Findings: NLP accurately identified symptoms in narrative notes Patients with more documented symptom categories had higher risk of emergency department visit or hospital admission Discussion: Further research is needed to explore additional symptoms and implement NLP systems in the homecare setting to enable early identification of concerning patient trends leading to emergency department visit or hospital admission',NLP
'Independent living care for polypharmacy patients can be complicated in those situations with medications that are pro re nata (PRN as needed) Such medication regimen may involve multiple dosing whereby specific drug contraindications might be easily overlooked by hospice and palliative care patients or by those isolated and not in regular contact with care providers The goal of this paper is to describe the development steps and current design of a system providing medication decision support for isolated patients With an increased number of patients living alone or isolated - a situation exacerbated during the COVID19 pandemic - polypharmacy patients may be challenged when PRN (as needed) medications confound what might ordinarily be a routine medication schedule Central to our medication management system design is the so-called conversational agent that when integrated with a natural language processing front- end and classification tree algorithm provide a dynamic framework for patient self-management of medications Research on patient need revealed patients were more likely to embrace the system if the system were autonomous secure and not cloud-based',NLP
'One of the features of Electronic Health Records (EHR) is to store the patient clinical data Despite the efforts to structure all this data clinical reports and notes containing essential information about each patient are still stored in free text Some of this information refers to the familys health history and may be highly relevance for diagnosis and prognosis We proposed two methodologies to unify this knowledge and extract family history information from clinical notes using rule-based techniques in natural language processing (NLP) With these methods we intend to collect the family members mentioned in the text as well as associations to diseases and living status The proposed methods were evaluated into two stages demonstrating F-scores of 072 and 074 for the discovery of family members and observations and 062 and 052 for the detection of the family relations with the observations and their living status Our methodologies raised new strategies to automatically annotate large amounts of EHRs facilitating the detection of comorbidities within family relations',NLP
'In this paper we introduce the first simulation framework that provides cost-effective means to evaluate indoor navigation systems for different user groups (eg users with visual impairment) various positioning techniques and navigation instructions algorithms The simulation engine uses the Unity game engine which tracks the virtual user interaction and motion in a virtual environment that represents the physical environment in which the user navigates The framework includes the following modules that will be defined by the indoor navigation system developers: 1) Positioning module which simulates indoor localization techniques and their observed accuracy; 2) Indoor navigation algorithm module that generates the navigation instructions using natural language phrases and 3) Virtual user model (VUM) that includes human perception and information processing that can understand navigation instructions perceive the surrounding environment and act based on this information The framework also includes a performance analysis module that evaluates the indoor navigation system performance in terms of navigation success rate and route similarity',NLP
'Text summarization is an incipient practice for verdict out the summary of the text article Text summarization has grew so uses such as Due to the enormous aggregate of information getting augmented on internet; it is challenging for the user to verve through altogether the information accessible on web The large availability of internet content partakes constrained a broad research area in the extent of automatic text summarization contained by the Natural Language Processing (NLP) especially statistical machine learning communal Terminated the bygone half a century the defaulting has been addressed from numerous diverse standpoints in erratic domains and using innumerable archetypes In this survey paper we investigate the popular and important work done in the field of single and multiple document summarizations generous distinctive prominence towards pragmatic approaches and extractive techniques Particular auspicious slants that quintessence on unambiguous minutiae of the summarization are also deliberated Exceptional consideration is ardent to involuntary assessment of summarization classifications as forthcoming investigation on summarization is sturdily reliant over evolvement in this problem space',NLP
'Word sense induction (WSI) is important to many natural language processing tasks because word sense ambiguity is pervasive in linguistic expressions The majority of existing WSI algorithms are not applicable to capture both lexical semantics and syntactic relations without involving excessive task-specific feature engineering Moreover it remains a challenge to explore a sense clustering method which is capable of determining the number of word senses for the polysemous words automatically and properly In this paper we learn continuous semantic space representations for the ambiguous instances via recursive context composition allowing us to capture lexical semantics and syntactic relations simultaneously Using the learned representations of ambiguous instances we further adapt rival penalization competitive learning to conduct instances based word sense clustering allowing us to determine the number of word senses automatically We validate the effectiveness of our method on the SEMEVAL-2010 WSI dataset Experiment results show that our method is able to improve the quality of word sense clustering over several competitive baselines (C) 2015 Elsevier BV All rights reserved',NLP
'Sentiment classification has received much attention in both engineering and academic fields Deep belief networks (DBN) has proved powerful in many domains including natural language processing In this paper DBN is applied in sentiment classification while we propose a new way to improve the DBN based on the unsupervised training phase of restricted Boltzmann machines (RBM) That is the RBM generates the hidden layer in an unsupervised fashion and then we use this hidden layer as the output of a single-layer neural network which is trained using the delta rule The new weights trained from delta rule are then transmitted into the whole back propagation This way keeps much more correction signal information for each layer in back propagation compared to that in the same network structure Consequently our experimental results demonstrate that the new learning method performs relatively better on ten sentiment datasets which further proves the delta rule improves DBN performance for natural language processing tasks',NLP
'Natural language processing for biomedical text currently focuses mostly on entity and relation extraction These entities and relations are usually pre-specified entities eg proteins and pre-specified relations eg inhibit relations A shallow parser that captures the relations between noun phrases automatically from free text has been developed and evaluated It uses heuristics and a noun phraser to capture entities of interest in the text Cascaded finite state automata structure the relations between individual entities The automata are based on closed-class English words and model generic relations not limited to specific words The parser also recognizes coordinating conjunctions and captures negation in text a feature usually ignored by others Three cancer researchers evaluated 330 relations extracted from 26 abstracts of interest to them There were 296 relations correctly extracted from the abstracts resulting in 90% precision of the relations and an average of 11 correct relations per abstract (C) 2003 Elsevier Inc All rights reserved',NLP
'Iliad and Odyssey are products of a collective effort involving numerous authors each contributing unknown portions of text and it still cannot be determined whether a single individual (or distinct group of poets) contributed larger chunks of such additional verses or even whole Books In this work we employed character-level statistical language modeling to analyse the computational authorship of Homeric text and study the linguistic proximity and divergence between the books of Iliad and Odyssey We show that some pairs of books are much closer than others and that some books are linguistically far from the rest Furthermore we investigated the linguistic association between the Homeric poems and four Homeric hymns showing that To Aphrodite is linguistically close and that To Hermes is linguistically far from both Iliad and Odyssey In a final experiment we show that statistical language models can be used to classify excerpts between Iliad and Odyssey similarly to the average human expert',NLP
'Searching the web in Sinhala language does not provide satisfactory results and hence Sri Lankans who are not fluent in English find it difficult to browse the web for knowledge This issue can be solved by Cross Language Information Retrieval (CLIR) where the query in Sinhala is matched with documents in English using a query translation approach This study has experimented with different models which uses the concept of word embeddings to transform the Sinhala query to English where results were retrieved using the Google Search API by providing the equivalent English query obtained The retrieved results were translated back to Sinhala and re-ranked using two different approaches A user evaluation showed that re-ranking the results did not show a positive impact but obtaining results using the equivalent English query proved to be effective Hence this study shows that the quality of the results obtained when searching the web in Sinhala can be improved by performing CLIR',NLP
'Quantifying the uncertainty of supervised learning models plays an important role in making more reliable predictions Epistemic uncertainty which usually is due to insufficient knowledge about the model can be reduced by collecting more data or refining the learning models Over the last few years scholars have proposed many epistemic uncertainty handling techniques which can be roughly grouped into two categories ie Bayesian and ensemble This paper provides a comprehensive review of epistemic uncertainty learning techniques in supervised learning over the last five years As such we first decompose the epistemic uncertainty into bias and variance terms Then a hierarchical categorization of epistemic uncertainty learning techniques along with their representative models is introduced In addition several applications such as computer vision (CV) and natural language processing (NLP) are presented followed by a discussion on research gaps and possible future research directions(c) 2021 Elsevier BV All rights reserved',NLP
'Mental illness is highly prevalent nowadays constituting a major cause of distress in peoples life with impact on societys health and well-being Mental illness is a complex multi-factorial disease associated with individual risk factors and a variety of socioeconomic clinical associations In order to capture these complex associations expressed in a wide variety of textual data including social media posts interviews and clinical notes natural language processing (NLP) methods demonstrate promising improvements to empower proactive mental healthcare and assist early diagnosis We provide a narrative review of mental illness detection using NLP in the past decade to understand methods trends challenges and future directions A total of 399 studies from 10467 records were included The review reveals that there is an upward trend in mental illness detection NLP research Deep learning methods receive more attention and perform better than traditional machine learning methods We also provide some recommendations for future studies including the development of novel detection methods deep learning paradigms and interpretable models',NLP
'The advancement of natural language processing and text mining techniques facilitate automatic non-trivial pattern extraction and knowledge discovery from text data However text-based research has received less attention compared to image- and sensor-based research in the construction industry Hence this paper performs a comprehensive review to understand the current state and future insights of text analytics focusing on the data source and analysis method This study identifies various kinds of text data sources from project documents as well as open data in the websites In addition the review finds that the ontology- and rule-based approach has been dominant at the same time recent research has attempted to apply the state-of-the-art machine learning methods It is envisioned that there are potential advancements in construction engineering and management based on the latest text analysis methods along with the enriched data by the digital transformation',NLP
'the article represents the use of various methods of text analysis in natural language processing tasks and the extraction of key information from it In addition various approaches to image generation using machine learning technologies are discussed as well as the use of various methods for subsequent image styling The research aims are to develop an algorithm and technology for generating the cover of a printed book based on the analysis of the text of the work The research is divided into several main stages The first stage is searching for the necessary information The next step was to use text analysis methods to extract the main information from it Next generating images using machine learning technologies or parsing images for further placement on the cover In the end images are edited and stylized Different approaches were applied for each of the stages and the results were used to select the most appropriate one',NLP
'Online shopping has gained popularity for its omnipresence However visually impaired people are not able to make complete use of this e-commerce shopping due to lack of user-friendly nature to the visually impaired Here in this paper we have proposed a solution to make the e-commerce websites more user-friendly to the visually impaired using voice-based assistance Our solution includes Face Recognition technology using OpenCV for login and registration into the e-commerce website gTTS (Google Text to Speech) and speech_recognition libraries were used for making it completely speech driven After the search results to extract the data from the web page Web Scraping was used and the results were stored in the database to analyse the data and to choose the best-rated products After selection of the product the product was added to the cart using Selenium Web Driver',NLP
'Deep learning rapidly promotes many fields with successful stories in natural language processing An architecture of deep neural network (DNN) combining tree-structured long short-term memory (Tree-LSTM) network and back-propagation neural network (BPNN) is developed for predicting physical properties Inspired by the natural language processing in artificial intelligence we first developed a strategy for data preparation including encoding molecules with canonical molecular signatures and vectorizing bond-substrings by an embedding algorithm Then the dynamic neural network named Tree-LSTM is employed to depict molecular tree data-structures while the BPNN is used to correlate properties To evaluate the performance of proposed DNN the critical properties of nearly 1800 compounds are employed for training and testing the DNN models As compared with classical group contribution methods it can be demonstrated that the learned DNN models are able to provide more accurate prediction and cover more diverse molecular structures without considering frequencies of substructures',NLP
'Protein subcellular localization is an important topic in proteomics since it is related to a proteins overall function helps in the understanding of metabolic pathways and in drug design and discovery In this paper a basic approximation technique from natural language processing called the linear interpolation smoothing model is applied for predicting protein subcellular localizations The proposed approach extracts features from syntactical information in protein sequences to build probabilistic profiles using dependency models which are used in linear interpolation to determine how likely is a sequence to belong to a particular subcellular location This technique builds a statistical model based on maximum likelihood It is able to deal effectively with high dimensionality that hinders other traditional classifiers such as Support Vector Machines or k-Nearest Neighbours without sacrificing performance This approach has been evaluated by predicting subcellular localizations of Gram positive and Gram negative bacterial proteins (C) 2015 Elsevier Ltd All rights reserved',NLP
'This paper presents a review of state-of-the-art approaches to automatic extraction of biomolecular events from scientific texts Events involving biomolecules such as genes transcription factors or enzymes for example have a central role in biological processes and functions and provide valuable information for describing physiological and pathogenesis mechanisms Event extraction from biomedical literature has a broad range of applications including support for information retrieval knowledge summarization and information extraction and discovery However automatic event extraction is a challenging task due to the ambiguity and diversity of natural language and higher-level linguistic phenomena such as speculations and negations which occur in biological texts and can lead to misunderstanding or incorrect interpretation Many strategies have been proposed in the last decade originating from different research areas such as natural language processing machine learning and statistics This review summarizes the most representative approaches in biomolecular event extraction and presents an analysis of the current state of the art and of commonly used methods features and tools Finally current research trends and future perspectives are also discussed',NLP
'Engineering avionics software is a complex task Even more so due to their safety-critical nature Aviation authorities require avionics software suppliers to provide appropriate evidence of achieving DO-178C objectives for the verification of outputs from the requirements and design processes and requirements-based testing This concern is leading suppliers to consider and incorporate more effective engineering methods that can support them in their verification and certification endeavours This paper presents SpecML a modelling language providing a requirements specification infrastructure for avionics software The goal of SpecML is threefold: 1) enforce certification information mandated by DO-178C 2) capture requirements in natural language to encourage adoption in industry and 3) capture requirements in a structured semantically-rich formalism to enable requirements-based analyses and testing The modelling language has been developed as a UML profile extending SysML Requirements A reference implementation has been developed and experiences on its application to an openly-available avionics software specification are reported',NLP
'We present statistical models for morphological disambiguation in agglutinative languages with a specific application to Turkish Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology We propose to handle this by breaking up the morhosyntactic tags into inflectional groups each of which contains the inflectional features for each (intermediate) derived form Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models Among the four models that we have developed and tested the simplest model ignoring the local morphotactics within words performs the best Our best trigram model performs with 9395% accuracy on our test data getting all the morhosyntactic and semantic features correct If we are just interested in syntactically relevant features and ignore a very small set of semantic features then the accuracy increases to 9507%',NLP
'There is increasing evidence that better health outcomes for patients can be achieved with improvement in mental health Loneliness one such condition is itself toxic consequently driving increase in both chronic disease morbidity and mortality Thus the early identification of loneliness and interventions to address it are of urgent importance One potential mechanism for identifying loneliness rests in analyzing care provider notes which include details regarding a providers interaction with patients and often provide insights about both mental and physical health To automatically determine which patients are suffering from loneliness a data science analysis based on natural language processing techniques was performed on clinical notes from 12 care providers for 128 patients The analysis techniques included co-occurrence of uni-gram bi-gram tri-gram and quad-gram words; sentiment analysis using AFINN sentiment lexicon scores; and word usage frequencies The results surfaced key challenges associated with determining the presence of loneliness suggested the importance of including validated clinical questionnaires specifically designed for identifying loneliness',NLP
'In the knowledge base of function word usage of trinity the auxiliary word DE has the characteristics of high frequency and flexible usage In this paper a neural network model (TG network) is proposed to automatically recognize the usage of DE In this network the self-attention mechanism is firstly adopted as the first-layer feature encoder and GRU (gated recurrent unit) as the second-layer semantic extractor and the recognition accuracy rate reaches 828% Experiments show that the recognition effect of TG network is better than that of previous methods In further experiments the larger the window the better the effect of the model is proved by setting different windows At the same time the fine-grained analysis of each usage category is carried out In the future it is expected that this model will automatically recognize more function words and the recognition results can be applied to other natural language processing tasks',NLP
'Source code summarization - creating natural language descriptions of source code behavior - is a rapidly-growing research topic with applications to automatic documentation generation program comprehension and software maintenance Traditional techniques relied on heuristics and templates built manually by human experts Recently data-driven approaches based on neural machine translation have largely overtaken template-based systems But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names the models fail to create good summaries In this paper we present a neural model that combines words from code with code structure from an AST Unlike previous approaches our model processes each data source as a separate input which allows the model to learn code structure independent of the text in code This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided We evaluate our technique with a dataset we created from 21m Java methods We find improvement over two baseline techniques from SE literature and one from NLP literature',NLP
'This paper presents a hybrid approach to an Automated Essay Grading System (AEGS) that provides automated grading and evaluation of student essays The proposed system has two complementary components: Writing Features Analysis tools whieh rely on natural language processing (NLP) techniques and neural network grading engine which rely on a set of pre-graded essays to judge the student answer and assign a grade By this way students essays could be evaluated with a feedback that would improve their writing skills The proposed system is evaluated using datasets from computer and information sciences college students essays in Mansoura University These datasets was written as part of mid-term exams in introduction to information systems course and Systems analysis and design course The obtained results shows an agreement with teachers grades in between 70% and nearly 90% with teachers grades This indicates that the proposed might be useful as a tool for automatie assessment of students essays thus leading to a considerable reduction in essay grading costs',NLP
'Automated phenotype identification plays a critical role in cohort selection and bioinformatics data mining Natural Language Processing (NLP)-informed classification techniques can robustly identify phenotypes in unstructured medical notes In this paper we systematically assess the effect of naive lexically normalized and semantic feature spaces on classifier performance for obesity atherosclerotic cardiovascular disease (CAD) hyperlipidemia hypertension and diabetes We train support vector machines (SVMs) using individual feature spaces as well as combinations of these feature spaces on two small training corpora (730 and 790 documents) and a combined (1520 documents) training corpus We assess the importance of feature spaces and training data size on SVM model performance We show that inclusion of semantically-informed features does not statistically improve performance for these models The addition of training data has weak effects of mixed statistical significance across disease classes suggesting larger corpora are not necessary to achieve relatively high performance with these models (C) 2015 Elsevier Inc All rights reserved',NLP
'Objectives This review examines work on automated summarization of electronic health record (EHR) data and in particular individual patient record summarization We organize the published research and highlight methodological challenges in the area of EHR summarization implementation Target audience The target audience for this review includes researchers designers and informaticians who are concerned about the problem of information overload in the clinical setting as well as both users and developers of clinical summarization systems Scope Automated summarization has been a long-studied subject in the fields of natural language processing and human-computer interaction but the translation of summarization and visualization methods to the complexity of the clinical workflow is slow moving We assess work in aggregating and visualizing patient information with a particular focus on methods for detecting and removing redundancy describing temporality determining salience accounting for missing data and taking advantage of encoded clinical knowledge We identify and discuss open challenges critical to the implementation and use of robust EHR summarization systems',NLP
'We put Object-Role Modeling (ORM) to work in the context of the creation of System Dynamics (SD) models SD focuses on the structure and behavior of systems composed of interacting feedback loops The art of SD modeling lies in discovering and representing the feedback processes and other elements that determine the dynamics of the system (typically a process in an organization) However SD shows a lack of instruments for discovering and expressing precise language-based concepts in domains At the same time the field of conceptual modeling has long since focused on deriving models from natural expressions We therefore turn to ORM as a prime example of this school of thought to integrate its strong natural language based modeling approach into the creation of SD models A two-step schema based approach for transforming an ORM domain model into a SD stock and flow diagram is presented We discuss how typical ORM conceptualization can be linked to SD conceptualization and how such a transformation can be performed Examples are provided',NLP
'In this paper we give an overview for the shared task at the CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2017): Chinese News Headline Categorization The dataset of this shared task consists 18 classes 12000 short texts along with corresponded labels for each class The dataset and example code can be accessed at https://githubcom/FudanNLP/n1pcc2017_news_ headline_categorization',NLP
'We present a system that uses semantic methods and natural language processing capabilites in order to provide comprehensive and easy-to-use access to tourist information in the WWW Thereby the system is designed such that as background knowledge and linguistic coverage increase the benefits of the system improve while it guarantees state-of-the-art information and database retrieval capabilities as its bottom line',NLP
'Methods relying on diagnostic codes to identify suicidal ideation and suicide attempt in Electronic Health Records (EHRs) at scale are suboptimal because suicide-related outcomes are heavily under-coded We propose to improve the ascertainment of suicidal outcomes using natural language processing (NLP) We developed information retrieval methodologies to search over 200 million notes from the Vanderbilt EHR Suicide query terms were extracted using word2vec A weakly supervised approach was designed to label cases of suicidal outcomes The NLP validation of the top 200 retrieved patients showed high performance for suicidal ideation (area under the receiver operator curve [AUROC]: 986 95% confidence interval [CI] 971-995) and suicide attempt (AUROC: 973 95% CI 952-987) Case extraction produced the best performance when combining NLP and diagnostic codes and when accounting for negated suicide expressions in notes Overall we demonstrated that scalable and accurate NLP methods can be developed to identify suicidal behavior in EHRs to enhance prevention efforts predictive models and precision medicine',NLP
'Background The accurate prediction of biological features from genomic data is paramount for precision medicine and sustainable agriculture For decades neural network models have been widely popular in fields like computer vision astrophysics and targeted marketing given their prediction accuracy and their robust performance under big data settings Yet neural network models have not made a successful transition into the medical and biological world due to the ubiquitous characteristics of biological data such as modest sample sizes sparsity and extreme heterogeneity Results Here we investigate the robustness generalization potential and prediction accuracy of widely used convolutional neural network and natural language processing models with a variety of heterogeneous genomic datasets Mainly recurrent neural network models outperform convolutional neural network models in terms of prediction accuracy overfitting and transferability across the datasets under study Conclusions While the perspective of a robust out-of-the-box neural network model is out of reach we identify certain model characteristics that translate well across datasets and could serve as a baseline model for translational researchers',NLP
'Many valuable insights and enormous amount of knowledge are buried in the unstructured data of biomedical literature With the help of technological progression organizations and drug manufacturers have invested in extracting major findings and insights from the vast amount of data Extracting the drug names diseases genes is now easy due to technology innovation in Text Mining and Natural Language Processing Entity recognition and relations extraction have helped numerous pharmacists reduce the amount of time in novel drugs production In this article the innovative text-mining tools were used to identify named entities and automatically extract thousands of disease-gene relations from 3000 publications from PubMed medical literature specified in Breast Cancer Firstly the bio-entities in the dataset were recognized by ScispaCy After that the relations between the extracted disease-gene entities were identified by the NeMo toolkit The trained model provided a decent accuracy (F1 score: 8003) in the drug-disease associations which plays a significant role in developing novel drugs for incurable diseases',NLP
'Deep learning models have showed great capabilities in data modelling on natural language processing various applications including sentiment analysis part-of-speech tagging machine translation and many others In particular convolutional neural network (CNNs) and long-term short memory (LSTM) have proved to be effective in capturing long-term dependencies in sequential data that result in state-of-the-art performance in comparison to traditional machine learning algorithms This research paper therefore structures an enhanced model of both CNNs and LSTM for the feature resourcefulness of Arabic text data on freely available benchmark datasets with word2vec representation model for each corpus The model is projected for Arabic sentiment analysis (ASA) in highlight The proposed architecture has achieved better performance on three datasets out of five in comparison to previous studies In research conduct the model achieved a total accuracy of 0881 for Main-AHS 0968 for Sub-AHS 0842 for Ar-Twitter 07918 for ASTD 0903 for OCLAR',NLP
'Current question answering systems are usually based on a knowledge base which is populated with domain specific knowledge and managed through Unstructured Information Management Architecture (UIMA) But drawback in this approach is that knowledgebase may be grown with knowledge which is not relevant to the users connected with the system In order to address this drawback we propose unsupervised knowledge accumulation algorithm which can monitor user preferences and acquire knowledge without any supervision of the system management unit Basically this algorithm learns domain of interest of each and every user connected with the system and extract knowledge from the web or from a given corpus We have also adopted several Natural Language Processing algorithms to design this high-level algorithm Knowledge modelling is done through a conceptual graph based knowledge base This novel paradigm is evaluated with the help of several connected users and with more than 280 questions We have achieved excellent accuracy during the evaluation phase It shows our novel approach is effective and can be used to address the drawback decently',NLP
'Verbal communication is a joint activity; however speech production and comprehension have primarily been analyzed as independent processes within the boundaries of individual brains Here we applied fMRI to record brain activity from both speakers and listeners during natural verbal communication We used the speakers spatiotemporal brain activity to model listeners brain activity and found that the speakers activity is spatially and temporally coupled with the listeners activity This coupling vanishes when participants fail to communicate Moreover though on average the listeners brain activity mirrors the speakers activity with a delay we also find areas that exhibit predictive anticipatory responses We connected the extent of neural coupling to a quantitative measure of story comprehension and find that the greater the anticipatory speaker-listener coupling the greater the understanding We argue that the observed alignment of production- and comprehension-based processes serves as a mechanism by which brains convey information',NLP
'Sentence similarity is useful in many Natural Language Processing tasks such as plagiarism checking and paraphrasing So far only conventional unsupervised sentence similarity measurement techniques (knowledge-based corpus-based string similarity-based and hybrid) have been used to measure sentence similarity for Tamil and Sinhala languages In this paper we present a Deep Learning technique to measure sentence similarity for these two languages which makes use of a Siamese Neural Network that consists of two Long Short-Term Memory (LSTM) networks and neural word embeddings as the input representation This approach achieved a 307% higher Pearson correlation coefficient for the dataset of 2500 Tamil sentence pairs and a 361% higher Pearson correlation for the dataset of 5000 Sinhala sentence pairs over the conventional unsupervised sentence similarity measurement techniques',NLP
'An approach for mining repositories of web-based user documentation for patterns of evolutionary change in the context of internationalization and localization is presented Sets of documents that are changed together during the translation process are uncovered and documented to support future evolution of the system A sequential-pattern mining technique is used to uncover the patterns from Subversion repositories The approach is applied to the open source KDE system KDE maintains documentation for over fifty different natural languages and presents a prime example of the problem Characteristics of the uncovered patterns such as size frequency and occurrences within a single language or across multiple languages are discussed Such patterns help provide insight as to the effort required in retranslation due to a change in the documentation and help user communities estimated the progress of documentation in their respective languages',NLP
'This paper defines a learning algorithm for plan grammars used for plan recognition The algorithm learns Combinatory Categorial Grammars (CCGs) that capture the structure of plans from a set of successful plan execution traces paired with the goal of the actions This work is motivated by past work on CCG learning algorithms for natural language processing and is evaluated on five well know planning domains',NLP
'The focus of traditional workflow management systems is on control flow within one process definition The process definition describes how a single case (ie workflow instance) in isolation is handled For many applications this paradigm is inadequate Interaction between cases to support communication and collaboration is at least as important This paper introduces and advocates the use of interacting proclets ie lightweight workflow processes By promoting interactions to first-class citizens it is possible to model complex workflows in a more natural mariner In addition the expressive power and flexibility are improved compared to the more traditional workflow modeling languages',NLP
'Recently the RVC-CAL dataflow language has enabled video codecs to be specified in a more natural way than imperative languages by allowing implicit expression of parallelism and side effect freeness The tools developed for RVC-CAL have also enabled the automatic generation of parallel C code among others from dataflow specifications This paper introduces a new approach allowing the integration of dataflow components within legacy code The approach makes use of a generic interface definition that allows seamless interaction between I/O components which are mostly state operations and are best implemented in imperative languages with data processing components which are mostly stateless dataflow operations and are best implemented in dataflow languages The advantage of the approach is the ease of development by allowing each language to be used on those parts of the application that it is most appropriate for The functionality of the approach is demonstrated by using the generic interface to add a new dataflow based MPEG and HEVC decoder into the legacy video transcoding library FFmpeg',NLP
'Arabic is not just one language but rather a collection of dialects in addition to Modern Standard Arabic (MSA) While MSA is used in formal situations dialects are the language of every day life Until recently there was very little dialectal Arabic in written form With the advent of social-media however the landscape has changed We provide the first romanized code-switched Algerian Arabic-French corpus annotated for word-level language id We review the history and sociological factors that make the linguistic situation in Algerian unique and highlight the value of this corpus to the natural language processing and linguistics communities To build this corpus we crawled an Algerian newspaper and extracted the comments from the news story We discuss the informal nature of the language in the corpus and the challenges it will present Additionally we provide a preliminary analysis of the corpus We then discuss some potential uses of our corpus of interest to the computational linguistics community',NLP
'Objective: This article summarizes our approach to extracting medication and corresponding attributes from clinical notes which is the focus of track 1 of the 2022 National Natural Language Processing (NLP) Clinical Challenges(n2c2) shared task Methods: The dataset was prepared using Contextualized Medication Event Dataset (CMED) including 500 notes from 296 patients Our system consisted of three components: medication named entity recognition (NER) event classification (EC) and context classification (CC) These three components were built using transformer models with slightly different architecture and input text engineering A zero-shot learning solution for CC was also explored Results: Our best performance systems achieved micro-average F1 scores of 0973 0911 and 0909 for the NER EC and CC respectively Conclusion: In this study we implemented a deep learning-based NLP system and demonstrated that our approach of (1) utilizing special tokens helps our model to distinguish multiple medications mentions in the same context; (2) aggregating multiple events of a single medication into multiple labels improves our models performance',NLP
'Sequence labeling in which a class or label is assigned to each token in a given input order is a fundamental task in natural language processing Many advanced neural network architectures have recently been proposed to solve the sequential labeling problem affecting this task By contrast only a few approaches have been proposed to address the sequential ensemble problem In this paper we resolve the sequential ensemble problem by applying the sequential alignment method in a proposed ensemble framework Specifically we propose a simple but efficient ensemble candidate generation framework with which multiple heterogeneous systems can easily be prepared from a single neural sequence labeling network To evaluate the proposed framework experiments were conducted with part-of-speech (POS) tagging and dependency label prediction problems The results indicate that the proposed framework achieved accuracy values that were higher by 019 and 033 than those achieved by the hard-voting method on the Penn-treebank POS-tagged and Universal dependency-tagged datasets respectively',NLP
'Classroom Assistant is a publicly distributable Slack application that intelligently answers student questions in place of an instructor The application creates a personalized knowledge base per Slack workspace and learns based on information and responses provided by an instructor This provides an automated solution enabling instructors to only answer a question once Classroom Assistant utilizes Dialogflow a Natural Language Processing (NLP) service on Google Cloud Platform (GCP) to detect question intent and store workspace-specific knowledge Dialogflow provides a basic solution for integrating its service with Slack however it is not scalable nor flexible as any changes to the knowledge base have to go through the Dialogflow web interface on GCP We address this limitation by creating a custom integration that is highly scalable and also enables the public distribution of the app Via a single click installation our Classroom Assistant allows instructors to deploy the chatbot app to different Slack workspaces and auto-update the knowledge base via a direct answer to students questions inside Slack',NLP
'Dementia is a disease characterized by cognitive impairment that leads to incoherent or illogical thoughts and speech There are attempts to identify dementia through speech analyses but there is a dearth of research on casual conversation analysis This work examined communication impairment detection of people with early-stage memory loss including mild dementia and mild cognitive impairment The data sets included semi-structured interviews from two studies conducted at the University ofWashington (UW) the DementiaBanks Pitt Corpus and the ADReSS Challenge at INTERSPEECH 2020 We applied Transformer-based deep learning models to automatically extract linguistic features for identifying individuals with dementia Our results showed the models abilities on detecting linguistic deficits with the best mean F1-score of 76% on the Pitt Corpus 84% on the ADReSS 90% on the augmented ADReSS and 74% on the UW transcripts The results suggest the potential possibility of a more flexible examination setting casual semi-structured individual or group interview for detecting incoherent or illogical thoughts and speech in patients with dementia',NLP
'In the natural language processing (NLP) field it is fairly common that an entity is nested in another entity Most existing named entity recognition (NER) models focus on flat entities but ignore nested entities In this paper we propose a neural model for nested named entity recognition Our model employs a multi-label boundary detection module to detect entity boundaries avoiding boundary detection conflict existing in the boundary-aware model Besides our model with a boundary detection module and a category detection module detects entity boundaries and entity categories simultaneously avoiding the error propagation problem existing in current pipeline models Furthermore we introduce multitask learning to train the boundary detection module and the category detection module to capture the underlying association between entity boundary information and entity category information In this way our model achieves better performance of entity extraction In evaluations on two nested NER datasets and a flat NER dataset we show that our model outperforms previous state-of-the-art models on nested and flat NER',NLP
'As Android platform is protected by permissions which is one of the most powerful access control models of Android it is possible to restrict the use of certain sensitive APIs In contrast many applications declare permissions more than they need requiring API permission specification to detect these cases However prominent previous research has not been focused on java documents or comment from Android developers despite a lot of information To address this problem we propose a novel method to analyze naturally written comments from Android developers for API permission map construction We extract all comments and java documents from raw Android source code and extract permission information using natural language processing techniques At the same time we parse naturally written source code and extract API signature to perform the mapping between permission Moreover we categorize all permissions and APIs according to their behavior explained in the comment to measure the potential risk level resulted from misuse Our experiment on Android 10 which is the latest version mapped 3012 APIs with permission and categorized semantically with seven different categories',NLP
'This is a full paper in the Innovate Practice category examining a Doctor of Professional Studies (DPS) in Computing program at Pace University which is a specialized degree program designed for active Information Technology (IT) professionals with at least five years of full time experience in the computing field The first question that guided our research is how is the Pace University DPS program related to other doctoral computing programs in the United States and globally The second question that guided our inquiry is dissertation topics pursued by graduates in dissertation research specifically by IT professionals in the Pace University program To answer this second question we analyzed the first 114 dissertation abstracts that have been defended at Pace University in the Seidenberg School of Computer Science and Information Systems We used machine learning and natural language processing to determine commonalities among research topics in order to gain an understanding of the topic categories and topic spread produced by the program',NLP
'General criteria of preventive healthcare based on the preventive care guidelines have been integrated with Electronic Health Record (EHR) systems through decision support systems and led to improved performance in healthcare delivery Advanced integration which considers factors such as ethnicity social history medical history family history need to be investigated Integrating the preventive healthcare guidelines with the EHR based on above factors requires the extraction of the relevant information from these guidelines using text mining and natural language processing techniques In this research we propose a framework to extract information according to the EHR modules Our results show that the proposed framework successfully extracts terms and concepts and adequately maps them to the proposed data interchange structure that is based on the EHR functional modules The extracted information and the populated data interchange structures eases the integration of the modifiable risk factors with the patients records in the EHR The proposed framework can be extended to other clinical healthcare guidelines where modifiable risk factors are critical',NLP
'Greater demand for information has resulted in an increasing number of duties for the corporate library staff With the advent of the library without walls librarians and information specialists are asked to provide a variety of digital library services beyond the traditional intermediated search Searches of high economic value remain the purview of the skilled information specialist who has both subject and information skills to apply to the searches required Preliminary screening searches competitive intelligence searches and searches of lower economic value are now performed by the end-user of the information satisfying his or her own needs These end-users may search infrequently and they usually have fewer formal information skills than their professional information colleagues A new range of databases and search engines with natural language interfaces are available to assist the occasional user perform high-quality searches The processes embedded in these systems provides an ease of use more closely related to the research skills of the end-user The techniques of database preparation query formulation and search execution using the DR-LINK interface will be described',NLP
'Visual Question Answering (VQA) is a task that connects the fields of Computer Vision and Natural Language Processing Taking as input an image I and a natural language question Q about I a VQA model must be able to produce a coherent answer R (also in natural language) to Q A particular type of visual question is one in which the question is binary (ie a question whose answer belongs to the set {yes no}) Currently deep neural networks correspond to the state of the art technique for training of VQA models Despite its success the application of neural networks to the VQA task requires a very large amount of data in order to produce models with adequate precision Datasets currently used for the training of VQA models are the result of laborious manual labeling processes (ie made by humans) This context makes relevant the study of approaches to augment these datasets in order to train more accurate prediction models This paper describes a crowdsourcing tool which can be used in a collaborative manner to augment an existing VQA dataset for binary questions Our tool actively integrates candidate items from an external data source in order to optimize the selection of queries to be presented to curators',NLP
'Introduction: Preventing dementia or modifying disease course requires identification of presymptomatic orminimally symptomatic high-risk individuals Methods: We used longitudinal electronic health records from two large academic medical centers and applied a validated natural language processing tool to estimate cognitive symptomatology We used survival analysis to examine the association of cognitive symptoms with incident dementia diagnosis during up to 8 years of follow-up Results: Among 267855 hospitalized patients with 1251858 patient years of follow-up data 6516 (24%) received a new diagnosis of dementia In competing risk regression an increasing cognitive symptom score was associated with earlier dementia diagnosis (HR 163; 154-172) Similar results were observed in the second hospital system and in subgroup analysis of younger and older patients Discussion: Acognitive symptom measure identified in discharge notes facilitated stratification of risk for dementia up to 8 years before diagnosis',NLP
'The automatic detection of negation is a crucial task in a wide-range of natural language processing (NLP) applications including medical data mining relation extraction question answering and sentiment analysis In this paper we present a syntactic path-based hybrid neural network architecture a novel approach to identify the scope of negation in a sentence Our hybrid architecture has the particularity to capture salient information to determine whether a token is in the scope or not without relying on any human intervention This approach combines a bidirectional long short-term memory (Bi-LSTM) network and a convolutional neural network (CNN) The CNN model captures relevant syntactic features between the token and the cue within the shortest syntactic path in both constituency and dependency parse trees The Bi-LSTM learns the context representation along the sentence in both forward and backward directions We evaluate our model on the Bioscope corpus and get 9082% F-score (7831% PCS) on the abstract sub-corpus outperforming features-dependent approaches',NLP
'A large amount of user-generated content on social media has led to the pursuit of quickly and accurately mining through data and gathering useful insights Text sentiment analysis has become a necessary tool in classifying user opinions within Web generated content Due to the various ways opinions can be conveyed performing text sentiment analysis in specific domains becomes a difficult task With an even greater degree of difficulty added when slang or colloquialisms are used There is a great deal of research into investigating various classifiers in a traditional natural language processing setting each with their own merits and demerits In this paper we present a slang-based dictionary classifier with the objective of determining the sentiment of Instagram comments within the context of fashion or more specifically sports shoes and compare it with the performance of other classifiers such as a Naive Bayes J48 lexicon and random forest The dataset used for the benchmark was created from popular fashion Instagram accounts Overall the random forest classifier yields the best results with an accuracy of 88% precision of 84% and a recall of 88%',NLP
'As one of the most powerful neural networks Long Short-Term Memory (LSTM) is widely used in natural language processing (NLP) tasks Meanwhile the BiLSTM-CRF model is one of the most popular models for named entity recognition (NER) and many state-of-the-art models for NER are based on it In this paper we propose a new residual BiLSTM model and perform it with a conditional random field (CRF) layer together on NER tasks Based on the most popular BiLSTM-CRF model we replace the BiLSTM with our residual BiLSTM blocks to encode words or characters We evaluate our model on Chinese and English datasets We utilize both word2vec and BERT to generate word or character vectors Furthermore we conduct experiments to compare the performance of NER by using different structures of residual blocks The experimental results show that our model can improve the performance of both Chinese and English NER effectively without introducing any external knowledge',NLP
'Teachers tend to set the free-text questions for testing the comprehensive ability of students That leads to the increasing attention to the intelligent auto-grading system for easing the grading load on examiners In this paper we present a novel automatic essay scoring system based on Natural Language Processing and Deep Learning technologies In particular the proposed system encodes an essay as sequential embeddings and harnesses a bi-directional LSTM to catch the semantic information Meanwhile the system constructs the attention for each essay so that the network can learn to focus on the valid information correctly in an article which can also provide the reasonable evidence of the predictive result The dataset for training and testing is the public essay set available in the Automated Student Assessment Prize on Kaggle The study shows that our system achieves state-of-the-art performance in grade prediction and more importantly our intelligent auto-grading system can focus on the critical words and sentences analyze the logical semantic relationship of the context and predict the interpretable grades',NLP
'The Social Media Platforms as the one of largest part of today data traffic on the Internet disseminate a vast volume of information including medical information in it Knowledge management system (KMS) approach is applied with a purpose to capture maintain and manage tacit or explicit knowledge available and collected within the social media platforms organizations database knowledge base or document repository By adding Indonesian Natural Language Processing (InaNLP) and Data Mining approach our research proposed a model which is theoretically designed to improve the previous research related to social media knowledge capture model and enhance its accuracy and reliability of knowledge retrieved compared to previous knowledge capture model Despite the proposed framework is still a theoretical model it can be applied in medical sector of Indonesia as a big picture of medical knowledge capture model This model mainly aimed for medical practitioner to give a quick suggestion of the diseases regarding to the early diagnose which has been taken in the first place',NLP
'The Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records focused on the identification of medications their dosages modes (routes) of administration frequencies durations and reasons for administration in discharge summaries This challenge is referred to as the medication challenge For the medication challenge i2b2 released detailed annotation guidelines along with a set of annotated discharge summaries Twenty teams representing 23 organizations and nine countries participated in the medication challenge The teams produced rule-based machine learning and hybrid systems targeted to the task Although rule-based systems dominated the top 10 the best performing system was a hybrid Of all medication-related fields durations and reasons were the most difficult for all systems to detect While medications themselves were identified with better than 075 F-measure by all of the top 10 systems the best F-measure for durations and reasons were 0525 and 0459 respectively State-of-the-art natural language processing systems go a long way toward extracting medication names dosages modes and frequencies However they are limited in recognizing duration and reason fields and would benefit from future research',NLP
'The processing of strings which are semantically distinct but can be easily confused with each other often on account of being pronounced identically is a prime example of context dependency in Natural Language Processing This problem arises when a system needs to distinguish whether a bank is a river bank or a financial institution and it also challenges systems for context-sensitive spelling and grammar correction because pairs like their/there and I/me are one common source of issues that such systems must address In practice this type of context-dependency can be especially prominent in languages with rich morphology where large paradigms of inflected word forms lead to a proliferation of such confusion sets In this paper we present our novel confusion set corpus for Icelandic as well as our findings from an experiment that uses well-known classification algorithms to disambiguate confusion sets that appear in our corpus',NLP
'Research and development on Intelligent humanoid robot for general purpose focuses on speech recognition systems to be able to interact with people naturally In this research we would like to propose humanoid robot with the self-learning capability based on Indonesian language and small vocabularies for accepting and giving response from people based on Natural Language Processing (NLP) This kind of robot can be used widely in schools universities and public services The humanoid robot should consider the style of questions and conclude the answer through conversation under unreliable automatic speech in noisy environment In our scenario robot will detect users face and accept commands from the user to do an action where the answer from user will be processed to the Google Translator and the result will be compared with vocabularies defined on the system We describe how this formulation establish a promising framework by empirical results using subjects randomly The comparative experiments with samples from user were presented',NLP
'We study the problem of computing the probability that a given stochastic context-free grammar (SCFG) G generates a string in a given regular language L(D) (given by a DFA D) This basic problem has a number of applications in statistical natural language processing and it is also a key necessary step towards quantitative omega-regular model checking of stochastic context-free processes (equivalently 1-exit recursive Markov chains or stateless probabilistic pushdown processes) We show that the probability that G generates a string in L(D) can be computed to within arbitrary desired precision in polynomial time (in the standard Turing model of computation) under a rather mild assumption about the SCFG G and with no extra assumption about D We show that this assumption is satisfied for SCFGs whose rule probabilities are learned via the well-known inside-outside (EM) algorithm for maximum-likelihood estimation (a standard method for constructing SCFGs in statistical NLP and biological sequence analysis) Thus for these SCFGs the algorithm always runs in P-time',NLP
