Abstract,class
"We present a novel system for automatic identification of vehicles as part of an intelligent access control system for a garage entrance. Using a camera in the door cars are detected and matched to the database of authenticated cars. Once a car is detected License Plate Recognition (LPR) is applied using character detection and recognition. The found license plate number is matched with the database of authenticated plates. If the car is allowed access the door will open automatically. The recognition of both cars and characters (LPR) is performed using state-of-the-art shape descriptors and a linear classifier. Experiments have revealed that 90% of all cars are correctly authenticated from a single image only. Analysis of the computational complexity shows that an embedded implementation allows user authentication within approximately 300ms which is well within the application constraints.",CV
"The goal of the First International Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications is to bring together practitioners and researchers in computer vision and in HCI to share ideas and experiences in designing and implementing visual interfaces for ground truth data generation. It specifically presents and reports on the construction and analysis of user-oriented tools and interfaces to support automatic or semi-automatic ground truth annotation and labeling in many applications such as object detection object recognition scene segmentation and face recognition both in still images and in videos.",CV
"Several studies have shown that cyclists can reduce the risk of severe head injuries by wearing a helmet. A system is proposed to collect cyclist helmet usage data automatically from video footage. Computer vision techniques are used to track the moving objects and then to analyze the object trajectories and speed profiles to identify cyclists. Image features are extracted from a region around the cyclists head. Support vector machines determine whether the cyclist is wearing a helmet. The system can be approximately 90% accurate in cyclist classification when provided with accurate tracks of the cyclists head. Even for situations in which obtaining video to track a cyclist is challenging the proposed method provides an effective retrieval system potentially reducing the number of video records that must be analyzed manually to find instances of cyclists not wearing helmets.",CV
"In this paper we provide a broad survey of developments in active vision in robotic applications over the last 15 years. With increasing demand for robotic automation research in this area has received much attention. Among the many factors that can be attributed to a high-performance robotic system the planned sensing or acquisition of perceptions on the operating environment is a crucial component. The aim of sensor planning is to determine the pose and settings of vision sensors for undertaking a vision-based task that usually requires obtaining multiple views of the object to be manipulated. Planning for robot vision is a complex problem for an active system due to its sensing uncertainty and environmental uncertainty. This paper describes such problems arising from many applications e. g. object recognition and modeling site reconstruction and inspection surveillance tracking and search as well as robotic manipulation and assembly localization and mapping navigation and exploration. A bundle of solutions and methods have been proposed to solve these problems in the past. They are summarized in this review while enabling readers to easily refer solution methods for practical applications. Representative contributions their evaluations analyses and future research trends are also addressed in an abstract level.",CV
"Though high resolution benefits computer vision performance they are not commonly used in convolutional neural network (CNN)-based vision algorithms due to the limitation of memory and computation resource. Learning in the frequency domain makes high resolution images directly acceptable by CNNs but the computation time and energy overhead for preprocessing including image signal processing (ISP) and domain transformation can be large. This paper explores different image processing and domain transformation operations and proposes an efficient end-to-end frequency domain learning pipeline from RAW images to vision tasks. In particular we simplify the preprocessing part by skipping the entire ISP pipeline and replacing the Discrete Cosine Transform (DCT) with a multiplication-free approximated one. Experimental results show that the final vision performance of the proposed pipeline is very close to that of the conventional pipeline while significant amount of redundant operations can be saved.",CV
"This paper presents an automatic facial analysis system which is able to perform gender detection hair segmentation and geometry detection color attributes extraction (hair skin eyebrows eyes and lips) accessories (eyeglasses) analysis from facial images. For the more complex tasks (gender detection hair segmentation eyeglasses detection) we used state of the art convolutional neural networks and for the other tasks we used classical image processing algorithms based on geometry and appearance models. When data was available the proposed system was evaluated on public datasets. An acceptance study was also performed to assess the performance on the system in real life scenarios.",CV
"In this paper we propose a complete framework namely Mercury that combines Computer Vision and Deep Learning algorithms to continuously monitor the driver during the driving activity. The proposed solution complies to the requirements imposed by the challenging automotive context: the light invariance in order to have a system able to work regardless of the time of day and the weather conditions. Therefore infrared-based images i.e. depth maps (in which each pixel corresponds to the distance between the sensor and that point in the scene) have been exploited in conjunction with traditional intensity images. Second the non-invasivity of the system is required since drivers movements must not be impeded during the driving activity: in this context the use of cameras and vision-based algorithms is one of the best solutions. Finally real-time performance is needed since a monitoring system must immediately react as soon as a situation of potential danger is detected.",CV
"In machine learning training sample set management has an important impact on the performance of visual detection and tracking algorithms as corrupted training samples degrade the tracking performance especially in practical scenarios such as vehicular networks. However how to evaluate and remove the corrupted training samples still remains a challenging topic. In this paper we propose a novel scheme to remove the corrupted training samples in visual tracking which will improve the tracking performance dramatically. In the proposed scheme a novel training sample set management method based on the adaptive sample weight is presented. Specifically similarity learning is first utilized to evaluate the quality of training samples with similarity score. Then if the similarity score is below a certain threshold the training sample is deemed as the corrupted one and is removed from the training sample set. The experimental results show that the proposed scheme obtains superior performances on visual tracking benchmarks and vehicular scenarios.",CV
"Quality inspection of textile products is an important problem for textile manufacturers. This paper presents a computer vision-based system for the quality inspection and control of cotton web in carding machine. The hardware and software platform developed to solve this problem is presented and an improved algorithm for defect detection is proposed. Based on the median filtering contrast enhancing thresholding labeling algorithms the proposed system gives good results in the detection of cotton nep.",CV
"This study presents a review of the state-of-the-art and a novel classification of current vision-based localisation techniques in unknown environments. Indeed because of progresses made in computer vision it is now possible to consider vision-based systems as promising navigation means that can complement traditional navigation sensors like global navigation satellite systems (GNSSs) and inertial navigation systems. This study aims to review techniques employing a camera as a localisation sensor provide a classification of techniques and introduce schemes that exploit the use of video information within a multi-sensor system. In fact a general model is needed to better compare existing techniques in order to decide which approach is appropriate and which are the innovation axes. In addition existing classifications only consider techniques based on vision as a standalone tool and do not consider video as a sensor among others. The focus is addressed to scenarios where no a priori knowledge of the environment is provided. In fact these scenarios are the most challenging since the system has to cope with objects as they appear in the scene without any prior information about their expected position.",CV
"This work presents a comparative study between two different approaches to build an automatic classification system for Modality values in the Portuguese language. One approach uses a single multi-class classifier with the full dataset that includes eleven modal verbs; the other builds different classifiers one for each verb. The performance is measured using precision recall and F-1. Due to the unbalanced nature of the dataset a weighted average approach was calculated for each metric. We use support vector machines as our classifier and experimented with various SVM kernels to find the optimal classifier for the task at hand. We experimented with several different types of feature attributes representing parse tree information and compare these complex feature representation against a simple bag-of-words feature representation as baseline. The best obtained F-1 values are above 0.60 and from the results it is possible to conclude that there is no significant difference between both approaches.",NLP
"In this lighting talk paper we present a dataset of jokes in Russian and deep learning model for solving humor recognition task. The new large dataset was collected from various online resources and complemented carefully with unfunny texts with similar lexical properties. In total there are more than 300000 short texts which is significantly larger than any previous humor-related corpus. Manual annotation of 2000 items proved the reliability of corpus construction approach. Further we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 which constitutes a considerable gain over baseline methods.",NLP
"The extraction of Quranic knowledge is a challenging task since the Quran is different from human literature. The unique arrangement of its texts has made the task of directly accessing its meaning which is rich in its linguistics and multi-layered meanings difficult if done without the use of other resources such as the Hadith. Hence this paper describes the extraction of the first layer and the use of natural language pattern in extracting the knowledge of Quranic English translation texts using natural language processing techniques. Evaluation is performed using the true positive false positive and false negative metrics and encouraging results have been obtained.",NLP
"Topic segmentation is important for many natural language processing applications such as information retrieval text summarization... In our work we are interested in the topic segmentation of textual document. We present a survey of related works particularly C99 and TextTiling. Then we propose an adaptation of these topic segmenters for textual document written in Arabic language named as ArabC99 and ArabTextTiling. For experimental results we construct an Arabic corpus based on newspapers of different Arab countries. Finally we evaluate the performance of these new segmenters by comparing them together and to related works using the metrics WindowDiff and F-measure. (C) 2014 Published by Elsevier B.V.",NLP
"The Unified Medical Language System (UMLS) is an extensive source of biomedical knowledge developed and maintained by the U.S. National Library of Medicine (NLM). The UMLS began to include biomedical terms in other languages a few years ago. However providing foreign terms for existing concepts is only the first step for the UMLS to become international. The current limits of the use of the UMLS in French are analyzed (partial translation unique source of the translated concepts improper character set and absence of lexical resources for lexical matching tools). Some suggestions are given for French to be better integrated into the UMLS especially for adapting the lexical resources to French. Once completed our present work is expected to give the UMLS the capability to be effectively queried in French.",NLP
"Social Network Aggregators are used to maintain and manage manifold accounts over multiple online social networks. Displaying the Activity feed for each social network on a common dashboard has been the status quo of social aggregators for long however retrieving the desired data from various social networks is a major concern. A user inputs the query desiring the specific outcome from the social networks. Since the intention of the query is solely known by user therefore the output of the query may not be as per users expectation unless the system considers user-centric factors. Moreover the quality of solution depends on these user-centric factors the user inclination and the nature of the network as well. Thus there is a need for a system that understands the users intent serving structured objects. Further choosing the best execution and optimal ranking functions is also a high priority concern. The current work finds motivation from the above requirements and thus proposes the design of a query processing system to retrieve information from social network that extracts users intent from various social networks. For further improvements in the research the machine learning techniques are incorporated such as Latent Dirichlet Algorithm (LDA) and Ranking Algorithm to improve the query results and fetch the information using data mining techniques. The proposed framework uniquely contributes a user-centric query retrieval model based on natural language and it is worth mentioning that the proposed framework is efficient when compared on temporal metrics. The proposed Query Processing System to Retrieve Information from Social Network (QPSSN) will increase the discoverability of the user helps the businesses to collaboratively execute promotions determine new networks and people. It is an innovative approach to investigate the new aspects of social network. The proposed model offers a significant breakthrough scoring up to precision and recall respectively.",NLP
"We present statistical models for morphological disambiguation in agglutinative languages with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features then the accuracy increases to 95.07%.",NLP
"Nowadays we see huge amount of information is available on both online and offline sources. For single topic we see hundreds of articles are available containing vast amount of information about it. It is really a difficult task to manually extract the useful information from them. To solve this problem automatic text summarization systems are developed. Text summarization is a process of extracting useful information from large documents and compressing them into short summary preserving all important content. This survey paper hand out a broad overview on the work done in the field of automatic text summarization in different languages using various text summarization approaches. The focal centre of this survey paper is to present the research done on text summarization on Indian languages such as Hindi Punjabi Bengali Malayalam Kannada Tamil Marathi Assamese Konkani Nepali Odia Sanskrit Sindhi Telugu and Gujarati and foreign languages such as Arabic Chinese Greek Persian Turkish Spanish Czeh Rome Urdu Indonesia Bhasha and many more. This paper provides the knowledge and useful support to the beginner scientists in this research area by giving a concise view on various feature extraction methods and classification techniques required for different types of text summarization approaches applied on both Indian and non-Indian languages.",NLP
"Increasing complexity of robotic systems in respect to their abilities and reactions to environmental influences complicate the error and accident analysis. Even for experts the analysis of the vast amount of data gathered during robot operation is difficult due to the usually cryptic and unstructured form of data logs. In this work a system is introduced which analysis logs from a human-robot cooperation system and produces human readable event logs through natural language generation. For generation a template based approach is introduced which allows for adaption of detail granularity in order to facilitate different expert levels of end users. The information used during context-sensitive log generation is the result of different recognition and situation analysis methods. It is shown that the proposed system is capable of analyzing a data stream according to contextual relations and generates appropriate summarizations of occurred events.",NLP
"The multiple schema for the classification of soils rely on differing criteria but the major soil science systems including the United States Department of Agriculture (USDA) and the international harmonized World Reference Base for Soil Resources soil classification systems are primarily based on inferred pedogenesis. Largely these classifications are compiled from individual observations of soil characteristics within soil profiles and the vast majority of this pedologic information is contained in non-quantitative text descriptions. We present initial text mining analyses of parsed text in the digitally available USDA soil taxonomy documentation and the Soil Survey Geographic database. Previous research has shown that latent information structure can be extracted from scientific literature using Natural Language Processing techniques and we show that this latent information can be used to expedite query performance by using syntactic elements and part-of-speech tags as indices. Technical vocabulary often poses a text mining challenge due to the rarity of its diction in the broader context. We introduce an extension to the common English vocabulary that allows for nearly-complete indexing of USDA Soil Series Descriptions.",NLP