{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <small>\n",
    "Copyright (c) 2017-21 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning: A Visual Approach\n",
    "## by Andrew Glassner, https://glassner.com\n",
    "### Order: https://nostarch.com/deep-learning-visual-approach\n",
    "### GitHub: https://github.com/blueberrymusic\n",
    "------\n",
    "\n",
    "### What's in this notebook\n",
    "\n",
    "This notebook is provided as a “behind-the-scenes” look at code used to make some of the figures in this chapter. It is cleaned up a bit from the original code that I hacked together, and is only lightly commented. I wrote the code to be easy to interpret and understand, even for those who are new to Python. I tried never to be clever or even more efficient at the cost of being harder to understand. The code is in Python3, using the versions of libraries as of April 2021. \n",
    "\n",
    "This notebook may contain additional code to create models and images not in the book. That material is included here to demonstrate additional techniques.\n",
    "\n",
    "Note that I've included the output cells in this saved notebook, but Jupyter doesn't save the variables or data that were used to generate them. To recreate any cell's output, evaluate all the cells from the start up to that cell. A convenient way to experiment is to first choose \"Restart & Run All\" from the Kernel menu, so that everything's been defined and is up to date. Then you can experiment using the variables, data, functions, and other stuff defined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 23: Creative Applications - Notebook 2: Loss Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this code:\n",
    "This notebook is adapted from\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb\n",
    "by François Chollet.\n",
    "\n",
    "See License C in LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K_backend\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from skimage.io import imread, imsave\n",
    "import time\n",
    "import os\n",
    "\n",
    "K_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for Keras issues on Mac computers (you can comment this\n",
    "# out if you're not on a Mac, or not having problems)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we will be making many images, and we don't\n",
    "# want to fill up the notebook with one after another,\n",
    "# we use imread and imsave from skimage.io instead \n",
    "# of the file_helper. \n",
    "#\n",
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = False\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)\n",
    "\n",
    "# get the input and output directories\n",
    "input_data_directory = file_helper.get_input_data_dir()\n",
    "output_data_directory = file_helper.get_saved_output_dir()\n",
    "\n",
    "# make sure the output directory exists\n",
    "already_existed = file_helper.check_for_directory(output_data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_size(target_image_path, target_height):\n",
    "    '''Return the size of the images we want to use. We use the same\n",
    "    aspect ratio as the target, but with the given height.'''\n",
    "    width, height = load_img(target_image_path).size\n",
    "    img_width = int(width * target_height / height)\n",
    "    return (img_width, target_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_height, img_width):\n",
    "    '''Process an image using VGG16 conventions'''\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''Undo VGG16 image conventions'''\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # change channel order from'BGR' to 'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VGG16(input_tensor):\n",
    "    '''Get VGG16 model. Leave off the final fully-connected layers.\n",
    "    Set it up with ImageNet weights. Set the input tensor to input_tensor.'''\n",
    "    model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content loss\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    '''The content loss wants the final layer of VGG16 to report\n",
    "    the same conclusions for both the original target and the\n",
    "    synthesized image.'''\n",
    "    return K_backend.sum(K_backend.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style loss\n",
    "\n",
    "def gram_matrix(x):\n",
    "    '''Compute a Gram matrix. Given a matrix F, element (i,j) of the Gram\n",
    "    matrix is the dot product of rows i and j from matrix F.'''\n",
    "    features = K_backend.batch_flatten(K_backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K_backend.dot(features, K_backend.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def style_loss(style, combination, img_height, img_width):\n",
    "    '''Compute the style loss. We want the Gram matrix for the original\n",
    "    target and the synthetic image to be the same.'''\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K_backend.sum(K_backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss\n",
    "\n",
    "def total_variation_loss(x, img_height, img_width):\n",
    "    '''Compute the total variation loss. This prevents neighboring pixels from\n",
    "    becoming too different from one another.'''\n",
    "    a = K_backend.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n",
    "    b = K_backend.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n",
    "    return K_backend.sum(K_backend.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An object to hold both loss and gradients, so we can calculate\n",
    "# both at once and hang onto them, returning either one when asked.\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, img_height, img_width, fetch_loss_and_grads):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.fetch_loss_and_grads = fetch_loss_and_grads\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, self.img_height, self.img_width, 3))\n",
    "        outs = self.fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_image(target_image_path, style_reference_image_path, output_prefix,\n",
    "                        target_height, iterations, content_layer, style_layers,\n",
    "                        total_variation_weight, style_weight, content_weight, random_seed):\n",
    "\n",
    "    img_width, img_height = get_target_size(target_image_path, target_height)\n",
    "\n",
    "    target_image = K_backend.constant(preprocess_image(target_image_path, img_height, img_width))\n",
    "    style_reference_image = K_backend.constant(preprocess_image(style_reference_image_path, img_height, img_width))\n",
    "\n",
    "    # This placeholder will contain our generated image\n",
    "    # AG combination_image = K_backend.placeholder((1, img_height, img_width, 3))\n",
    "    combination_image = K_backend.random_uniform_variable(shape=(1, img_height, img_width, 3), low=0, high=1)\n",
    "\n",
    "    # We combine the 3 images into a single batch\n",
    "    input_tensor = K_backend.concatenate([target_image,\n",
    "                                  style_reference_image,\n",
    "                                  combination_image], axis=0)\n",
    "\n",
    "    model = get_VGG16(input_tensor)\n",
    "\n",
    "    # Dict mapping layer names to activation tensors\n",
    "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "\n",
    "    # Define the loss by adding all components to a `loss` variable\n",
    "    loss = K_backend.variable(0.)\n",
    "    layer_features = outputs_dict[content_layer]\n",
    "    target_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    #loss += content_weight * content_loss(target_image_features,\n",
    "                                          #combination_features)\n",
    "    c_loss = content_loss(target_image_features, combination_features)\n",
    "    print('c_loss before numpy = ',c_loss)\n",
    "    c_loss = c_loss.eval()\n",
    "    print('c_loss after numpy = ',c_loss)\n",
    "    loss = loss + (content_weight * c_loss)\n",
    "    \n",
    "    \n",
    "    if len(style_layers) > 0:\n",
    "        for layer_name in style_layers:\n",
    "            layer_features = outputs_dict[layer_name]\n",
    "            style_reference_features = layer_features[1, :, :, :]\n",
    "            combination_features = layer_features[2, :, :, :]\n",
    "            sl = style_loss(style_reference_features, combination_features, img_height, img_width)\n",
    "            loss += (style_weight / len(style_layers)) * sl\n",
    "        loss += total_variation_weight * total_variation_loss(combination_image, img_height, img_width)\n",
    "\n",
    "\n",
    "    # Get the gradients of the generated image wrt the loss\n",
    "    grads = K_backend.gradients(loss, combination_image)[0]\n",
    "\n",
    "    # Function to fetch the values of the current loss and the current gradients\n",
    "    fetch_loss_and_grads = K_backend.function([combination_image], [loss, grads])\n",
    "\n",
    "    evaluator = Evaluator(img_height, img_width, fetch_loss_and_grads)\n",
    "\n",
    "    # Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "    # so as to minimize the neural style loss.\n",
    "    # This is our initial state: the target image.\n",
    "    # Note that `scipy.optimize.fmin_l_bfgs_b` can only process flat vectors.\n",
    "    x = preprocess_image(target_image_path, img_height, img_width)\n",
    "    np.random.seed(random_seed)\n",
    "    x = np.random.uniform(low=-128, high=128, size=x.shape)\n",
    "\n",
    "    x = x.flatten()\n",
    "    for i in range(iterations):\n",
    "        print('Start of iteration', i)\n",
    "        start_time = time.time()\n",
    "        x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n",
    "                                         fprime=evaluator.grads, maxfun=20)\n",
    "        print('Current loss value:', min_val)\n",
    "        # Save current generated image\n",
    "        img = x.copy().reshape((img_height, img_width, 3))\n",
    "        img = deprocess_image(img)\n",
    "        fname = output_prefix+'-iteration-{:d}-sw-{:.5f}-cw-{:.5f}-tvw-{:.6f}.png'.\\\n",
    "                                format(i, style_weight, content_weight, total_variation_weight)\n",
    "        imsave(fname, img)\n",
    "        end_time = time.time()\n",
    "        print('Image saved as', fname)\n",
    "        print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_style_visualizations():\n",
    "    \n",
    "    vgg16_layers_list = [\n",
    "       'block1_conv1', 'block1_conv2',\n",
    "       'block2_conv1', 'block2_conv2',\n",
    "       'block3_conv1', 'block3_conv2', 'block3_conv3', \n",
    "       'block4_conv1', 'block4_conv2', 'block4_conv3', \n",
    "       'block5_conv1', 'block5_conv2', 'block5_conv3', \n",
    "    ]\n",
    "\n",
    "    target_image_path = file_helper.get_input_file_path('waters-3038803_1280-crop.jpg')  # irrelevant for style viz\n",
    "    style_reference_image_path = file_helper.get_input_file_path('HR-Self-Portrait-1907-Picasso.jpg')\n",
    "    target_height = 200\n",
    "    num_iterations = 50\n",
    "    content_layer = 'block4_conv1'   # irrelevant for style\n",
    "    total_variation_weight = 1e-3\n",
    "    style_weight = 1\n",
    "    content_weight = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(vgg16_layers_list)):\n",
    "        style_layers = vgg16_layers_list[:i+1]\n",
    "        random_seed = 42+i\n",
    "        output_prefix = output_data_directory+'/style-to-'+style_layers[-1]\n",
    "        print(\"Starting style viz of layers \",style_layers)\n",
    "        outputs = make_synthetic_image(target_image_path=target_image_path, \n",
    "                                       style_reference_image_path=style_reference_image_path, \n",
    "                                       output_prefix = output_prefix,\n",
    "                                       target_height = target_height, \n",
    "                                       iterations = num_iterations,\n",
    "                                       content_layer = content_layer,\n",
    "                                       style_layers = style_layers,\n",
    "                                       total_variation_weight = total_variation_weight,\n",
    "                                       style_weight = style_weight,\n",
    "                                       content_weight = content_weight,\n",
    "                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_layer_style_visualizations():\n",
    "    \n",
    "    vgg16_layers_list = [\n",
    "       'block1_conv1', 'block1_conv2',\n",
    "       'block2_conv1', 'block2_conv2',\n",
    "       'block3_conv1', 'block3_conv2', 'block3_conv3', \n",
    "       'block4_conv1', 'block4_conv2', 'block4_conv3', \n",
    "       'block5_conv1', 'block5_conv2', 'block5_conv3', \n",
    "    ]\n",
    "\n",
    "    target_image_path = \\\n",
    "        file_helper.get_input_file_path('waters-3038803_1280-crop.jpg')  # irrelevant for style viz\n",
    "    style_reference_image_path = \\\n",
    "        file_helper.get_input_file_path('HR-Self-Portrait-1907-Picasso.jpg')\n",
    "    target_height = 200\n",
    "    num_iterations = 50\n",
    "    content_layer = 'block4_conv1'   # irrelevant for style\n",
    "    total_variation_weight = 1e-3\n",
    "    style_weight = 1\n",
    "    content_weight = 0\n",
    "    \n",
    "    # we skip the first layer because we got it when we did the sets\n",
    "    for i in range(1, len(vgg16_layers_list)):\n",
    "        style_layers = [vgg16_layers_list[i]]\n",
    "        random_seed = 424+i\n",
    "        output_prefix = output_data_directory+'/style-only-'+style_layers[-1]\n",
    "        print(\"Starting style viz of layers \",style_layers)\n",
    "        outputs = make_synthetic_image(target_image_path=target_image_path, \n",
    "                                       style_reference_image_path=style_reference_image_path, \n",
    "                                       output_prefix = output_prefix,\n",
    "                                       target_height = target_height, \n",
    "                                       iterations = num_iterations,\n",
    "                                       content_layer = content_layer,\n",
    "                                       style_layers = style_layers,\n",
    "                                       total_variation_weight = total_variation_weight,\n",
    "                                       style_weight = style_weight,\n",
    "                                       content_weight = content_weight,\n",
    "                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_visualizations():\n",
    "    \n",
    "    vgg16_layers_list = [\n",
    "        'block1_conv1', 'block1_conv2',\n",
    "        'block2_conv1', 'block2_conv2',\n",
    "        'block3_conv1', 'block3_conv2', 'block3_conv3',\n",
    "        'block4_conv1', 'block4_conv2', 'block4_conv3',\n",
    "        'block5_conv1', \n",
    "        'block5_conv2', 'block5_conv3', \n",
    "    ]\n",
    "        \n",
    "    target_image_path = file_helper.get_input_file_path('waters-3038803_1280-crop.jpg')\n",
    "    style_reference_image_path = file_helper.get_input_file_path('HR-Self-Portrait-1907-Picasso.jpg') # irrelevant for content viz\n",
    "    style_layers = [] # irrelevant for content viz\n",
    "    target_height = 200\n",
    "    num_iterations = 50\n",
    "    total_variation_weight = 1e-3\n",
    "    style_weight = 0\n",
    "    content_weight = 1\n",
    "    \n",
    "    for i in range(len(vgg16_layers_list)):\n",
    "        content_layer = vgg16_layers_list[i]\n",
    "        random_seed = 4242+i\n",
    "        print(\"Starting content viz of layer \",content_layer)\n",
    "        output_prefix = output_data_directory+'/content-'+content_layer\n",
    "        outputs = make_synthetic_image(target_image_path=target_image_path, \n",
    "                                       style_reference_image_path=style_reference_image_path, \n",
    "                                       output_prefix = output_prefix,\n",
    "                                       target_height = target_height, \n",
    "                                       iterations = num_iterations,\n",
    "                                       content_layer = content_layer,\n",
    "                                       style_layers = style_layers,\n",
    "                                       total_variation_weight = total_variation_weight,\n",
    "                                       style_weight = style_weight,\n",
    "                                       content_weight = content_weight,\n",
    "                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_style_and_content_visualizations():\n",
    "    #make_style_visualizations()\n",
    "    #make_content_visualizations()\n",
    "    make_single_layer_style_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_style_and_content_visualizations()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
