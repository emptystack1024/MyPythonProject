{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <small>\n",
    "Copyright (c) 2017-21 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning: A Visual Approach\n",
    "## by Andrew Glassner, https://glassner.com\n",
    "### Order: https://nostarch.com/deep-learning-visual-approach\n",
    "### GitHub: https://github.com/blueberrymusic\n",
    "------\n",
    "\n",
    "### What's in this notebook\n",
    "\n",
    "This notebook is provided as a “behind-the-scenes” look at code used to make some of the figures in this chapter. It is cleaned up a bit from the original code that I hacked together, and is only lightly commented. I wrote the code to be easy to interpret and understand, even for those who are new to Python. I tried never to be clever or even more efficient at the cost of being harder to understand. The code is in Python3, using the versions of libraries as of April 2021. \n",
    "\n",
    "This notebook may contain additional code to create models and images not in the book. That material is included here to demonstrate additional techniques.\n",
    "\n",
    "Note that I've included the output cells in this saved notebook, but Jupyter doesn't save the variables or data that were used to generate them. To recreate any cell's output, evaluate all the cells from the start up to that cell. A convenient way to experiment is to first choose \"Restart & Run All\" from the Kernel menu, so that everything's been defined and is up to date. Then you can experiment using the variables, data, functions, and other stuff defined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 21: Reinforcement Learning - Notebook 2: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some code here is adapted from\n",
    "https://www.nervanasys.com/demystifying-deep-reinforcement-learning/\n",
    "\n",
    "This is for learning with Q-Learning and Sarsa\n",
    "\n",
    "There are two games: three_in_a_row_score and two_by_two_box_score (no reason not to add more)\n",
    "\n",
    "Each returns 1 if the condition is exactly met, else 0\n",
    "(so we're doing positive reinforcement only, no punishment. To punish return -1 if not a win)\n",
    "\n",
    " We have the ability to decay the exploration by reducing the chance of picking a random\n",
    " action, rather than the best. I've found that with these tasks, going random about as\n",
    "   much as possible is best, since we get to search the space broadly in a relatively small\n",
    "   number of runs.\n",
    "\n",
    " This file is basically a bunch of tools for learning Flippers and building the\n",
    " images. For each game illustration, set the learning strategy (Q or Sarsa), the\n",
    " action-selection policy, the global variables, the number of episodes, the game\n",
    " number, etc. Then train the model (if needed) and then run the game, saving the\n",
    " results in a PostScript file. Each game needs the file to be manually configured\n",
    " in this way and then re-run. \n",
    "\n",
    " Unlike most of the other figure-making notebooks, this one needs to be modified\n",
    " by hand to produce each of the figures that we want. I'm sure with some effort it\n",
    " can be cleaned up and all the steps parameterized, but this code is reasonably\n",
    " clean, and it works properly, so I'm going to leave it as-is.\n",
    "\n",
    " There are lots of globals in this code, so it's important to run\n",
    " the cells sequentially. \n",
    "\n",
    " The first block of variables control the building of the Q table, including Q or Sarsa\n",
    " The second block of variables control the layout of the PostScript files showing the game\n",
    "\n",
    "#### TO MAKE ILLUSTRATIONS:\n",
    "\n",
    " Because Python's native graphics are so lousy, pictures are saved as PostScript files.\n",
    "\n",
    "-   Set the algorithm and #runs and the game type\n",
    "-    If your configuration doesn't exist, it will learn and save to file. \n",
    "- Else it will read the file.\n",
    "    \n",
    "To find good examples:\n",
    "\n",
    "-    Print out the number of states used for every starting board#    Try different #runs (1k, 5k, 10k, 50k, etc.) and note different #states\n",
    "-    Find games that are long with small #runs, and shorter later, showing learning\n",
    "-    For 3 in a row, game 343 is nice, with 7 steps at 1k runs, but only 3 at 10k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = False\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the heart of the learning algorithm. We play a game and update the Q table.\n",
    "def run_one_episode_of_learning(episode_number):\n",
    "    global board, state_visited, Q\n",
    "    reset_all()\n",
    "    state0 = get_board_state()\n",
    "    score0 = get_board_score()\n",
    "    max_steps = 99\n",
    "    step_count = 0\n",
    "    if learning_algorithm == 'Sarsa':\n",
    "        action0 = epsilon_greedy_policy(episode_number, state0)\n",
    "\n",
    "    while step_count < max_steps:\n",
    "        if score0 == 1:\n",
    "            for action_number in range(num_actions):\n",
    "                win_value = 1 / math.sqrt(max(1, step_count))\n",
    "                Q[state0][action_number] = win_value  # give a winning score (nice when printing game charts)\n",
    "            return  # the episode is over when we have a winning board\n",
    "\n",
    "        if learning_algorithm == 'Q':\n",
    "            action0 = epsilon_greedy_policy(episode_number, state0)\n",
    "\n",
    "        # Take the action\n",
    "        (x, y) = action_to_xy(action0)\n",
    "        make_game_move(x, y)\n",
    "\n",
    "        # rarely, random stuff happens in the world and the board changes on us\n",
    "        add_surprise()\n",
    "\n",
    "        # Find the new state and new score\n",
    "        state1 = get_board_state()\n",
    "        score1 = get_board_score()\n",
    "\n",
    "        # Get the reward. It's 1 if we've won, else -1\n",
    "        reward = non_winning_reward\n",
    "        if score1 == 1:\n",
    "            reward = 1\n",
    "\n",
    "        # get next action\n",
    "        if learning_algorithm == 'Q':\n",
    "            action1 = np.argmax(Q[state1])\n",
    "        else:\n",
    "            action1 = epsilon_greedy_policy(episode_number, state1)\n",
    "\n",
    "        # Bellman's equation to update the Q score\n",
    "        Q[state0][action0] += learning_rate * (reward + (gamma * Q[state1][action1]) - Q[state0][action0])\n",
    "\n",
    "        # The new state becomes the starting state for the next time through\n",
    "        state0 = state1\n",
    "        score0 = score1\n",
    "        action0 = action1\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "        # A little optimization: if we've seen this state before, we're in a loop so quit.\n",
    "        #if state_visited[state0]:\n",
    "            #return\n",
    "        state_visited[state0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(episode_number, state):\n",
    "    # Choose an action. If under epsilon (which decays), pick randomly, else pick best Q-score\n",
    "    threshold = 0\n",
    "    drop_limit = epsilon_stop_percent * num_episodes\n",
    "    if episode_number < drop_limit:\n",
    "        threshold = epsilon_max * np.exp(-5 * episode_number / drop_limit) # starts at e_max, drops to .006 (call it 0)\n",
    "    if random.rand() < threshold:\n",
    "        action = random.randint(0, num_actions)\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for a new episode\n",
    "def reset_all():\n",
    "    global board, state_visited\n",
    "    board_state = random.randint(0, num_states)\n",
    "    board_from_state(board_state)\n",
    "    state = get_board_state()\n",
    "    state_visited = [False for i in range(num_states)]\n",
    "    state_visited[state] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Utilities for manipuating the board, working with actions, etc.\n",
    "# These are from Q-table-builder, and probably should be in a common\n",
    "# file that both modules import, but for now repeating them is file\n",
    "##########\n",
    "\n",
    "# return the x and y coordinates for an action number\n",
    "def action_to_xy(action):\n",
    "    y = int(action * 1. / board_size)\n",
    "    x = int(action - (y * board_size))\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "# cells are worth 1, 2, 4, 8, 16... in reading order from UL\n",
    "def get_board_state():\n",
    "    flat_board = np.ravel(board)\n",
    "    score = sum([flat_board[i] * (2 ** i) for i in range(flat_board.size)])\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "# given a state number, recreate the board\n",
    "def board_from_state(state):\n",
    "    global board\n",
    "    binary_state = state_in_binary(state)\n",
    "    flat_board = [int(binary_state[i]) for i in range(len(binary_state))]\n",
    "    # print(\"   board from state: flat_board = {}\".format(flat_board))\n",
    "    board = np.reshape(flat_board, [board_size, board_size])\n",
    "\n",
    "\n",
    "# return a binary string describing the board contents, starting in UL, row 1 L->R, row 2 L->R, etc.\n",
    "def state_in_binary(state):\n",
    "    format_string = \"{0:0\" + str(num_actions) + \"b}\"\n",
    "    binary_version = ''.join(reversed(format_string.format(state)))\n",
    "    return binary_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surprise():\n",
    "    global board, num_actions, random_event_probability\n",
    "    if random.rand() < random_event_probability:\n",
    "        action = random.randint(0, num_actions)\n",
    "        (x, y) = action_to_xy(action)\n",
    "        make_game_move(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 1 if we have exactly 1 row or column of 1's, else 0\n",
    "def three_in_a_row_score():\n",
    "    global board\n",
    "    total_ones = np.sum(np.ravel(board))\n",
    "    if total_ones != board_size:\n",
    "        return 0  # early exit, too few or too many 1's\n",
    "    column_totals = np.sum(board, 0)\n",
    "    if max(column_totals) == board_size:\n",
    "        return 1\n",
    "    row_totals = np.sum(board, 1)\n",
    "    if max(row_totals) == board_size:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def three_in_a_row_move(x, y):\n",
    "    global board\n",
    "    board[y][x] = 1 - board[y][x]\n",
    "\n",
    "\n",
    "def three_in_a_row_name():\n",
    "    return 'three-in-a-row'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 1 if we have a 2-by-2 box. This could be faster, but this is simple\n",
    "def two_by_two_box_score():\n",
    "    global board\n",
    "    total_ones = np.sum(np.ravel(board))\n",
    "    if total_ones != 4:\n",
    "        return 0  # early exit, too few or too many 1's\n",
    "    for y in range(board_size - 1):\n",
    "        for x in range(board_size - 1):\n",
    "            if board[y][x] == 1:\n",
    "                if (board[y + 1][x] == 1) and (board[y][x + 1] == 1) and (board[y + 1][x + 1] == 1):\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def two_by_two_box_move(x, y):\n",
    "    global board\n",
    "    board[y][x] = 1 - board[y][x]\n",
    "\n",
    "\n",
    "def two_by_two_box_name():\n",
    "    return 'two-by-two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridiculously slow. scan every 1 and see if it's a plus center\n",
    "def plus_sign_score():\n",
    "    global board\n",
    "    total_ones = np.sum(np.ravel(board))\n",
    "    if total_ones != 5:\n",
    "        return 0  # early exit, too few or too many 1's\n",
    "    for y in np.arange(1, board_size - 1):\n",
    "        for x in np.arange(1, board_size - 1):\n",
    "            if board[y][x] == 1:\n",
    "                if (board[y - 1][x] == 1) and (board[y + 1][x] == 1) and (board[y][x - 1] == 1) and (\n",
    "                    board[y][x + 1] == 1):\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def plus_sign_move(x, y):\n",
    "    global board\n",
    "    board[y][x] = 1 - board[y][x]\n",
    "    if x > 0:\n",
    "        board[y][x - 1] = 1 - board[y][x - 1]\n",
    "    if x < board_size - 1:\n",
    "        board[y][x + 1] = 1 - board[y][x + 1]\n",
    "    if y > 0:\n",
    "        board[y - 1][x] = 1 - board[y - 1][x]\n",
    "    if y < board_size - 1:\n",
    "        board[y + 1][x] = 1 - board[y + 1][x]\n",
    "\n",
    "\n",
    "def plus_sign_name():\n",
    "    return 'plus-sign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Building the Q table\n",
    "###############################\n",
    "num_episodes  = 3000            # number of games to play to train Q table (10000)\n",
    "board_size = 3                   # side length of square board (more than 4 is playing with fire) (3)\n",
    "learning_rate = 0.99             # lerp value from old score to new (0.99)\n",
    "gamma = 0.85                     # discount on future scores (0.85)\n",
    "random_event_probability = 0.10  # how likely it is that a random tile will flip on us after a move (0.1)\n",
    "epsilon_max = .8                 # how likely we are to pick a random action, decays with episodes\n",
    "epsilon_stop_percent = .5        # how far into this series of runs until epsilon hits 0\n",
    "num_random_games = 1000          # for the \"average game length\" printout after the table is built (1000)\n",
    "random_seed = 3                  # seeding the RNG\n",
    "non_winning_reward = 0           # set to -1 for \"punishment\", 0 for \"positive reinforcement only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Choose the game to play\n",
    "###############################\n",
    "game_functions = ( (three_in_a_row_score, three_in_a_row_move, three_in_a_row_name),\n",
    "                   (two_by_two_box_score, two_by_two_box_move, two_by_two_box_name),\n",
    "                   (plus_sign_score, plus_sign_move, plus_sign_name) )\n",
    "(get_board_score, make_game_move, game_name) = game_functions[0]  # choose the game to play\n",
    "\n",
    "# choose these by hand to make nice examples for figures\n",
    "game_list_3_cross = (163, 343, 313, 495)  # good for 3-by-3 cross\n",
    "game_list_4_cross = (33444, 10344, 44122, 18705) # good for 4-by-4 cross\n",
    "game_list = ( 49, 182 )\n",
    "\n",
    "learning_algorithm = 'Q'        # either 'Q' or 'Sarsa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Global variables built from the user-defined variables\n",
    "###############################\n",
    "num_actions = num_states = total_board_cells = Q = state_visited = board = None\n",
    "\n",
    "\n",
    "def start_run():\n",
    "    global Q\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    filename = data_dir + \"/\" + \"Qtable-size-\" + str(board_size) + \"-alg-\" + learning_algorithm + \"-episodes-\" + str(num_episodes) + \"-game-\" + game_name() + \".pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        read_data_file(filename)\n",
    "    else:\n",
    "        make_derived_variables()\n",
    "        Q = np.zeros([num_states, num_actions], dtype='float')  # initialize Q table to all 0\n",
    "        random.seed(random_seed)\n",
    "        for episode_number in range(num_episodes):\n",
    "            if episode_number % 1000 == 0:\n",
    "                print(\"Starting episode {}\".format(episode_number))\n",
    "            run_one_episode_of_learning(episode_number)\n",
    "        save_data_file(filename)\n",
    "\n",
    "    #print_all_game_lengths()   # this helps us choose good games to plot\n",
    "\n",
    "    #save_games(save_chart, game_list)\n",
    "    #save_games(save_sequence, game_list)\n",
    "\n",
    "    #plot_results()  # in PyCharm this hangs until the window is closed, so make it last step\n",
    "    print_averages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_averages():\n",
    "    total_moves = 0\n",
    "    max_moves = 0\n",
    "    for game_number in range(num_states):\n",
    "        (action_list, state_list) = run_game(game_number)\n",
    "        total_moves += action_list.size\n",
    "        max_moves = max(max_moves, action_list.size)\n",
    "    average_moves = total_moves * 1.0 / num_states\n",
    "    flat_Q = np.ravel(Q)\n",
    "    average_Q = np.average(flat_Q)\n",
    "    min_Q = min(flat_Q)\n",
    "    max_Q = max(flat_Q)\n",
    "    print(\"Zlearning_algorithm =A {} Znum_episodes =B {} Zaverage_moves =C {}    Zmin_Q =D {} Zaverage_Q =E {} Zmax_Q =F {}\".\n",
    "          format(learning_algorithm, num_episodes, average_moves, min_Q, average_Q, max_Q))\n",
    "    print(\"max_moves = {}\".format(max_moves))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_file(filename):\n",
    "    master_dict = {}\n",
    "    master_dict.update({'num_episodes': num_episodes, 'board_size': board_size, 'learning_rate': learning_rate})\n",
    "    master_dict.update({'gamma': gamma, 'random_event_probability': random_event_probability})\n",
    "    master_dict.update({'epsilon_max': epsilon_max, 'epsilon_stop_percent': epsilon_stop_percent})\n",
    "    master_dict.update({'random_seed': random_seed})\n",
    "    master_dict.update({'Q_table': Q})\n",
    "    pickle.dump(master_dict, open(filename, \"wb\"))\n",
    "    #print(\"saving Q={}\".format(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(filename):\n",
    "    global num_episodes, board_size, learning_rate, gamma, random_event_probability\n",
    "    global epsilon_max, epsilon_stop_percent, random_seed, Q\n",
    "    global num_actions, num_states, total_board_cells, state_visited, board\n",
    "\n",
    "    master_dict = pickle.load(open(filename, \"rb\"))\n",
    "    num_episodes = master_dict['num_episodes']\n",
    "    board_size = master_dict['board_size']\n",
    "    learning_rate = master_dict['learning_rate']\n",
    "    gamma = master_dict['gamma']\n",
    "    random_event_probability = master_dict['random_event_probability']\n",
    "    epsilon_max = master_dict['epsilon_max']\n",
    "    epsilon_stop_percent = master_dict['epsilon_stop_percent']\n",
    "    random_seed = master_dict['random_seed']\n",
    "    Q = master_dict['Q_table']\n",
    "    #print_input_variables()\n",
    "    make_derived_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_input_variables():\n",
    "    print(\"READ IN VARIABLES\")\n",
    "    print(\"num_episodes = {}\".format(num_episodes))\n",
    "    print(\"board_size = {}\".format(board_size))\n",
    "    print(\"learning_rate = {}\".format(learning_rate))\n",
    "    print(\"gamma = {}\".format(gamma))\n",
    "    print(\"random_event_probability = {}\".format(random_event_probability))\n",
    "    print(\"epsilon_max = {}\".format(epsilon_max))\n",
    "    print(\"epsilon_stop_percent = {}\".format(epsilon_stop_percent))\n",
    "    print(\"random_seed = {}\".format(random_seed))\n",
    "    print(\"--------\")\n",
    "    print(\"num_actions = {}\".format(num_actions))\n",
    "    print(\"num_states = {}\".format(num_states))\n",
    "    print(\"total_board_cells = {}\".format(total_board_cells))\n",
    "    print(\"board = {}\".format(board))\n",
    "    print(\"--------\")\n",
    "    print(\"Q.shape = {}\".format(Q.shape))\n",
    "    print(\"Q = {}\".format(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_derived_variables():\n",
    "    global num_actions, num_states, total_board_cells, Q, state_visited, board\n",
    "\n",
    "    num_actions = board_size * board_size\n",
    "    num_states = 2 ** num_actions\n",
    "    total_board_cells = board_size ** 2\n",
    "    board = np.zeros([board_size, board_size])\n",
    "    state_visited = np.zeros(num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Plot the contents of the Q table in a scatter plot\n",
    "###############################\n",
    "def plot_results():\n",
    "    global Q\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            x_coords.append(Q[state][action])\n",
    "            y_coords.append(state)\n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=200)\n",
    "    plt.scatter(x_coords, y_coords, s=10, color=['black'])\n",
    "    file_helper.save_figure('Q-table-scatter')\n",
    "    plt.show()\n",
    "\n",
    "    # plot game lengths\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    total_moves = 0\n",
    "    max_moves = 0\n",
    "    for game_number in range(num_states):\n",
    "        (action_list, state_list) = run_game(game_number)\n",
    "        x_coords.append(action_list.size)\n",
    "        y_coords.append(game_number)\n",
    "        total_moves += action_list.size\n",
    "        max_moves = max(max_moves, action_list.size)\n",
    "        #print(\"game {} uses {} steps\".format(game_number, action_list.size))\n",
    "    average_moves = total_moves * 1.0 / num_states\n",
    "    print(\"average number of moves in {} games: {}, longest game = {} moves\".format(num_states, average_moves, max_moves))\n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=200)\n",
    "    plt.scatter(x_coords, y_coords, s=10, color=['black'])\n",
    "    file_helper.save_figure('Q-table-game-lengths')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Estimate the average game length by running a bunch of games\n",
    "###############################\n",
    "\n",
    "def run_games():\n",
    "    total_moves = 0\n",
    "    for game in range(num_random_games):\n",
    "        (action_list, state_list) = run_random_game()\n",
    "        num_moves = action_list.size\n",
    "        total_moves += num_moves\n",
    "    average_moves = total_moves * 1.0 / num_random_games\n",
    "    print(\"average number of moves in {} games: {}\".format(num_random_games, average_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_game():\n",
    "    game_number = random.randint(0, num_states)\n",
    "    return run_game(game_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(game_number):\n",
    "    global board, Q, get_board_score\n",
    "    board_from_state(game_number)\n",
    "    score = get_board_score()\n",
    "    action_list = np.array([], dtype='int32')\n",
    "    state = get_board_state()\n",
    "    state_list = np.array([state], dtype='int32')\n",
    "    visited_state = [False for i in range(num_states)]\n",
    "    while (score != 1) and (state_list.size < 99): # and not visited_state[state]:\n",
    "        visited_state[state] = True\n",
    "        best_action = np.argmax(Q[state])\n",
    "        (x, y) = action_to_xy(best_action)\n",
    "        make_game_move(x, y)\n",
    "        add_surprise()\n",
    "        state = get_board_state()\n",
    "        action_list = np.append(action_list, best_action)\n",
    "        state_list = np.append(state_list, state)\n",
    "        score = get_board_score()\n",
    "    return (action_list, state_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_game_lengths():\n",
    "    for game_number in range(num_states):\n",
    "        (action_list, state_list) = run_game(game_number)\n",
    "        print(\"game number {}, states = {}\".format(game_number, state_list.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Control the layout of the PostScript file that shows a game chart\n",
    "###############################\n",
    "page_width = 791         # width in PS pixels of drawing area\n",
    "page_height = 791        # height in PS pixels of drawing area\n",
    "page_border = 50         # pstopdf clips thick lines on the edges, so add padding all around\n",
    "gd = .5                  # target x gap ratio\n",
    "ge = .5                  # target y gap ratio\n",
    "gs = .25                 # ratio of score box height to box height\n",
    "board_length = 0         # length in PS pixels of a board drawing that meets all above constraints\n",
    "x_chunk = 0              # width of board plus gap\n",
    "y_chunk = 0              # height of board plus score plus gap\n",
    "score_height = 0         # height of score box\n",
    "cell_size = 0            # size of one cell in game box\n",
    "output_file = None       # output PS file\n",
    "max_display_states = 10  # show this many states at most\n",
    "data_dir = \"flippers-data\"        # directory for placing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_games(save_game_style, game_list):\n",
    "    for game_num in game_list:\n",
    "        (action_list, state_list) = run_game(game_num)\n",
    "        print(\"game {} uses {} states\".format(game_num, state_list.size))\n",
    "        save_game_style(game_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chart(game_num):\n",
    "    global board_length, x_chunk, y_chunk, score_height, output_file, cell_size\n",
    "    padded_game_num = '{:03}'.format(game_num)\n",
    "    chart_name = \"chart-\" + padded_game_num + \".ps\"\n",
    "    #print(\"chart_name = <{}>\".format(chart_name))\n",
    "    output_file = open(data_dir+\"/\"+chart_name, \"w\")\n",
    "    output_file.write(\"%!PS\\n\")\n",
    "    output_file.write(\"%% chart for game {}\\n\".format(game_num))\n",
    "    output_file.write(\"<< /PageSize [{} {}] >> setpagedevice\\n\\n\".\n",
    "                      format(page_width + (2 * page_border), page_height + (2 * page_border)))\n",
    "    output_file.write(\"2 setlinewidth\\n\")\n",
    "\n",
    "    (action_list, state_list) = run_game(game_num)  # get the actual run of the game\n",
    "    action_list = action_list[:max_display_states]  # clip the output after the first few states if needed\n",
    "    state_list = state_list[:max_display_states]    # if there's too many states they're all tiny and ugly\n",
    "    num_steps = state_list.size\n",
    "\n",
    "    if num_steps > 1:\n",
    "        num_rows = num_actions\n",
    "        num_columns = num_steps+1\n",
    "        w = page_width / (num_columns + ((num_columns - 1) * gd))\n",
    "        h = page_height / ((num_rows * (1 + gs + ge)) - ge)\n",
    "        b = min(h, w)\n",
    "        d = (page_width - ((num_columns) * b)) / (num_columns - 1)\n",
    "        x_chunk = b + d\n",
    "        s = b * gs\n",
    "        e = (page_height - (num_rows * (b + s))) / (num_rows - 1)\n",
    "        y_chunk = b + s + e\n",
    "        board_length = b\n",
    "    else:\n",
    "        x_chunk = 100\n",
    "        y_chunk = 100\n",
    "        board_length = 100\n",
    "\n",
    "    score_height = board_length * gs\n",
    "    cell_size = board_length * 1.0 / board_size\n",
    "\n",
    "    # starting state is special: no score, no variations\n",
    "    left = page_border\n",
    "    top = page_border + (page_height / 2.) + (board_length / 2.)\n",
    "    state = state_list[0]\n",
    "    action_number = -1\n",
    "    draw_game_board(left, top, state, action_number, False, False)\n",
    "\n",
    "    for step in range(num_steps - 1):\n",
    "        draw_column(step, action_list, state_list)\n",
    "\n",
    "    # final state is special: no score, no variations\n",
    "    left = page_border + (num_steps * x_chunk)\n",
    "    top = page_border + (page_height / 2.) + (board_length / 2.)\n",
    "    state = state_list[-1]\n",
    "    action_number = -1\n",
    "    draw_game_board(left, top, state, action_number, False, False)\n",
    "\n",
    "    output_file.write(\"showpage\\n\")\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequence(game_num):\n",
    "    global board_length, x_chunk, y_chunk, score_height, output_file, cell_size\n",
    "    padded_game_num = '{:03}'.format(game_num)\n",
    "    sequence_name = \"sequence-\" + padded_game_num + \".ps\"\n",
    "    #print(\"sequence_name = <{}>\".format(sequence_name))\n",
    "    output_file = open(data_dir+\"/\"+sequence_name, \"w\")\n",
    "    output_file.write(\"%!PS\\n\")\n",
    "    output_file.write(\"%% sequence for game {}\\n\".format(game_num))\n",
    "    output_file.write(\"<< /PageSize [{} {}] >> setpagedevice\\n\\n\".\n",
    "                      format(page_width + (2 * page_border), page_height + (2 * page_border)))\n",
    "    output_file.write(\"2 setlinewidth\\n\")\n",
    "\n",
    "    (action_list, state_list) = run_game(game_num)\n",
    "    num_steps = state_list.size\n",
    "    num_columns = num_steps + 1\n",
    "\n",
    "    if num_columns > 1:\n",
    "        num_columns = num_steps+1\n",
    "        w = page_width / (num_columns + ((num_columns - 1) * gd))\n",
    "        b = w\n",
    "        d = (page_width - ((num_columns) * b)) / (num_columns - 1)\n",
    "        x_chunk = b + d\n",
    "        board_length = b\n",
    "        top = page_height + (b/2.)\n",
    "    else:\n",
    "        x_chunk = 100\n",
    "        y_chunk = 100\n",
    "        board_length = 100\n",
    "\n",
    "    score_height = board_length * gs\n",
    "    cell_size = board_length * 1.0 / board_size\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        if step==0:\n",
    "            draw_game_board(page_border, top, state_list[0], -1, False, False)\n",
    "        if step == num_steps-1:\n",
    "            draw_game_board(page_border+((step+1)*x_chunk), top, state_list[-1], -1, False, False)\n",
    "        else:\n",
    "            draw_game_board(page_border+((step+1)*x_chunk), top, state_list[step], action_list[step], False, True)\n",
    "\n",
    "    output_file.write(\"showpage\\n\")\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_column(step, action_list, state_list):\n",
    "    state = state_list[step]\n",
    "    board_from_state(state)\n",
    "    left = page_border + ((step + 1) * x_chunk)\n",
    "    for action_number in range(num_actions):\n",
    "        top = (page_height + page_border) - (action_number * y_chunk)\n",
    "        highlight = action_number == action_list[step]\n",
    "        draw_game_board(left, top, state, action_number, highlight, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_game_board(left, top, state, action_number, highlight, draw_score):\n",
    "    global board\n",
    "    # draw the thick black highlight if we need it\n",
    "    if highlight:\n",
    "        output_file.write(\"15 setlinewidth\\n\")\n",
    "        draw_box(left, top, board_size * cell_size, (board_size * cell_size + score_height), True, (0, 0, 0), True,\n",
    "                 (0, 0, 0))\n",
    "        output_file.write(\"2 setlinewidth\\n\")\n",
    "    # load the board, and draw a box for each cell. Normally draw a gray circle in\n",
    "    # each cell of value 1. But if that cell is the one we're thinking of changing,\n",
    "    # draw a solid circle if it was empty, or an unfilled circle if it was full\n",
    "    board_from_state(state)\n",
    "    saved_board = np.copy(board)\n",
    "    action_x = action_y = -1\n",
    "    if action_number >= 0:\n",
    "        (action_x, action_y) = action_to_xy(action_number)\n",
    "        make_game_move(action_x, action_y)\n",
    "    for y in range(board_size):\n",
    "        for x in range(board_size):\n",
    "            cx = left + ((x + .5) * cell_size)\n",
    "            cy = top - ((y + .5) * cell_size)\n",
    "            radius = cell_size * .3\n",
    "            this_action = (y * board_size) + x\n",
    "            if (x == action_x) and (y == action_y):\n",
    "                draw_box(left + (x * cell_size), top - (y * cell_size), cell_size, cell_size, True, (0, 0, 0), True,\n",
    "                         (.76, .76, 1))\n",
    "            else:\n",
    "                draw_box(left + (x * cell_size), top - (y * cell_size), cell_size, cell_size, True, (0, 0, 0), True,\n",
    "                         (1, 1, 1))\n",
    "            if board[y][x] == saved_board[y][x]:\n",
    "                if board[y][x] == 1:\n",
    "                    draw_circle(cx, cy, radius, False, (0, 0, 0), True, (.3, .3, .3))\n",
    "            else:\n",
    "                if board[y][x] == 0:\n",
    "                    draw_circle(cx, cy, radius, True, (1, 0, 0), False, (1, 0, 0))\n",
    "                else:\n",
    "                    draw_circle(cx, cy, radius, False, (1, 0, 0), True, (1, 0, 0))\n",
    "    if draw_score:\n",
    "        # draw the score box with a color bar indicating score value\n",
    "        draw_box(left, top - board_length, board_length, score_height, True, (0, 0, 0), True, (1, 1, 1))\n",
    "        flat_Q = np.ravel(Q)\n",
    "        min_Q_score = min(flat_Q)\n",
    "        max_Q_score = max(flat_Q)\n",
    "        Q_score = (Q[state][action_number] - min_Q_score) / (max_Q_score - min_Q_score)\n",
    "        draw_box(left, top - board_length, board_length * Q_score, score_height,\n",
    "                 True, (0, 0, 0), True, (1 - Q_score, Q_score, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(left, top, width, height, do_stroke, stroke_rgb, do_fill, fill_rgb):\n",
    "    output_file.write(\"newpath\\n\")\n",
    "    output_file.write(\"{} {} moveto\\n\".format(left, top))\n",
    "    output_file.write(\"{} {} lineto\\n\".format(left + width, top))\n",
    "    output_file.write(\"{} {} lineto\\n\".format(left + width, top - height))\n",
    "    output_file.write(\"{} {} lineto\\n\".format(left, top - height))\n",
    "    output_file.write(\"closepath\\n\")\n",
    "    if do_stroke:\n",
    "        output_file.write(\"gsave\\n\")\n",
    "        output_file.write(\"{} {} {} setrgbcolor\\n\".format(stroke_rgb[0], stroke_rgb[1], stroke_rgb[2]))\n",
    "        output_file.write(\"stroke\\n\")\n",
    "        output_file.write(\"grestore\\n\")\n",
    "    if do_fill:\n",
    "        output_file.write(\"{} {} {} setrgbcolor\\n\".format(fill_rgb[0], fill_rgb[1], fill_rgb[2]))\n",
    "        output_file.write(\"fill\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(cx, cy, radius, do_stroke, stroke_rgb, do_fill, fill_rgb):\n",
    "    output_file.write(\"newpath\\n\")\n",
    "    output_file.write(\"{} {} {} 0 360 arc closepath\\n\".format(cx, cy, radius))\n",
    "    if do_stroke:\n",
    "        output_file.write(\"gsave\\n\")\n",
    "        output_file.write(\"{} {} {} setrgbcolor\\n\".format(stroke_rgb[0], stroke_rgb[1], stroke_rgb[2]))\n",
    "        output_file.write(\"stroke\\n\")\n",
    "        output_file.write(\"grestore\\n\")\n",
    "    if do_fill:\n",
    "        output_file.write(\"{} {} {} setrgbcolor\\n\".format(fill_rgb[0], fill_rgb[1], fill_rgb[2]))\n",
    "        output_file.write(\"fill\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 0\n",
      "Starting episode 1000\n",
      "Starting episode 2000\n",
      "Zlearning_algorithm =A Q Znum_episodes =B 3000 Zaverage_moves =C 17.6875    Zmin_Q =D 0.0 Zaverage_Q =E 0.1384620876097466 Zmax_Q =F 1.8449527826599037\n",
      "max_moves = 98\n"
     ]
    }
   ],
   "source": [
    "start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
